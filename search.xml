<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring Cloud Stream入门]]></title>
    <url>%2FCKING.github.io%2F2019%2F10%2F29%2FSpring-Cloud-Stream%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Spring Cloud Stream​ 官方定义Spring Cloud Stream是一个构建消息驱动微服务的框架。应用程序通过inputs或者outputs来与Spring Cloud Stream中binder交互，通过我们来配置binding，而Spring Cloud Stream的binder负责与消息中间件交互。所以，我们只需要搞清楚如何与Spring Cloud Stream交互就可以方便使用消息驱动的方式。它通过使用Spring Integration来连接消息中间件以实现消息事件驱动。Spring Cloud Stream为一些供应商的消息中间件产品提供了个性化的自动化配置实现，引用了发布-订阅、消费组、分区的三个概念。目前支持市场上主流的多个消息中间件。 发布/订阅​ 简单的讲就是一种生产者、消费者模式。发布者是生产，将输出发布到数据中心，订阅者是消费者，订阅自己感兴趣的数据。当有数据到达数据中心，就把数据发送给对应的订阅者。 消费组​ 直观的理解就是一群消费者一起处理消息。需要注意的是：每个发动到消费组的数据，仅有消费组中的一个消费者处理。 分区​ 类比于消费组，分区是将数据分区。例如，某个应用有多个实例，都绑定到同一个数据中心，也就是不同实例都将数据发布到同一个数据中心。分区就是将数据中心的数据再细分成不同的区。为什么需要分区？因为即使是同一个应用，不同实例发布的数据类型可能不同，也希望这些数据由不同的消费者处理。这就需要，消费者可以仅订阅一个数据中心的部分数据，这就需要分区这个东西了。 Stream解决了什么问题​ Stream解决了开发人员无感知地使用消息中间件的问题，因为Stream对消息中间件的进一步封装，可以做到代码层面对中间件的无感知，甚至于动态的切换中间件（rabbitMQ切换为Kafka），使得微服务开发的高度解耦，服务可以关注更多自己的业务流程。结构图如下： 组成 说明 Middleware 中间件，支持市场上多种主流的MQ中间件 Binder Binder是应用与消息中间件之间的封装。通过Binder可以很方便地连接中间件，可以动态地改变消息类型（对应于Kafka的topic，RabbitMQ的exchange），这些都可以通过配置文件来实现 @Input 注解标识输入通道，通过该输入通道接收到的信息进入应用程序 @Output 注解标识输出通道，发布的消息将通过该通道离开应用程序 @StreamListener 监听队列，用于消费者队列的消息接收 @EnableBinding 指信道channel和exchange绑定在一起 消息驱动入门案例​ 现在通过一个入门案例来演示通过stream整合RabbitMQ来实现消息的异步通信的效果。首先是先安装部署RabbitMQ，具体方法自行百度。我这边是用docker安装的RabbitMQ，参考的是这篇文章：Docker 安装部署RabbitMQ。 ​ RabbitMQ安装好之后，就开始我们的代码了。首先先创建SpringCloud的一个父工程，然后在父工程下面新建两个服务： cloud-stream-producer-rabbitmq：作为一个发布者，将消息推动到RabbitMQ cloud-stream-consumer-rabbitmq：消费者消费信息 ​ 首先是添加依赖，其中最主要的是spring cloud stream的RabbitMQ依赖，还有就是为了使用spring cloud stream，我们还要引入spring cloud依赖，整个pom文件如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;cloud-stream-producer-rabbitmq&lt;/module&gt; &lt;module&gt;cloud-stream-consumer-rabbitmq&lt;/module&gt; &lt;/modules&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.springcloudstream&lt;/groupId&gt; &lt;artifactId&gt;rabbitmqdemo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;rabbitmqdemo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud.version&gt;Greenwich.SR1&lt;/spring-cloud.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-test-support&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 创建生产者​ 如前所述，将消息从发布者传递到队列的整个过程是通过通道Channel完成的，因此，我们创建一个HelloBinding接口，其中包含我们自定义的消息信道greetingChannel。 12345public interface HelloBinding &#123; @Output("greetingChannel") MessageChannel greeting();&#125; ​ 因为这个是要发布消息的，所以我们使用@Output注解，方法名可以是我们想要的任何名称，当然，我们可以在一个接口中有多个Channel（通道）。 ​ 现在，我们创建一个Controller，它将消息推动到这个Channel（通道） 12345678910111213141516@RestControllerpublic class ProducerController &#123; private MessageChannel greet; public ProducerController(HelloBinding binding) &#123; greet = binding.greeting(); &#125; @GetMapping("/greet/&#123;name&#125;") public void publish(@PathVariable String name) &#123; String greeting = "hello " + name + "!"; Message&lt;String&gt; msg = MessageBuilder.withPayload(greeting).build(); this.greet.send(msg); &#125;&#125; ​ 上面我们创建了一个ProducerController类，它有一个MessageChannel类型的属性，这是我们通过我们前面声明的方法在构造函数中初始化的。然后，我们有一个简单的Restful接口， 它接收PathVariable的name，并使用MessageBuilder创建一个String类型的消息。最后，我们使用MessageChannel上的.send()方法来发布消息。 ​ 现在,我们将在的主类中添加@EnableBinding注解，传入HelloBinding告诉Spring加载。 12345678@EnableBinding(HelloBinding.class)@SpringBootApplicationpublic class ProducerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ProducerApplication.class, args); &#125;&#125; ​ 最后，我们必须告诉Spring如何连接到RabbitMQ，并将greetingChannel连接到一可用的消费这。而这些都是在application.properties配置文件中定义的。 123456789spring.rabbitmq.addresses=47.105.176.129spring.rabbitmq.username=rootspring.rabbitmq.password=xxxspring.rabbitmq.port=5672spring.rabbitmq.virtual-host=my_vhostspring.rabbitmq.publisher-confirms=truespring.cloud.stream.bindings.greetingChannel.destination=greetingsserver.port=8080 ​ 其中，spring.cloud.stream.bindings.greetingChannel.destination的意思是greetingChannel这个通道的目的地，类似于Kafka的Topic和RabbitMQ的队列的概念 。后面的消费者也是通过这个去配置消费者去相同的Channel中取数据。另外一个配置spring.rabbitmq.virtual-host，是配置当前用户的权限，这个我们可以通过RabbitMQ的管理界面去确定这个配置的内容： 创建消费者​ 现在，我们需要监听之前创建的通道greetingChannel。让我们创建一个绑定，为了区分，消费者的Channel我们命名为helloChannel。 1234567public interface HelloBinding &#123; String GERRTING = "helloChannel"; @Input(GERRTING) SubscribableChannel greeting();&#125; ​ 与生产者绑定的两个非常明显的区别。因为我们是要消费信息，所以我们使用SubscribableChannel和@Input标识它为消费者。消息推送将被推送到这里。 ​ 现在，我们创建处理数据的方法： 12345678@EnableBinding(HelloBinding.class)public class HelloListener &#123; @StreamListener(target = HelloBinding.GERRTING) public void processHelloChannelGreeting(String msg) &#123; System.out.println(msg); &#125;&#125; ​ 在这里，我们创建一个HelloListener类，在processHelloChannelGreeting方法上添加@StreamListener注解，这个方法需要一个字符串作为参数。我们还在类添加@EnableBinding启用了HelloBinding。 ​ 注意，我们在这里使用@EnableBinding，而不是主类，我们的主类，其实是没有任何修改的： 123456@SpringBootApplicationpublic class ConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerApplication.class, args); &#125;&#125; ​ 最后，我们修改消费者的配置文件： 123456789spring.rabbitmq.addresses=47.105.176.129spring.rabbitmq.username=rootspring.rabbitmq.password=xxxspring.rabbitmq.port=5672spring.rabbitmq.virtual-host=my_vhostspring.rabbitmq.publisher-confirms=truespring.cloud.stream.bindings.helloChannel.destination=greetingsserver.port=9090 ​ 其中，spring.cloud.stream.bindings.helloChannel.destination的意思是helloChannel这个通道的目的地是greetings，这个跟生产者是一样的，从而让消费者指向了跟生产者一样的目的地。 测试​ 我们同时启动生产者和消费者，通过浏览器或postman访问http://localhost:8080/greet/ckin来生产消息，可以在打印台中看到看到消息内容： ​ 现在我们启动另一个消费者服务。端口号为9091，当我们点击生产者的REST端点生产消息时，我们看到两个消费者都收到了消息： ​ 如果我们只想让一个消费者消费一条消息的话，我们可以在application.properties中创建一个消费者组。消费者的文件如下： 1spring.cloud.stream.bindings.greetingChannel.group = greetings-group ​ 相关代码已上传到github，需要的可以去下载。 参考资料Spring Cloud Stream入门介绍 消息驱动式微服务：Spring Cloud Stream &amp; RabbitMQ]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM之垃圾回收器]]></title>
    <url>%2FCKING.github.io%2F2019%2F10%2F23%2FJVM%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[​ 在新生代和老年代进行垃圾回收的时候，都是要用垃圾回收器进行回收的，不同的区域用不同的垃圾回收器。常用的垃圾回收机有一下几种： Serial和Serial Old垃圾回收器：分别用来回收新生代和老年代的垃圾对象。工作原理就是单线程运行，垃圾回收的时候会停止我们自己写的系统的其他工作线程，让我们系统直接卡死不动，然后让他们垃圾回收，这个现在一般写后台Java系统几乎不用。 ParNew和CMS垃圾回收器：ParNew现在一般都是用在新生代的垃圾回收器，CMS是用在老年代的垃圾回收器，他们都是多线程并发的机制，性能更好，现在一般是线上生产系统的标配组合。 G1垃圾回收器：统一收集新生代和老年代，采用了更加优秀的算法和设计机制。下面会详细介绍这个G1垃圾回收器。 GC的大概流程​ 看下图，新生代的内存一般都是分为一个Eden和两个Survivor ​ 此时系统不停的运行，然后把Eden给塞满了，此时就会触发Minor GC。进行垃圾回收时有专门的垃圾回收线程的，而且对不同的内存区域会有不同的垃圾回收器。相当于垃圾回收线程和垃圾回收器配合起来，使用自己的垃圾回收算法，对指定的内存区域进行垃圾回收。 ​ 由上图可知，垃圾回收会通过一个后台运行的垃圾回收线程来执行它具体的一个逻辑。比如针对新声代我们会用ParNew垃圾回收器进行回收，然后ParNew垃圾回收器针对新生代采用的就是复制算法来垃圾回收。 GC的时候还能继续创建新的对象吗​ 我们写好的Java系统在运行期间能不能继续在新生代里创建新的对象？假设一个场景，如下图： ​ 如果所示，如果一边垃圾回收器在想办法把Eden和Survivor2里的存活对象标记出来转移到Survivor2去，然后还在想办法把Eden和Survivor2的垃圾对象清理掉，结果这个时候系统程序还在不停的在Eden里创建新的对象。而这些新的对象很快就成了垃圾对象，有的还有人引用是存活对象。这样子就会全部乱套了，对于程序新创建的这些对象，你怎么让垃圾回收器去持续追踪这些新对象的新状态？所以，在垃圾回收过程中，同时还允许我们写的Java系统不停的运行在Eden里持续创建新的对象，目前来看是不合适的。 Stop the World​ 由上述可知，平时使用的JVM最大的痛点，就是在垃圾回收的这个过程。因为在垃圾回收的时候，尽可能让垃圾回收器专心致志的干活，不能随便让我们写的Java系统继续创建对象了，所以此时JVM会在后台直接进入“stop the World”状态。即，它会直接停止我们写的Java系统的所有工作线程，让我们写的代码不再运行，然后让垃圾回收线程可以专心致志地进行垃圾回收的工作。 ![stop the world](JVM之垃圾回收器/stop the world.png) ​ 这样的话，就可以让我们的系统暂停运行，然后不再创建新的对象，同时让垃圾回收线程尽快完成垃圾回收的工作，就是标记和转移Eden和Survivor2的存活对象到Survivor1中去，然后尽快一次性地回收掉Eden和Survivor2中的垃圾对象。一旦垃圾回收完毕，既可以继续恢复我们写的Java系统的工作线程了，然后我们的那些代码就可以继续运行，继续在Eden中创建新的对象。 Stop the World造成的系统停顿​ 现在大家清楚了“Stop the World”对系统造成的影响了，假设我们的Minor GC要运行100ms，那么可能会导致我们系统直接停顿100ms不能处理任何请求。如果因为内存分配不合理，导致对象频繁进入老年代，平均七八分钟一次Full GC，而Full GC是最慢的，有的时候弄不好一次回收要运行几秒钟，甚至是几分钟都是有可能的。 ​ 因此，无论是新生代GC还是老年代GC，都尽量不要让频率过高，也避免持续时间过长，避免影响系统正常运行，这也是使用JVM过程中一个最需要优化的地方，也是最大的一个痛点。 新生代垃圾回收器：ParNew​ 一般来说，假设没有最新的G1垃圾回收器的话，大家线上系统都是ParNew垃圾回收器作为新生代的垃圾回收器。 ​ 新生代的ParNew垃圾回收器主打的是多线程垃圾回收机制。另外一种Serial垃圾回收器主打的是单线程垃圾回收，他们两都是回收新生代的，唯一的区别就是单线程和多线程的区别，但是垃圾回收算法都是一致的。 ​ 如下图，ParNew垃圾回收器如果一旦在合适的时期执行Minor GC的时候，就会把系统程序的工作线程全部停掉，禁止程序继续运行创建新的对象，然后自己就用多个垃圾回收线程去进行垃圾回收，回收的机制和算法跟之前是一样的。 为线上系统指定使用ParNew垃圾回收器​ 线上系统，如果部署到Tomcat时可以在Tomcat的catalina.sh中设置Tomcat的JVM参数，使用Spring Boot也可以在启动时指定JVM参数。 ​ 在启动系统的时候，使用-XX:+UseParNewGC选项，就可以对系统指定使用ParNew垃圾回收器。那么Minor GC的时机，检查机制，包括垃圾回收的具体过程，以及对象升入老年代的机制，都是我们之前说的那套原理了，只不过，ParNew会使用多个线程来进行垃圾回收。 ParNew垃圾回收器默认情况下的线程数量​ 因为现在一般我们不熟系统的服务器都是多核CPU，所以为了在垃圾回收的时候充分利用多核CPU的资源，一旦我们指定了使用ParNew垃圾回收器之后，他默认给自己设置的垃圾回收线程的数量就是跟CPU的核数是一样的。 ​ 比如我们线上机器假设用的是4核CPU或者8核CPU，那么此时ParNew的垃圾回收线程数就会分别是4个线程、8个线程。 ​ 这个东西一般不用我们手动去调节，因为跟CPU核数一致的线程数量，是可以充分进行并行处理的。如果要调节ParNew的垃圾回收线程数量，可以使用-XX:ParallelGCThreads参数即可。但是一般不建议随意动这个参数。 老年代垃圾回收器：CMS​ 一般老年代我们选择的垃圾回收器是CMS，他采用的是标记整理算法，其实非常简单，就是先用之前讲过的标记方法区标记出哪些对象是垃圾对象，然后把这些垃圾对象清理掉。 先Stop the World，再垃圾回收？​ 如果先Stop the World，然后再采用“标记-整理”算法去回收垃圾。会造成系统卡死时间过长，很多相应无法处理。所以CMS垃圾回收器采取的是垃圾回收线程和系统工作线程尽量同时执行的模式来处理的。 CMS的垃圾回收过程​ CMS在执行一次垃圾回收的过程一共分为4个阶段： 初始标记 并发标记 重新标记 并发清理 初始标记​ 首先，CMS要进行垃圾回收，会先执行初始标记阶段，这个阶段会让系统的工作线程全部停止，进入“Stop the World”状态。而所谓的“初始标记”，就是标记出来所有GC Roots直接引用的对象。例如下面的代码： 12345678public class Kafka &#123; public static ReplicaManager replicaManager = new ReplicaManager();&#125;public class ReplicaManager &#123; private ReplicaFetcher replicaFetcher = new ReplicaFetcher();&#125; ​ 在初始标记阶段，仅仅过通过“replicaManager”这个类的静态变量代表的GC Roots，去标记出他直接引用的ReplicaManager对象，这就是初始标记的过程。它不会去管ReplicaFetcher这种对象，因为ReplicaFetcher对象是被ReplicaManager类的“replicaFetcher”实例变量引用的。之前说过，方法的局部变量和类的静态变量是GC Roots。但类的实例变量不是GC Roots。 ​ 所以第一个阶段，初始标记，虽然要造成“Stop the World”暂停一切工作线程，但是其实影响并不大，因为他的速度很快，仅仅标记GC Roots直接引用的那些对象而已。 并发标记​ 第二个阶段是并发标记，这个阶段会让系统线程可以随意创建各种对象，继续运行。在运行期间可能会创建新的存活对象，有可能让部分存活对象失去引用，变成垃圾对象。在这个过程中，会尽可能地对已有的对象进行GC Roots追踪。 ​ 所谓进行GC Roots追踪，意思就是对类似“ReplicaFetcher”之类的全部老年代里的对象，看它被谁引用了。比如这里是被“ReplicaManager”对象的实例变量引用了，接着会看，“ReplicaManager”对象被谁引用了，会发现被“Kafka”类的静态变量引用了。那么此时可以认定“ReplicaFetcher”对象是被GC Roots间接引用的，因此此时就不需要回收它。但是在这个过程，在进行并发标记的时候，系统程序会不停的工作，它可能会创建出新的对象，部分对象可能变成为垃圾，如下图： ​ 第二个阶段，就是标记出 GC roots 关联到的对象的引用对象有哪些。比如说 A -&gt; B (A 引用 B，假设 A 是 GC Roots 关联到的对象)，那么这个阶段就是标记出 B 对象， A 对象会在初始标记中标记出来。 这个阶段其实是最耗时的，但是这个最耗时的阶段，是跟系统并发运行的，所以这个阶段不会对系统运行造成影响。 重新标记​ 在第二阶段并发标记中，因为一边标记存活对象和垃圾对象，一边系统不停运行创建对象，让老对象变成垃圾。所以第二阶段结束之后，会有很多存活对象和垃圾对象，是之前第二阶段没标记出来的。所以此时进入第三阶段，要继续让系统程序停下来，再次进入“Stop the World”状态。然后重新标记下在第二阶段里创建的一些对象，还有一些已有对象可能失去引用变成垃圾的情况。 ​ 重新标记的阶段，速度是很快的。因为它其实就是对在第二阶段中被系统程序运行变动过的少数对象进行标记，所以运行速度很快。 并发清理​ 这个阶段就是让系统程序随意运行，然后它来清理之前标记为垃圾的对象即可。这个阶段其实也很耗时，因为需要进行对象的清理，但是它也是跟随系统程序并发运行的，所以也不影响系统的执行。 CMS的垃圾回收机制性能分析​ 从上述我们知道CMS的垃圾回收机制已经尽可能地进行了性能优化。其中最耗时的，就是对老年代全部对相关进行GC Roots追踪，标记出来哪些可以回收，然后就是对各种垃圾对象从内存里清理掉。 ​ 但是他的第二和第四阶段，即并发标记和并发清理，都是和系统程序并发执行的，所以基本对性能影响不大。只有第一和第三阶段是需要“Stop the World”的，但是这两个阶段都是简单的标记而已，速度非常快，所以基本上对系统运行影响也不大。 CMS的一些细节并发回收垃圾导致CPU资源紧张​ CMS垃圾回收器有一个问题，虽然能在垃圾回收的同事让系统同事工作，但在并发标记和并发清理两个最耗时的阶段，垃圾回收线程和系统工作线程同时工作，会导致有限的CPU资源被垃圾回收线程占用了一部分。CMS默认启动的垃圾会回收线程的数量是（CPU核数 + 3）/ 4。假设是2核CPU，那么CMS会有（2 + 3）/ 4 = 1个垃圾回收线程，去占用一个CPU。所以CMS这个并发垃圾回收机制，第一个问题就是会消耗CPU资源。 Concurrent Mode Failure问题​ 在并发清理阶段，CMS只不过是回收之前标记好的垃圾对象。但是这个阶段系统一直在运行，可能会随着系统运行让一些对象进入老年代，同时还变成垃圾对象，这种垃圾对象被称为为“浮动垃圾”。 ​ 虽然它成为了垃圾，但是CMS只能回收之前标记出来的垃圾对象，不会回收它们，需要等待到下一次GC的时候才会回收它们。所以为了保证CMS垃圾回收期间，还有一定的内存空间让一些对象可以进入老年代，一般会预留一些空间。CMS垃圾回收的触发时机，其中有一个就是当老年代内存占用达到一定比例了，就会自动执行GC。 ​ -XX:CMSInitiatingOccupancyFaction参数可以用来设置老年代占用多少比例的时候触发CMS垃圾回收，JDK1.6默认的值是92%。即老年代占用了92%的空间了，就自动进行CMS垃圾回收，预留8%的空间给并发回收期间，系统程序把一些新对象放入老年代中。 ​ 如果CMS垃圾回收期间，系统程序要放入老年代的对象大于可用内存空间，这个时候，会发生Concurrent Mode Failure，就是说并发垃圾回收失败了，我一边回收，你一边把对象放入老年代，内存都不够了。 ​ 此时就会自动用“Serial Old”垃圾回收器替代CMS，就是直接把系统程序“Stop the World”，重新进行长时间的GC Roots追踪，标记出全部垃圾对象，不允许新的对象产生，然后一次性把垃圾对象都回收掉，完事了再恢复系统线程。 ​ 所以在生产实践中，这个自动触发CMS垃圾回收的比例需要合理优化一下，避免“Concurrent Mode Failure”问题。 内存碎片问题​ 老年代的CMS采用“标记-清理”算法，每次都是标记出来垃圾对象，然后一次性回收掉。这样会导致大量的内存碎片产生。如果内存碎片太多，会导致后续对象进入老年代找不到可用的连续内存空间，然后就触发Full GC。所以CMS不是完全仅仅用“标记-清理”算法的，因为太多的内存碎片实际上会导致更加频繁的Full GC。 ​ CMS有一个参数是-XX:+UseCMSCompactAtFullCollection，默认是打开的，意思是在Full GC之后要再次进入“Stop the World”，停止工作线程，然后进行碎片整理，就是把存活对象挪到一起，空出来大片连续内存空间，避免内存碎片。还有一个参数时-XX:CMSFullGCsBeforeCompaction,这个意思是执行多少次Full GC之后再执行一次内存碎片整理的工作，默认是0，意思是每次Full GC之后都会进行一次内存整理。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM之垃圾回收]]></title>
    <url>%2FCKING.github.io%2F2019%2F10%2F18%2FJVM%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[本节思维导图 ​ 在Java中，平时我们系统运行创建的对象都是优先分配在新生代里的。如果新生代里的对象越来越多，都快满了，此时就会触发垃圾回收，把新生代没有引用的对象给回收掉，从而释放内存空间。现在我们来看看，JVM是按照什么规则来回收垃圾对象的。 哪些引用对象不能被回收​ JVM中使用了可达性分析算法来判定哪些对象是可以被回收的。这个算法的意思是，对每个对象，都分析一下有谁在引用他，然后一层一层往上去判断，看是否有一个GC Roots。其中方法的局部变量、类的静态变量都可以看做是一种GC Roots。 案例​ 如下一段代码，就是在一个方法中创建了一个对象，然后有一个局部变量引用了这个对象。 12345678910public class Kafka &#123; public static void main(String[] args) &#123; loadReplicaFromDisk(); &#125; public static void loadReplicaFromDisk() &#123; ReplicaManager replicaManager = new ReplicaManager(); &#125;&#125; ​ 分析代码可知，“main()”方法的桢栈入栈，然后调用“loadReplicaFromDisk()”方法，桢栈入栈，接着让局部变量“replicaManager”引用堆内存里的“ReplicaManager”实例对象，如下图： ​ 现在上图中“ReplicaManager”对象被局部变量给引用了，此时一旦新生代满了，发生垃圾回收，就会分析这个“ReplicaManager”对象的可达性。此时会发现它是不能被回收的，因为它被人引用了，而且是局部变量“replicaManager”引用的。 ​ 只要一个对象被局部变量引用了，那么说明它有一个GC Roots，此时就不能被回收了。 ​ 另外一种情况，如下面代码： 1234public class Kafka &#123; public static ReplicaManager replicaManager = new ReplicaManager();&#125; ​ 跟上面的那个一样，一分析，发现“ReplicaManager”对象被Kafka类的一个静态变量”replicaManager”给引用了，此时就不会去回收它。 ​ 总结：只要你的对象被方法的局部变量、类的静态变量给引用了，就不会回收它们。 Java中对象不同的引用类型​ 关于引用和垃圾回收的关系，我们要有一个概念，就是Java里有不同的引用类型。分别是强引用、软引用、弱引用和虚引用。 强引用​ 强引用，就是类似下面的代码： 1234public class Kafka &#123; public static ReplicaManager replicaManager = new ReplicaManager();&#125; ​ 这个就是最普通的代码，一个变量引用一个对象。只要是强引用的类型，那么垃圾回收的时候就绝对不会去回收这个对象的。 软引用​ 软引用，类似下面的代码： 12345public class Kafka &#123; public static SoftReference&lt;ReplicaManager&gt; replicaManager = new SoftReference&lt;ReplicaManager&gt;(new ReplicaManager());&#125; ​ 就是把“ReplicaManager”实例对象用一个“SoftReference”软引用类型的对象给包裹起来，此时这个“replicaManager”变量对“ReplicaManager”对象的引用就是软引用了。 ​ 正常情况下垃圾回收时不会回收软引用对象的，但是如果进行垃圾回收之后，发现内存空间还不不够存放新的对象，此时就会把这些软引用对象给回收了。即便它被变量引用了，但是因为它是软引用，所以还是可以回收的。 弱引用​ 弱引用，类似下面代码： 12345public class Kafka &#123; public static WeakReference&lt;ReplicaManager&gt; replicaManager = new WeakReference&lt;ReplicaManager&gt;(new ReplicaManager());&#125; ​ 弱引用就跟没有引用是类似的，如果发生垃圾回收，就会把这个对象回收掉。 虚引用​ 虚引用，正如其名，对一个对象而言，这个引用形同虚设，有和没有一样。此外，虚引用必须和引用队列一起使用。 finalize()方法的作用​ 从上面可知，有GC Roots引用的对象不能回收，没有GC Roots引用的对象可以别回收。如果有GC Roots引用，但是引用时软引用或者弱引用，也有可能被回收。 ​ 但是没有GC Roots引用的对象，一定会被立马回收吗？其实并不是，这里有一个finalize()方法可以抢救一下。如下代码： 123456789public class ReplicaManager &#123; public static ReplicaManager instance; @Override protected void finalize() throws Throwable &#123; ReplicaManager.instance = this; &#125;&#125; ​ 如果有一个ReplicaManager对象要被垃圾回收了，那么假如这个对象重写了Object类中的finalize()方法。此时会先尝试调用它的finalize方法，看是否把这个实例对象给了某个GC Roots变量，比如上面代码就给了ReplicaManager类的静态变量。这样就重新让某个GC Roots变量引用了自己，那么就不用被垃圾回收了。 垃圾回收算法标记-清除算法​ 改算法会从每个GC Roots出发，依次标记没有引用关系的对象，最后将没有被标记的对象清除。但是这种算法会带来大量的空间碎片，导致需要分配一个较大连续空间时容易触发full GC. 标记-整理算法​ 为了解决“标记-清除”算法导致的大量内存碎片问题，又提出了“标记-整理算法”。改算法类似计算机的磁盘整理，首先会从GC Roots出发标记存活的对象，然后将存活的对象整理到内存空间的一端，形成连续的已使用空间，最后把已使用空间之外的部分全部清除掉，这样就不会产生空间碎片的问题。 复制算法​ 为了能够并行地标记和整理，将空间分为两块，每次只激活其中一块，垃圾回收时只需把存活的对象复制到另一块未激活的空间上，将未激活空间标记为已激活，将已激活空间标记为未激活，然后清除原空间中的原对象。两块空间就这么重复循环使用。复制算法现作为主流的YGC算法进行新生代的垃圾回收。 JVM中对复制算法的优化​ 在实际真正的复制算法中，把新生代内存区域划分为三块：1个Eden区，2个Survivor区。其中Eden区占80%内存空间，每一块Survivor区各占10%内存空间。平时可以使用的，就是Eden区和其中一块Survivor区。但是刚开始对象都是分配在Eden区的，如果Eden区满了吗，此时就会触发YGC。 ​ 此时就会把Eden区中的存活对象都一次性转移到空着的Survivor区，接着Eden区就会被清空，然后再次分配新对象到Eden区。这就就会变成Eden区和Survivor区里都是有对象的，其中Survivor区里放的是上一个YGC存活后的对象。 ​ 这么设计会始终保持一个Survivor区的空着的，就这样一直循环只用这三块内存区域。这么最最大的好处是，只有10%的空间时被闲置的，90%的内存都被用上了。 老年代和新生代怎样变成老年代​ 对象一般都先分配在新生代，但什么情况下新生代会变成老年代呢？ 躲过15次GC之后进入老年代​ 一般情况下，我们系统刚启动的时候，创建的各种各样的对象，都是分配在新生代里的。然后系统跑着跑着，新生代就满了，此时就会触发Minor GC，可能就是1%的少量存活对象转移到空着的Survivor区中。然后系统继续运行，继续在Eden区了分配各种对象。大概就是这个流程。 ​ 但那些每次在新生代里躲过一次GC被转移到一块Survivor区域中，它的年龄就会增长一岁。默认情况下，当对象的年龄达到15岁时，也就是躲过15次GC的时候，它就会转移到老年代里去。具体是多少岁进入老年代，可以通过参数-XX:MaxTenuringThreshold来设置，默认是15岁。 动态对象年龄判断​ 这里跟这个对象年龄有另外一个规则可以让对象进入老年代，不用等待15次GC过后才可以。大致的规则是：假如当前放对象的Survivor区域里，一批对象的总大小大于了这块区域的内存大小的50%，那么此时大于等于这批对象年龄的对象，就可以直接进入老年代了。 ​ 假设图里的Survivor2区有两个对象，这两对象的年龄一样，都是2岁。然后这两对象加起来超过了50MB，超过了Survivor2区的100MB内存的一半了，这个时候，Survivor2区里的大于等于2岁的对象，就要全部进入老年代里去。 ​ 这就是所谓的动态年龄判断的规则。实际上这个规则运行的时候是如下的逻辑：年龄1 + 年龄2 + 年龄n的多个年龄对象总和超过了Survivor区域的50%，此时就会把年龄n以上的对象都放入老年代。 大对象直接进入老年代​ 有一个JVM参数，就是-XX:PretenureSizeThreshold，可以把他的值设置为字节数，比如“1048576”，就是1MB。它的意思是，如果你要创建一个大于等于这个大小的对象，比如一个超大的数组，此时就直接把这个大对象放到老年代去，不会经过新生代。 ​ 这么做的原因，就是要避免新生代里出现那种大对象，然后屡次躲过GC，还得把它在两个Survivor区域里来回复制多次之后才能进入老年代，那么大的对象在内存里来回复制，浪费时间。 Minor GC后的对象太多无法放入Survivor区怎么办​ 如果Minor GC后的对象太多无法放入Survivor，那么这个时候就必须把这些对象直接转移到老年代中去。 老年代空间分配担保规则 在执行任何一次Minor GC之前，JVM会先检查一下老年代可用的内存空间，是否大于新生代所有对象的总大小。为什么检查这个，因为极端情况下，可能新生代Minor GC过后，所有对象都存活下来。 如果老年代的内存大小是大于新生代所有对象的，此时就可以放心大胆的对新生代发起一次Minor GC。因此即使Minor GC之后所有对象存活，Survivor区放不下，也可以转移到老年代去。 如果老年代的可用内存已经小于新生代的全部对象了，就去看-XX:-HandlePromotionFailure参数是否设置。 如果设置了，就看老年代的内存大小，是否大于之前每一次Minor GC后进入老年代的对象的平均大小。例如，之前每次Minor GC后，平均都有10MB左右的对象会进入老年代，那么此时老年代可用内存大于10MB。很可能这次Minor GC过后也是差不多10MB左右的对象会进入老年代，此时老年代空间是够的。 如果判断失败，或者是-XX:-HandlePromotionFailure参数没设置，此时就会触发一次“Full GC”，就是对老年代进行垃圾回收，尽量腾出一些内存空间，然后再执行Minor GC。 如果上面两个步骤都判断成功了，那就可以冒风险尝试一下Minor GC。此时进行Minor GC有几种可能。 第一种可能，Minor GC过后，剩余存活的对象的大小小于Survivor区的大小，那么此时存活对象进入Survivor区域即可。 第二种可能，Minor GC过后，剩余的存活对象的大小，大于Survivor区域的大小，但是小于老年代可用内存大小，此时直接进入老年代。 第三种可能，Minor GC过后，剩余的存活对象的大小，大于Survivor区域的大小，也大于老年代可用内存的大小。此时老年代都放不下这些存活对象了，就会发生“Handle Promotion Failure”的情况，这个时候就会触发一次“Full GC”。Full GC就是对老年代进行垃圾回收，同时一般也会对新生代进行垃圾回收。 如果Full GC之后，老年代还是没有足够的空间存放Minor GC过后的剩余存活对象，那么此时就会导致所谓的“OOM”内存溢出。 老年代垃圾回收算法​ 通过上面的内容，可以总结一句话：对老年代触发垃圾回收时机，一般就是两个： 在Minor GC之前，检查发现很可能Minor GC之后要进入老年代的对象太多了，老年代放不下，此时需要提前触发Full GC然后再带着进行Minor GC。 在Minor GC之后，发现剩余对象太多，老年代内存不够。 ​ 那么对老年代进行垃圾回收采用的是什么算法呢？简单地说，就是上面提到过的标记-整理算法。但是，这个老年代的垃圾回收算法的速度比新生代的垃圾回收算法的速度慢10倍。如果系统频繁出现老年代的Full GC，会导致系统性能被严重影响，出现频繁卡顿的情况。 ​ 其实， 所谓JVM优化，就是尽可能让对象都在新生代里分配和回收，尽量别让太多对象频繁进入老年代，避免频繁对老年代进行垃圾回收，同时给系统充足的内存大小，避免新生代频繁的进行垃圾回收。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列之RocketMQ]]></title>
    <url>%2FCKING.github.io%2F2019%2F10%2F10%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BRocketMQ%2F</url>
    <content type="text"><![CDATA[​ RocketMQ是阿里开源并贡献给Apache基金会的一款分布式消息平台，具有低延迟。高性能和可靠性、万亿级容量和灵活的可伸缩性的特点。单机也可以支持亿级的消息堆积能力、单机写入TPS单实例越7万条/秒，单机部署3个Broker，最高可以跑到12万条/秒。 RocketMQ的基本构成​ 整个RocketMQ消息系统主要由4个部分组成。 ​ 从中间件服务角度来看整个RocketMQ消息系统（服务端）主要分为：NameSrv和Broker两个部分。 NameSrv​ 在RocketMQ分布式消息系统中，NameSrv主要提供两个功能： 提供服务发现和注册。这里主要是管理Broker，NameSrv接受来自Broker的注册，并通过心跳机制来检测Broker服务的健康性。 提供路由功能。集群（这里是指以集群方式部署的NameSrv）中的每个NameSrv都保存了Broker集群（这是是指以集群方式部署的Broker）中整个的路由信息和队列信息。这里需要注意，在NameSrv集群中，每个NameSrv都是相互独立的，所以每个Broker需要连接所有的NameSrv，每创建一个新的topic都要同步到所有的NameSrv上。 Broker​ 主要是负责消息的存储、传递、查询以及高可用（HA）保证等。其由如下几个子模块构成： remoting：是Broker的服务入口，负责客户端的接入（Producer和Consumer）和请求处理。 client：管理客户端和维护消费者对于Topic的订阅。 store：提供针对存储和消息查询的简单的API（数据存储在物理磁盘）。 HA：提供数据在主从节点间同步的功能特性。 Index：通过特定的key构建消息索引，并提供快速的索引查询服务。 ​ 而从客户端的角度看主要有：Producer、Consumer两个部分。 Producer​ 消息的生产者，由用户进行分布式部署，消息有Producer通过多种负载均衡模式发送到Broker集群，发送低延时，支持快速失败。 Consumer​ 消息的消费者，也有用户部署，支持PUSH和PULL两种消费模式，支持集群消费和广播消费，提供实时的消息订阅机制，满足大多数消费场景。 RocketMQ的其他概念Producer Group​ 相同角色的生产者被组织到一起。在事务提交后，生产组中不同实例都可以连接broker执行提交或回滚事务，以防原生产者在提交后就挂掉。 Consumer Group​ 具有完全相同角色的消费者被组合在一起并命名为消费者组，消费群体是一个很好的概念，它在消费信息方面实现负载平衡和容错目标是非常容易的。另外，消费者组的消费者实例必须具有完全相同的主题订阅。 Topic​ 主题是生产者提供消息和消费者提取消息的类别。主题与生产者和消费者的关系非常松散，具体而言，一个主题可能有零个，一个或多个向其发送消息的生产者；相反，生产者可以发送不同主题的信息。从消费者的角度来看，一个主题可能有零个，一个或多个消费者群体订阅。同样，一个消费群体可以订阅一个或多个主题，只要这个群体的实例保持其订阅的一致性即可。 Message​ 消息是要传递的信息。一条信息必须要有一个主题，可以将其解释为要发送给您的信件的地址。一条消息也可能有一个可选标签和额外的键值对。例如，你可以为消息设置业务密钥，并在Broker上查找消息以在开发期间诊断问题。 Message Queue​ 主题被划分为一个或多个子主题，这就是消息队列。 Tag​ 标签可以理解为更细一级的主题，为使用者提供更灵活的查找。使用标签，来自同一业务模块的具有不同目的消息可能就有相同的主题和不同的标签。标签将有助于保持代码的清洁和一致性，并且标签还可以方便RocketMQ提供的查询系统。 Message Order​ 当使用DefaultMQPushConsumer时，你可能需要决定消费是顺序的还是并发的。 Orderly（顺序）：有序的消息意味着消息的使用顺序与生产者为每个消息队列发送的顺序相同。如果你的使用场景要求是必须顺序的，你要确保只用一个队列存放消息。如果消费顺序被指定，最大的消费并发数就是这个消费者组的消息队列的订阅数。 Concurrently（并发）：并发使用消息时，消费消息的最大并发性仅受限于为每个消费者客户端指定的线程池。 安装RocketMQ1、下载Apache最新rocketmq二进制压缩文件​ 可以到官网下载后上传到服务器上，也可以用wget命令。 1wget https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.5.2/rocketmq-all-4.5.2-bin-release.zip/ 2、解压安装​ 使用unzip命令进行解压。 1unzip -d /usr/local rocketmq-all-4.5.2-bin-release.zip 3、环境变量​ 配置环境变量 vi /etc/profile。添加如下代码： 1export NAMESRV_ADDR=127.0.0.1:9876 4、启动RocketMQ​ 进入rocketmq的bin目录，修改runserver.sh，如下代码： 1JAVA_OPT="$&#123;JAVA_OPT&#125; -server -Xms8g -Xmx8g -Xmn4g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m" ​ 主要根据自己机器内存酌情修改-Xms -Xmx -Xmn这几个参数的内存。 ​ 同理修改bin目录下的runbroker.sh的JVM参数。 ​ 进入conf目录，修改broker.conf，新增一行： 1brokerIP1=xx.xx.xx.xx # 你的公网IP ​ 然后开始启动mqnamesrv，进入到rocket目录，执行nohup sh bin/mqnamesrv &amp;启动namesrv，然后再执行nohup sh bin/mqbroker -n localhost:9876 -c conf/broker.conf &amp;启动broker。 ​ 执行jps，看namesrv和broker是否启动成功，如果没成功。可以通过执行tail -f ~/logs/rocketmqlogs/namesrv.log和tail -f ~/logs/rocketmqlogs/broker.log查看相应日志。 RocketMQ集群模式​ RocketMQ集群部署有多种方式，对于NameSrv来说可以同时部署多个节点，并且这些节点间也不需要有任何的信息同步，因为这里每个NameSrv节点都会存储全量路由信息。在NameSrv集群模式下，每个Broker都需要同时向集群中的每个NameSrv节点发送注册信息，所以这里对于NameSrv的集群部署来说并不需要做什么额外的设置。 ​ 而对于Broker集群来说就有多种模式了，主要有如下几个模式： 单个Master模式​ 一个Broker作为主服务，不设置任何Slave，这种方式风险比较大，存在单节点故障会导致整个基于消息的服务挂掉，所以生产环境不可能采用这种模式。 多Master模式​ 这种模式的Broker集群，全是Master，没有Slave。 优点​ 配置会比较简单一些，如果单个Master挂掉或重启维护的话对应用是没有什么影响的。如果磁盘配置为RAID10（服务器的磁盘阵列模式）的话，即使在机器宕机不可恢复的情况下，由于RAID10磁盘本身的可靠性，消息也不会丢失（异步刷盘丢失少量消息，同步刷盘一条不丢），这个Broker的集群模式性能相对来说是最高的。 缺点​ 在单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前是不可以进行消息订阅的，这对消息的实时性有一些影响。 多Master多Slave模式（异步复制）​ 这种模式下的Broker集群存在多个Master节点，并且每个Master节点都会对应一个Slave节点，有多对Master-Slave，HA（高可用）之间采用异步复制的方式进行信息同步，在这种方式下主从之间会有短暂的毫秒级的消息延迟。 优点​ 这种模式下即使磁盘损坏了，消息丢失的情况也非常少，因为主从之间有消息备份。并且，这种模式下的实时性也不会受影响，因为Master宕机后Slave可以自动切换为Master模式，这样Consumer仍然可以通过Slave进行消息消费，而这个过程对应用来说是完全透明的，并不需要人工干预。另外，这种模式的性能与多Master模式几乎差不多。 缺点​ 如果Master宕机，并且在磁盘损坏的情况下，会丢失少量的消息。 多Master多Slave模式（同步复制）​ 这种模式与上面那个差不多，只是HA采用的是同步双写的方式，即主备都写成功后，才会向应用返回成功。 优点​ 这种模式下数据与服务不存在单点的情况，在Master宕机的情况下，消息也没有延迟，服务的可用性以及数据的可用性都非常高。 缺点​ 性能相比于异步复制略低一些（大约10%）。 SpringBoot整合RocketMQ​ 这里主要讲解一下生产者和消费者的代码，完整的项目代码已上传到github上。 消息生产者RocketMQProvider​ 以顺序发消息为例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.example.rocketmqDemo.rocketmq;import org.apache.rocketmq.client.producer.DefaultMQProducer;import org.apache.rocketmq.client.producer.MessageQueueSelector;import org.apache.rocketmq.client.producer.SendResult;import org.apache.rocketmq.common.message.Message;import org.apache.rocketmq.common.message.MessageQueue;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Service;import org.springframework.util.StopWatch;import java.util.List;@Servicepublic class RocketMQProvider &#123; @Value("$&#123;apache.rocketmq.producer.producerGroup&#125;") private String producerGroup; @Value("$&#123;apache.rocketmq.namesrvAddr&#125;") private String namesrvAddr; public void defaultMQProducer() &#123; //生产组的名称 DefaultMQProducer producer = new DefaultMQProducer(producerGroup); //指定NameServer地址，多个地址以;隔开 producer.setNamesrvAddr(namesrvAddr); try &#123; //Producer对象在使用之前必须要调用start初始化，初始化一次即可 //注意：切记不可以在每次发送消息时，都调用start方法 producer.start(); //创建一个消息实例，包含topic、tag和消息体 //如下：topic为"TopicTest"，tag为"push" Message message = new Message("TopicTest", "push", "发送消息----".getBytes()); StopWatch stop = new StopWatch(); stop.start(); for(int i = 0; i &lt; 10; i++) &#123; SendResult result = producer.send(message, new MessageQueueSelector() &#123; @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Integer id = (Integer) arg; int index = id % mqs.size(); return mqs.get(index); &#125; &#125;, 1); System.out.println("发送相应：MsgId: " + result.getMsgId() + ".发送状态: " + result.getSendStatus()); &#125; stop.stop(); System.out.println("-----------------------发送十条消息耗时：" + stop.getTotalTimeMillis()); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; producer.shutdown(); &#125; &#125;&#125; ​ 一般消息时通过轮询所有队列来发送的（负载均衡策略），顺序消息可以根据业务发送到同一个队列。比如将订单号相同的消息发送到同一个队列。下面的代码中指定了1,1处这个值相同的获取到的队列是同一个队列。 12345678SendResult result = producer.send(message, new MessageQueueSelector() &#123; @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Integer id = (Integer) arg; int index = id % mqs.size(); return mqs.get(index); &#125;&#125;, 1); 消息消费者RocketMQConsumer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.example.rocketmqDemo.rocketmq;import org.apache.rocketmq.client.consumer.DefaultMQPushConsumer;import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyStatus;import org.apache.rocketmq.client.consumer.listener.MessageListenerConcurrently;import org.apache.rocketmq.common.consumer.ConsumeFromWhere;import org.apache.rocketmq.common.message.MessageExt;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.CommandLineRunner;import org.springframework.stereotype.Service;@Servicepublic class RocketMQConsumer implements CommandLineRunner &#123; //消费者的组名 @Value("$&#123;apache.rocketmq.consumer.PushConsumer&#125;") private String consumerGroup; @Value("$&#123;apache.rocketmq.namesrvAddr&#125;") private String namesrvAddr; public void defaultMQPushConsumer() &#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(consumerGroup); consumer.setNamesrvAddr(namesrvAddr); try &#123; //订阅PushTopic下Tag为push的消息 consumer.subscribe("TopicTest", "push"); //设置Consumer第一次启动是从头部开始消费还是队列尾部开始消费 //如果非第一次启动，那么按照上次消费的位置继续消费 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.registerMessageListener((MessageListenerConcurrently)(list, context)-&gt; &#123; try &#123; for(MessageExt messageExt : list) &#123; //输出消息内容 System.out.println("messageExt: " + messageExt); String messageBody = new String(messageExt.getBody()); //输出消息内容 System.out.println("消费响应：msgId : " + messageExt.getMsgId() + ", msgBody : " + messageBody); &#125; &#125;catch (Exception e) &#123; e.printStackTrace(); //稍后重试 return ConsumeConcurrentlyStatus.RECONSUME_LATER; &#125; //消费成功 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;); consumer.start(); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Override public void run(String... args) throws Exception &#123; this.defaultMQPushConsumer(); &#125;&#125; ​ 消费者中我们实现了CommandLineRunner接口。它的作用是让消费者在springboot启动时执行。具体可以参考CommandLineRunnable详解。 ​ 项目成功启动后，测试的结果应该是： 123456789101112131415161718192021222324252627282930313233342019-10-11 17:13:50.785 INFO 12872 --- [nio-8080-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring DispatcherServlet 'dispatcherServlet'2019-10-11 17:13:50.785 INFO 12872 --- [nio-8080-exec-2] o.s.web.servlet.DispatcherServlet : Initializing Servlet 'dispatcherServlet'2019-10-11 17:13:50.793 INFO 12872 --- [nio-8080-exec-2] o.s.web.servlet.DispatcherServlet : Completed initialization in 8 ms发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OK发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OKmessageExt: MessageExt [queueId=1, storeSize=178, queueOffset=51, sysFlag=0, bornTimestamp=1570785233301, bornHost=/61.144.97.110:2191, storeTimestamp=1570785231900, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEAFFF, commitLogOffset=28225535, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=52, CONSUME_START_TIME=1570785233370, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]messageExt: MessageExt [queueId=1, storeSize=178, queueOffset=50, sysFlag=0, bornTimestamp=1570785233233, bornHost=/61.144.97.110:2191, storeTimestamp=1570785231856, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEAF4D, commitLogOffset=28225357, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=52, CONSUME_START_TIME=1570785233370, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息----消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息----发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OKmessageExt: MessageExt [queueId=1, storeSize=178, queueOffset=52, sysFlag=0, bornTimestamp=1570785233346, bornHost=/61.144.97.110:2191, storeTimestamp=1570785231943, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEB0B1, commitLogOffset=28225713, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=53, CONSUME_START_TIME=1570785233411, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息----发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OKmessageExt: MessageExt [queueId=1, storeSize=178, queueOffset=53, sysFlag=0, bornTimestamp=1570785233387, bornHost=/61.144.97.110:2191, storeTimestamp=1570785231986, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEB163, commitLogOffset=28225891, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=54, CONSUME_START_TIME=1570785233453, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息----发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OKmessageExt: MessageExt [queueId=1, storeSize=178, queueOffset=54, sysFlag=0, bornTimestamp=1570785233430, bornHost=/61.144.97.110:2191, storeTimestamp=1570785232028, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEB215, commitLogOffset=28226069, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=55, CONSUME_START_TIME=1570785233495, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息----发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OKmessageExt: MessageExt [queueId=1, storeSize=178, queueOffset=55, sysFlag=0, bornTimestamp=1570785233472, bornHost=/61.144.97.110:2191, storeTimestamp=1570785232071, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEB2C7, commitLogOffset=28226247, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=56, CONSUME_START_TIME=1570785233538, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息----发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OKmessageExt: MessageExt [queueId=1, storeSize=178, queueOffset=56, sysFlag=0, bornTimestamp=1570785233516, bornHost=/61.144.97.110:2191, storeTimestamp=1570785232114, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEB379, commitLogOffset=28226425, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=57, CONSUME_START_TIME=1570785233580, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息----发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OKmessageExt: MessageExt [queueId=1, storeSize=178, queueOffset=57, sysFlag=0, bornTimestamp=1570785233557, bornHost=/61.144.97.110:2191, storeTimestamp=1570785232156, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEB42B, commitLogOffset=28226603, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=58, CONSUME_START_TIME=1570785233622, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息----发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OKmessageExt: MessageExt [queueId=1, storeSize=178, queueOffset=58, sysFlag=0, bornTimestamp=1570785233600, bornHost=/61.144.97.110:2191, storeTimestamp=1570785232199, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEB4DD, commitLogOffset=28226781, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=59, CONSUME_START_TIME=1570785233665, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息----发送相应：MsgId: 847F127D324818B4AAC2373225500000.发送状态: SEND_OK-----------------------发送十条消息耗时：1479messageExt: MessageExt [queueId=1, storeSize=178, queueOffset=59, sysFlag=0, bornTimestamp=1570785233642, bornHost=/61.144.97.110:2191, storeTimestamp=1570785232241, storeHost=/47.105.176.129:10911, msgId=2F69B08100002A9F0000000001AEB58F, commitLogOffset=28226959, bodyCRC=800335203, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=60, CONSUME_START_TIME=1570785233707, UNIQ_KEY=847F127D324818B4AAC2373225500000, WAIT=true, TAGS=push&#125;, body=16]]消费响应：msgId : 847F127D324818B4AAC2373225500000, msgBody : 发送消息---- 参考资料Centos7下安装Rocket RocketMQ连接异常 基于RocketMQ搭建生产级消息集群]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[外观模式]]></title>
    <url>%2FCKING.github.io%2F2019%2F10%2F08%2F%E5%A4%96%E8%A7%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[​ 在软件开发中，有时候为了完成一项较为复杂的功能，一个类需要和多个其他业务类交互，而这些需要交互的业务类经常会作为一个完整的整体出现，由于涉及的类比较多，导致使用时代码较为复杂，此时，需要一个类似服务员一样的角色，由它来负责和多个业务类进行交互，而使用这些业务类的类只需和该类交互即可。外观模式通过引入一个新的外观类来实现该功能，外观类充当了软件系统中的“服务员”，它为多个业务类的调用提供了一个统一的入口，简化了类与类之间的交互。 外观模式定义​ 外观模式要求一个子系统的外部与其内部的通信通过一个统一的外观角色进行，外观角色将客户端与子系统的内部复杂性分隔开，使得客户端只需要与外观角色打交道，而不需要与子系统内部的很多对象打交道，其定义如下： ​ 外部与一个子系统的通信通过一个统一的外观角色进行，为子系统中的一组接口提供一个一致的入口，外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。外观模式又称为门面模式，它是一种对象结构型模式。 外观模式结构图 ​ 在外观模式结构图中包含以下两种角色： Facade（外观角色）：在客户端可以调用这个角色的方法，在外观角色中可以知道相关的（一个或者多个）子系统的功能和责任，在正常情况下，它将所有从客户端发来的请求委派到相应的子系统中去，传递给相应的子系统对象处理。 SubSystem（子系统角色）：在软件系统中可以有一个或者多个子系统角色，每一个子系统可以不是一个单独的类，而是一个类的集合，它实现子系统的功能，每一个子系统都可以被客户端直接调用，或者被外观角色调用，它处理由外观类传过来的请求；子系统并不知道外观的存在，对于子系统而言，外观角色仅仅是另外一个客户端而已。 案例​ 某软件公司欲开发一个可应用与多个软件的文件加密模块，改模块可以对文件中的数据进行加密并将加密之后的数据存储在一个新文件中，具体流程包括三个部分，分别是读取源文件、加密、保存加密之后的文件。其中，读取文件和保存文件使用流来实现，加密操作通过求模运算来实现。这三个操作相互独立，为了实现代码的独立重用，让设计更符合单一职责原则，这3个操作的业务代码封装在3个不同的类中。 ​ 相关代码已上传至github上。 抽象外观类的引入​ 在标准的外观模式中，如果需要增加、删除或更换与外观类交互的子系统类，必须修改外观类或客户端的源代码，这将违背开闭原则，因此可以通过引入抽象外观类来对系统进行改进，在一定程度上解决该问题。在引入抽象外观类之后，客户端可以针对抽象外观类进行编程，对于新的业务需求，不需要修改原有外观类，而对应增加一个新的具体外观类，由新的具体外观类来关联新的子系统对象，同时通过修改配置文件来达到不修改任何源代码并更换外观类的目的。 ​ 如在上面的案例中，如果要更换原有的加密方式，换成新的加密方式。那么相应的解决思路如下图： 外观模式主要优缺点主要优点 对客户端屏蔽了子系统组件，减少了客户端所需处理的对象数目并使得子系统使用起来更加容易。通过引入外观模式，客户端代码变得很简单，与之关联的对象也很少。 实现了子系统与客户端之间的松耦合关系，这使得子系统的变化不会影响到调用它的客户端，只需要调整外观类即可。 一个子系统的修改对其他子系统没有任何影响，而且子系统内部变化也不会影响到外观对象。 只是提供了一个访问子系统的统一入口，并不影响客户端直接使用子系统类。 主要缺点 不能很好地限制客户端直接使用子系统类，如果对客户端访问子系统类做太多的限制则减少了可变性和灵活性。 如果设计不当，当增加新的子系统可能需要修改外观类的源代码，这违背了开闭原则。 外观模式适用场景 当要为访问一系列复杂的子系统提供一个简单入口时可以使用外观模式。 客户端程序与多个子系统之间存在很大的依赖。引入外挂类可以将子系统与客户端解耦，从而提供子系统的独立性和可移植性。 在层次化结构中，可以使用外观模式定义系统中每一层的入口，层与层之间不直接产生联系，而通过外观类建立联系，降低层之间的耦合度。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列之RabbitMQ]]></title>
    <url>%2FCKING.github.io%2F2019%2F10%2F07%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BRabbitMQ%2F</url>
    <content type="text"><![CDATA[​ RabbitMQ是一个消息代理，一个消息系统的媒介。它可以为你的应用提供一个通用的消息发送和接收平台，并且保证消息在传输过程中的安全。 基础概念​ RabbitMQ中除了正常的生产者消费者之外，还有一些其他的概念来支撑这样一个复杂的消息队列。 Broker​ Broker是消息服务中间件的一个服务节点，大部分情况下可以把一个Broker看成一个RabbitMQ的服务器。 ​ 上图可以看出Broker相当于一个消息服务的中央节点，而我们的消息队列核心功能也就在Broker上。 队列​ 消息都存储在队列中，下图是一个简单的模型。实际上，一个简单的消息队列服务只要有生成者、消费者和存储单元组成队列即可。 Exchange​ 交换机（Exchange）在RabbitMQ中承担了一些队列的逻辑处理功能。一般来说，对于生产者，我们只知道把产生的内容丢到MQ当中，但是发到哪个队列中，这一点对于生产者来说是无感知的，也不知道目前对列的状况如何。而Exchange就承担了发到哪个队列中的职责，用几种路由策略来决定如何分发给不同的队列。 Connection和Channel​ 每一个Connection是一条TCP连接，理论上而言每一个消费者和生产者都需要一条Connection，但是TCP连接的开销很大，所以我们会使用Channel来进行TCP复用，减少性能的开销。 Exchange类型fanout​ 我们比较常用的一种Exchange类型，它会把所有发送到该交换器的消息路由到所有与该交换器绑定的队列中。 direct​ 把消息路由到binding 可以和routing key完全匹配的队列中。 ​ binding key和routing key基本上可以理解为对一个Queue的称呼。 ​ 图中Queue1叫warning，Queue2可以叫info，warning或者debug，那样Exchange叫了声warning的时候会有两个Queue过来拿数据，而info只有Queue2会回应。 topic​ direct类型太过严格，大部分我们都用不上这么严格的规则，因此有了topic。topic可以看做是一种正则表达式规则，满足正则表达式的规则就会进入队列。 headers​ 这种类型根据发送信息中的header来匹配，性能差，基本没用。 死信队列​ DLX（Dead-Letter-Exchanage）。利用DLX，当消息在一个对列中变成死信之后，它能被重新publish到另一个Exchange，这个Exchange就是DLX。消息变成死信一般有以下几种情况： 消息被拒绝（basic.reject/basic.nack）并且不再重新投递 require = false 消息过期（rabbitmq Time-To-Live -&gt; messageProperties.setExpiration()） 队列达到最长长度 ​ 当一个消息变成死信导致队列无法处理的时候，开启死信队列，使得消息不会堆积在队列中，而换到死信队列被消费。在RabbitMQ中开启死信队列非常简单，只要配置为DLX即可。 公用Connection而不是Channel​ 公用Connection的理由在上面已经提过，那为什么不建议功能Channel呢？ ​ 计算机网络传输信息的时候，本质上都是二进制传输，而传输的数据经过一定的处理，最终变成我们可读可处理的数据，Channel已经是复用了TCP连接的，此时如果我们再进行并行的数据传输，很有可能会导致某一帧数据的异常。 RabbitMQ的高可用性​ RabbitMQ是基于主从做高可用性的。一般来说，RabbitMQ有三种模式：单机模式、普通集群模式和镜像集群模式。 单机模式​ 单机模式，就是Demo级别的，一般就是你本地启动了做做demo，基本没人生产用单机模式。 普通集群模式（无高可用性）​ 普通集群模式，意思就是在多台机器上启动多个RabbitMQ实例，每个机器启动一个。你创建的queue，只会放在一个RabbitMQ实例上，但是每个实例都同步queue的元数据（元数据可以认为是queue的一些配置信息，通过元数据，可以找到queue所在实例）。你消费的时候，实际上如果连接到了另一个实例，那么那个实例会从queue所在实例上拉取数据过来。 ​ 这种方式不仅麻烦，而且也没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例后拉取数据，要么固定连接那个queue所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。 ​ 而且如果那个放queue的实例宕机了，会导致接下来其他实例无法从那个实例拉取数据。如果你开启了消息持久化，让RabbitMQ落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个queue拉取数据。 ​ 因此这个方案主要是用来提供吞吐量的，就是让集群中多个节点来服务某个queue的读写操作，没有所谓的高可用性。 镜像集群模式（高可用性）​ 这种模式，才是所谓的RabbitMQ的高可用模式。跟普通集群模式不一样的是，你创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，就是说，每个RabbitMQ节点都有这个queue的一个完成镜像，包含queue的全部数据的意思。然后你每次写消息到queue的时候，都会自动把消息同步到多个实例的queue上。 ​ 如何开启镜像集群模式？RabbitMQ有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候可以要求数据同步到所有节点的，也可以要求同步指定数据的节点，再次创建queue的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。 ​ 这样处理，好处在于任何一个机器宕机了，其他机器还包含了这个queue的完整数据，别的consumer都可以到其他节点上去消费数据。坏处在于：第一，性能开销太大，消息需要同步到所有机器上，导致网络带宽压力和消耗都重；第二，这样处理并不是分布式的，没有扩展性可言，如果某个queue负载很重，你加机器，新的机器也包含了这个queue的所有数据，并没有办法线性扩展你的queue。 RabbitMQ的可靠性传输​ 数据的丢失问题，可能出现在生产者、MQ、消费者中，如下图： 生产者弄丢了数据​ 生产者将数据发送到RabbitMQ的时候，因为网络问题或其他情况，导致数据在半路就搞丢了。 事务机制​ 此时可以选择用RabbitMQ提供的事务功能，就是生产者发送数据之前开启RabbitMQ事务channel.txSelect，然后发送消息，如果消息没有成功被RabbitMQ接收到，那么生产者会收到异常报错，此时就可以回滚事务channel.txRollback，然后重试发送消息，如果收到了消息，那么可以提交事务channel.txCommit。 123456789101112// 开启事务channel.txSelecttry &#123; // 这里发送消息&#125; catch (Exception e) &#123; channel.txRollback // 这里再次重发这条消息&#125;// 提交事务channel.txCommit ​ 但是问题是，这样会导致吞吐量下来，因为太耗性能。 confirm机制​ 因此一般情况下，要确保RabbitMQ的消息别丢，可以开启confirm模式，在生产者那里设置开启confirm模式之后，你每次写的消息都会分配一个唯一的id，然后如果写入了RabbitMQ中，RabbitMQ会给你回传一个ack消息，告诉你这个消息OK了，如果RabbitMQ没能处理这个消息，会回调你的一个nack接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 两个机制的区别​ 事务机制和confirm机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那，但是confirm机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息RabbitMQ接收了之后会异步回调你的一个接口通知你这个消息接收到了。所以一般在生产者这块避免数据丢失，都是用confirm机制。 RabbitMQ弄丢了数据​ 即使RabbitMQ自己弄丢了数据，这个你必须开启RabbitMQ的持久化，就是消息写入之后会持久化到磁盘，哪怕是RabbitMQ自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非RabbitMQ还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率比较小。 ​ 设置持久化有两个步骤： 创建queue的时候将其设置为持久化，这样就可以保证RabbitMQ持久化queue的元数据，但是它不会持久化queue里的数据。 第二是是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时RabbitMQ就会将消息持久化到磁盘上去。 ​ 必须要同时设置这两个持久化才行，这样RabbitMQ哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。 ​ 但是哪怕是你给RabbitMQ开启了持久化机制，也有一种可能，就是这个消息写到了RabbitMQ中，但是还没来得及持久化到磁盘上，结果RabbitMQ挂了，就会导师内存里的一点点数据丢失。所以，持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产这ack了。这样即便是持久化到磁盘之前，RabbitMQ挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。 消费端弄丢了数据​ 如果消费端刚消费到消息，但还没处理，结果进程挂了，这样就尴尬了。RabbitMQ认为你都消费了，这数据也就丢了。 ​ 这个时候可以用RabbitMQ提供的ack机制，即，你必须关闭RabbitMQ的自动ack,可以通过一个api来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里ack一把。这样的话，如果你还没处理完，就没有ack,RabbitMQ就认为你还没处理完，这个时候RabbitMQ会把这个消费分配给别的consumer去处理，消息是不会丢的。]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis在实践中的一些常见问题与优化思路]]></title>
    <url>%2FCKING.github.io%2F2019%2F10%2F06%2FRedis%E5%9C%A8%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[fork耗时操作导致高并发请求延时​ Redis在开启RDB和AOF持久化机制的时候，会有生成RDB快照，AOF rewrite等耗费磁盘IO的操作，此时主进程会fork子进程去执行。fork的时候，子进程需要拷贝父进程的空间内存页表的，这个也会耗费一定的时间。一般来说，如果父进程内存有1个G的数据，那么fork可能会耗费20ms左右，如果是10G30G，那么就会耗费200600ms，也就是几百毫秒的时间。可以在redis中执行info stats，其中的latest_fork_usec，可以看到最近一次form的时长。 ​ 如果redis单机QPS是几万，fork可能一下子就会拖慢几万条操作的请求时长。 优化思路​ fork耗时跟redis主进程的内存有关系，一般控制redis的内存在10GB以内。 AOF的阻塞问题​ redis将数据写入AOF缓冲区，然后会每秒做一次fsync。但是redis主线程会检查两次fsync的时间，如果距离上次fsync时间超过了2秒，那么写请求就会阻塞。这样可以保证redis最多丢失2秒的数据，但一旦fsync超过2秒的延时，整个redis就会被拖慢。 优化思路​ 优化硬盘写入速度，建议采用SSD，不要用普通的机械硬盘。 主从复制延迟问题​ 主从复制可能会超时严重，这个时候需要良好的监控和报警机制，在redis中执行info replication，可以见到master和slave复制的offset，做一个差值就可以看到对应的延迟量，如果延迟过多，那么久进行报警。 主从复制风暴问题​ 如果一下子让多个slave从master去执行全量复制，一份大的RDB同时发送到多个slave，会导致网络带宽被严重占用。如果一个master需要挂载很多个slave，那么尽量用树状结构，不要用星型结构。 Linux系统内核的优化​ 不同版本的Linux系统设置可能不一样，以下的内容只是提供一个思路，具体命令请根据不同的版本号自行百度。 vm.overcommit_memory​ 执行cat /proc/sys/vm/overcommit_memory，默认情况会返回0。这些数字代表的意义如下： 0：检查有没有足够内存，没有的话申请内存失败 1：允许使用内存直到用完为止 2：内存地址空间不能超过swap + 50% ​ 如果是0的话，可能导致类似fork等操作执行失败，申请不到足够的内存空间。可以将该参数设置为1。可以先后执行echo &quot;vm.overcommit_memory=1&quot; &gt;&gt; /etc/sysctl.conf和sysctl vm.overcommit_memory=1。 swapiness​ 执行cat /proc/version，查看系统内核版本。 ​ 如果Linux内核版本&lt;3.5，那么swapiness设置为0，这样系统宁愿swap也不会oom killer（杀掉进程） ​ 如果Linux内核版本&gt;=3.5，那么swapiness设置为1，这样系统宁愿swap也不会oom killer。 ​ 这样可以保证redis不会被杀掉。 12echo 0 &gt; /proc/sys/vm/swappinessecho vm.swapiness=0 &gt;&gt; /etc/sysctl.conf 最大打开文件句柄​ ulimit -n 10032 10032 tcp backlog​ 开始之前我们先回忆一下TCP建立连接的三次握手： Client发出一个数据包并将SYN置1，表示希望建立连接。这个包中的序列号假设是x。并将状态修改为SYN_SENT。 Server抽到Client发过来的数据包后，通过SYN得知这是一个建立连接的请求。于是发送一个响应包并将SYN和ACK都置1。假设这个包中的序列号是y，而确认序列号必须是x+1，表示收到了A发过来的SYN。并将自己的状态修改为SYN_RCVD，并把该请求放到syns queue队里中。 Client收到Server的响应包后进行确认，确认包中将ACK置1，并将确认序列号设置为y+1，表示收到了B的SYN。此时将状态修改为ESTABLISHED Server收到ACK后，将状态修改为ESTABLISHED，并把该请求从syns queue中放到accept queue。 ​ 在Linux系统内核中维护了两个队列：sync queue和accept queue。 sync queue：用于保存半连接状态的请求，其大小通过/proc/sys/net/ipv4/tcp_max_syn_backlog指定，一般默认值是512，不过这个设置有效的前提是系统的syncookies功能被禁用。互联网常见的TCP SYN FLOOD恶意DOS攻击方式就是建立大量的半连接状态的请求，然后丢弃，导致syns queue不能保存其它正常的请求。 accept queue：用于保存全连接状态的请求，其大小通过/proc/sys/net/core/somaxconn指定，在使用listen函数时，内核会根据传入的backlog参数与系统参数somaxconn，取二者的较小值。 如果accpet queue队列满了，server将发送一个ECONNREFUSED错误信息Connection refused到client。 ​ 这个方案就是调大accept queue的大小，其默认值是128，我们可以将其设置为511. 12cat /proc/sys/net/core/somaxconnecho 511 &gt; /proc/sys/net/core/somaxconn]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始搭建Redis集群]]></title>
    <url>%2FCKING.github.io%2F2019%2F10%2F06%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BARedis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[​ Redis Cluster集群，要求至少3个master去组成一个高可用，健壮的分布式的集群，每个master都建议至少给一个slave，因此，最少要求3个master，3个slave。在正式环境中，建议在6台机器上搭建。也可是3台，但要保证每个master都跟自己的slave不在同一台机器上。 搭建Redis​ 搭建Redis Cluster之前，我们需要先搭建redis。先去官网中下载一个redis，目前稳定版本是redis-5.0.5。下载完后将其上传到Linux系统上，我一般放在/usr/local目录中。 上传完成后，执行tar -zxvf redis-5.0.5.tar.gz解压 执行cd redis-5.0.5/进入redis目录 执行make &amp;&amp; make test &amp;&amp; make install 编译过程中会报一个错误You need tcl 8.5 or newer in order to run the Redis test，说明我们需要安装tcl才能安装redis。执行yum install tcl进行安装 安装好之后再次执行make &amp;&amp; make test &amp;&amp; make install进行编译，需要等待一定的时间。等他编译好之后，有可能会报一个warning，这个可以不用管它。 这样我们就把redis装好了。执行ls查看redis文件夹下的文件 生产环境的redis启动方案​ 在生产环境中，要把redis作为一个系统的daemon进程去执行，每次系统启动，redis进程也一起启动。 在redis_util目录中，有个redis_init_scipt脚本。将这个脚本拷贝到Linux的/etc/init.d目录中，将redis_init_script重命名为redis_6379,6379使我们希望这个redis实例监听的端口号。cp redis_init_script /etc/init.d/redis_6379 修改redis_6379脚本的REDISPORT，设置为6379（默认就是6379）。 创建两个目录：/etc/redis（存放redis的配置文件），/var/redis/6379（存放redis的持久化文件）。 修改redis配置文件，默认在根目录下，拷贝到/etc/redis目录中，修改名称为6379.conf。 修改redis.conf的部分配置： daemonize yes pidfile /var/run/redis_6379.pid # 设置redis的pid文件位置 port 6379 #设置redis的监听端口号 dir /var/redis/6379 #设置持久化文件的存储位置 启动redis 分别执行以下语句 cd /etc/init.d、chmod 777 redis_6379、./redis_6379 start 执行ps -ef | grep redis确认redis进程是否启动。 在redis跟随系统启动自动启动。在redis_6379脚本中的上面，加入两行注释，然后执行chkconfig redis_6379 on。 123# chkconfig: 2345 90 10# description: Redis is a persistent key-value database 搭建Redis Cluster​ 现在开始搭建redis集群。这个案例模拟的是在三台服务器上搭建redis cluster。在搭建redis集群的时候先关闭所有的redis实例。在搭建之前，先了解几个redis cluster的重要配置： cluster-enabled &lt;yes/no&gt; cluster-config-file 这是指定一个文件，供cluster模式下的redis实例将集群状态保存在那里，包括集群中其他机器的信息，比如节点的上线和下线，故障转移。这个不是我们去维护，只是给它指定一个文件，让redis自己去维护。 cluster-node-time 节点存活超时时长，超过一定时长，认为节点宕机，master宕机的话就会触发主备切换，salve宕机就不会提供服务。 开始搭建redis cluster 先在三台服务器上安装redis。我们需要在三个服务器上搭建六个redis实例，端口号分别为7001、7002、7003、7004、7005和7006 先在三个服务器上建两个文件件mkdir -p /etc/redis-cluster 、mkdir -p /var/log/redis，然后在服务器1建立mkdir -p /var/redis/7001和mkdir -p /var/redis/7002文件夹，再在服务器2和服务器3分别建立7003、7004和7005、7006文件夹。 修改redis的配置文件。举一个例子，将上面的6379.conf配置文件复制一份并修改为7001.conf。重点修改一下几个配置： 12345678910port 7001cluster-enabled yescluster-config-file /etc/redis-cluster/node-7001.confcluster-node-timeout 15000daemonize yes pidfile /var/run/redis_7001.pid dir /var/redis/7001 logfile /var/log/redis/7001.logbind 192.168.31.187 appendonly yes 然后再接着生成并修改7002 7003 7004 7005 7006.conf配置文件，并放在相应的服务器中。 准备生产环境的启动脚本。在服务器1中，在/etc/init.d目录下，将redis_6379启动启动脚本复制一份并命名为redis_7001，并修改里面的端口号。同样的生成7002 7003 7004 7005 7006的启动脚本，并放在相应服务器的/etc/init.d目录下。 分别在3台服务器上，启动6个redis实例。 创建集群​ 6个redis实例启动后，就开始创建集群。 随便选中一台服务器，开始安装ruby。执行yum install -y ruby yum install -y rubygems gem install redis。 安装完成后，将redis-5.0.5目录中的src目录下的redis-trib.rb拷贝到/usr/local/bin中。cp /usr/local/redis-3.2.8/src/redis-trib.rb /usr/local/bin 执行redis-trib.rb create --replicas 1 192.168.0.112:7001 192.168.0.112:7002 192.168.0.113:7003 192.168.0.113:7004 192.168.0.114:7005 192.168.0.114:7006。其中上面的IP地址就是你的机器地址。 执行后会报下面的错误，这是因为yum安装的ruby的版本太低的缘故，可以执行ruby -v查看相应的版本。 1234567redis-trib.rb:6: odd number list for Hash white: 29, ^ redis-trib.rb:6: syntax error, unexpected &apos;:&apos;, expecting &apos;&#125;&apos; white: 29, ^ redis-trib.rb:7: syntax error, unexpected &apos;,&apos;, expecting kEND 执行yum remove -y ruby和yum remove -y rubygems 从官网下载最新的ruby压缩包，上传到服务器上 解压tar -zxvf ruby-2.6.5.tar.gz，解压后进入到相应目录中，开始编译。执行./configure 、make 、 make install，并等待一段时间。 执行ruby -v查看是否安装成功。 再次执行 redis-trib.rb create --replicas 1 192.168.0.112:7001 192.168.0.112:7002 192.168.0.113:7003 192.168.0.113:7004 192.168.0.114:7005 192.168.0.114:7006。此时会抱一个WARNING。 12345678910111213141516WARNING: redis-trib.rb is not longer available! You should use redis-cli instead. All commands and features belonging to redis-trib.rb have been moved to redis-cli. In order to use them you should call redis-cli with the --cluster option followed by the subcommand name, arguments and options. Use the following syntax: redis-cli --cluster SUBCOMMAND [ARGUMENTS] [OPTIONS] Example: redis-cli --cluster create 127.0.0.1:30001 127.0.0.1:30002 127.0.0.1:30003 127.0.0.1:30004 127.0.0.1:30005 127.0.0.1:30006 --cluster-replicas 1 To get help about all subcommands, type: redis-cli --cluster help 按照example执行以下命令redis-cli --cluster create 192.168.0.112:7001 192.168.0.112:7002 192.168.0.113:7003 192.168.0.113:7004 192.168.0.114:7005 192.168.0.114:7006 --cluster-replicas 1 这样就创建好集群了，它会帮你指定好谁当master谁当slave。你查看后觉得没问题就输入yes即可。 节点的增加与删除增加master节点 先按照上述操作，新建一个端口号为7007的redis实例，并启动。 在7001服务器上执行redis-cli --cluster add-node 192.168.0.114:7007 192.168.0.112:7001，将新增的7007redis实例增加到redis cluster中。 执行redis-cli --cluster check 192.168.0.114:7007查看redis cluster的情况，可以看到7007实例已经作为master新增到redis cluster中。但这个master只有0个hash slots，所以我们还要给他分配hash slots. 因为16364 / 4 = 4096，因此需要从其他三个master中迁移总共4096个节点到7007上。在任意一台服务器上执行redis-cli --cluster reshard 192.168.0.112 7001。执行后会出现How many slots do you want to move (from 1 to 16384)?，这是询问你要迁移多少slots，我们输入4096。执行后会出现What is the receiving node ID?这是询问你要迁移到哪里去，根据上图可知7007的ID是eb9267b3f16da7317e0f13f7f42fd2f2cf0857a1，输入进行执行。然后会出现Please enter all the source node IDs. Type &#39;all&#39; to use all the nodes as source nodes for the hash slots. Type &#39;done&#39; once you entered all the source nodes IDs.。这是让我们输入数据源的redis的ID，我们输入另外3个master的ID后输入done，之后再输入yes即可。 再次执行redis-cli --cluster check 192.168.0.114:7007查看redis cluster的情况。可以看到此时7007已经有4096个slots了。 增加slave节点 先按照上述操作，新建一个端口号为7008的redis实例，并启动。 执行redis-cli --cluster add-node 192.168.0.114:7008 192.168.0.112:7001 --cluster-slave --cluster-master-id f7b8e55612bce7574deecd57827e3b8203c1c9a6。其中的master-id是7004redis的ID。意思是将7008挂载为7004的slave。 执行redis-cli --cluster 192.168.0.113:7004查看7004的情况，可以看到7008已经是7004的slave了，但之前7001本来是7004的slave，却挂载到本来没有slave的7007master上，称为7007的slave。 删除节点 删除master之前。先用reshard将数据迁移到其他节点，确保node为空后，才能执行remove操作 假设我们要删除7007节点，先执行redis-cli --cluster reshard 192.168.0.112:7001，然后输入1365，将1365个slot迁移到其中一个master中。然后再依次迁移1365和1366个slot到另外两个master中。此时7007零个slots。 执行redis-cli --cluster del-node 192.168.0.112:7001 eb9267b3f16da7317e0f13f7f42fd2f2cf0857a1,，其中那串ID是7007的ID。当你清空了一个master的hashslot时，redis cluster就会自动将其slave挂载到其他master上去，这个时候就只要删除掉master就可以了。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[命令模式]]></title>
    <url>%2FCKING.github.io%2F2019%2F09%2F29%2F%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[​ 在软件开发中有很多类似这样的一个场景：一个按钮，它可能是一个“关闭窗口”请求的发送者，而按钮点击事件处理类则是该请求的接收者。为了降低系统的耦合度，将请求的发送者和接收者解耦，可以使用一种被称之为命令模式的设计模式来设计系统。在命令模式中，发送者与接收者之间引入了新的命令对象，将发送者的请求封装在命令模式中，再通过命令对象来调用接收者的方法。 ​ 在软件开发中，经常需要向某些对象发送请求（调用其中的某个或某些方法），但是并不知道请求的接收者是谁，也不知道被请求的操作是哪个，此时，希望能以一种松耦合的方式来设计软件，使得请求发送者与请求接收者能够消除彼此之间的耦合，让对象之间的调用关系更加灵活。 ​ 命令模式可以将请求发送者和接收者完全解耦，发送者与接收者之间没有直接引用关系，发送请求的对象只需要知道如何发送请求，而不必知道如何完成请求。 命令模式定义​ 将一个请求封装为一个对象，从而可用不同的请求对客户进行参数化；对请求排队或者记录日志，以及支持可撤销的操作。命令模式是一种对象行为型模式，又称为动作模式或事物模式。 命令模式结构图​ 命令模式的核心在于引入了命令类，通过命令类来降低发送者和接收者的耦合度，请求发送者只需要指定一个命令对象，再通过命令对象来调动请求接收者的处理方法，其结果如下： ​ 由上图可知命令模式包含以下四个角色： Command（抽象命令类）：抽象命令类一般是一个抽象类或接口，在其中声明了用于执行请求execute()等方法，通过这些方法可以调用请求接收者的相关操作。 ConcreteCommand（具体命令类）：具体命令类是抽象命令类的子类，实现了在抽象命令类中声明的方法，它对应具体的接收者对象，将接收者对象的动作绑定在其中。在实现execute()方法时，将调用接收者对象的相关操作（Action）. Invoker（调用者）：调用者即请求发送者，它通过命令对象来执行请求。一个调用者并不需要在设计时确定其接收者，因此它只与抽象命令类之间存在关联关系。在程序运行时可以将一个具体命令对象注入其中，再调用具体命令对象的execute()方法，从而实现间接调用请求接收者的相关操作。 Receiver（接收者）：接收者执行与请求相关的操作，它具体实现对请求的业务处理。 具体案例​ 某公司开发人员为公司内部OA系统开发了一个桌面版应用程序，该应用程序为客户提供了一系列自定义功能键，用户可以通过这些功能键来实现一些快捷操作。产品人员通过分析，发现不同的用户可能有不同的使用习惯，在设置功能键的时候每个人都有自己的喜好，例如有人喜欢将第一个功能键设置为“打开帮助文档”，有的人则喜欢将该功能键设置为“最小化至托盘”。为了让用户能够灵活地进行功能键的设置,开发人员提供了一个“功能键设置”窗口,如图所示： ​ 通过这个窗口界面，用户可以将功能键和相应功能绑定在一起，还可以根据需要来修改功能键的设置，而且系统未来还可能增加一些新的功能或功能键。 ​ 该软件使用命令模式设计，结构图如下所示： ​ 相关代码已上传到github上。 命令队列的实现​ 有时候需要将多个请求排队，当一个请求发送者发送一个请求时，不止一个请求接收者产生响应，这些请求接收者将逐个执行业务方法，完成对请求的处理。此时，可以通过命令队列来实现。 ​ 命令队列的实现方法有多种形式，其中最常用、灵活性最好的一种方式是增加一个CommandQueue类，由该类来负责存储多个命令对象，而不同的命令对象可以对应不同的请求接收者。CommandQueue类的典型代码如下： 123456789101112131415161718192021222324package com.command;import java.util.ArrayList;public class CommandQueue &#123; //定义一个ArrayList来存储命令队列 private ArrayList&lt;Command&gt; commands = new ArrayList&lt;Command&gt;(); public void addCommand(Command command) &#123; commands.add(command); &#125; public void removeCommand(Command command) &#123; commands.remove(command); &#125; //循环调用每一个命令对象的execute()方法 public void execute() &#123; commands.stream().forEach(command -&gt; &#123; command.execute(); &#125;); &#125;&#125; ​ 在增加了命令队列类CommandQueue以后，请求发送者类Invoker将针对CommandQueue编程，代码如下： 123456789101112131415161718192021package com.command;public class Invoker &#123; //维持一个CommandQueue对象的引用 private CommandQueue commandQueue; public Invoker(CommandQueue commandQueue) &#123; this.commandQueue = commandQueue; &#125; //设值注入 public void setCommandQueue(CommandQueue commandQueue) &#123; this.commandQueue = commandQueue; &#125; //调用CommandQueue类的execute()方法 public void call() &#123; commandQueue.execute(); &#125;&#125; ​ 命令队列与批处理有点类似。批处理，意思就是可以对一组对象（命令）进行批量处理，当一个发送者发送请求后，将有一系列接收者对请求作出相应，命令对象可以用于设计批处理应用程序，如果请求接收者的接受次序没有严格的先后次序，还可以使用多线程技术来并发调用命令对象的execute()方法，从而提高程序的执行效率。 撤掉操作的实现​ 在命令模式中，可以通过调用一个命令对象的execute()方法来实现对请求的处理，如果需要撤销请求，可以通过在命令类中增加一个逆向操作来实现。此外，还可以通过保存对象的历史状态来实现撤销。 案例​ 某公司欲开发一个简易计算器，该计算器可以实现简单的数学运算，还可以对运算实施撤销操作和恢复操作。 ​ 本例完整代码已上传到github上 请求日志​ 请求日志就是将请求的历史记录保存下来，通常以日志文件的形式永久存储在计算机系统中。在实现请求日志时，可以将命令对象通过序列化写到日志文件中，此时命令类必须实现java.io.Serializable接口。 案例​ 某公司开发了一个网站配置文件管理工具，可以通过一个可视化界面对网站配置文件进行增删改等操作，该工具使用命令模式进行设计，结构如下所示。现在改公司开发人员希望对配置文件的操作请求记录在日志文件中，如果网站重新部署，主需要执行保存在日志文件中的命令对象即可修改配置文件。 ​ 该案例的相关代码已上传到github上。 命令模式主要优缺点主要优点 降低系统的耦合度。由于请求者与接收者之间不存在直接引用，因此请求者与接收者之间实现完全解耦，相同的请求者可以对应不同的接收者，同样，相同的接收者也可以供不同的请求者使用，两者之间具有良好的独立性。 新的命令可以很容易地加入到系统中。由于增加新的具体命令类不会影响到其他类，因此增加新的具体命令类很容易，无须修改原有系统源代码甚至客户类代码，满足开闭原则。 可以比较容易地设计一个命令队列或宏命令。 为请求的撤销和恢复操作提供了一种设计和实现方案。 主要缺点​ 主要缺点是：使用命令模式可能会导致某些系统有过多的具体命令类。因为针对每一个请求接收者的调用操作都需要设计一个具体命令类，因此在某些系统中可能需要提供大量的具体命令类，这将影响命令模式的使用。 命令模式使用场景 系统需要将请求调用者和请求接收者解耦，使得调用者和接收者不直接交互。请求调用者无须知道接收者的存在，也无须知道接收者是谁，接收者也无须关心何时被调用。 系统需要在不同的时间指定请求、将请求排队和执行请求。一个命令对象和请求的初始调用者可以有不同的生命期。即，最初的请求发出者可能已经不在了，而命令对象本身仍然是活动的，可以通过该命令对象去调用请求接收者，而无须关系请求调用者的存在性，可以通过请求日志文件等机制来具体实现。 系统需要支持命令的撤销操作和恢复操作。 系统需要将一组操作组合在一起形成宏命令。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[策略模式]]></title>
    <url>%2FCKING.github.io%2F2019%2F09%2F25%2F%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[​ 在软件开发中，常常会遇到这种情况：实现某个功能有多条途径，每一条途径对应一种算法，此时可以使用一种设计模式来实现灵活地选择解决途径，也能够方便地增加新的解决途径。那就是策略模式。 ​ 在策略模式中，可以定义一些独立的类来封装不同的算法，每一个类封装一种具体的算法。在这里，每一个封装算法的类都可以称之为一种策略，为了保证这些策略在使用时具有一致性，一般会提供一个抽象的策略来做规则的定义，而每种算法则对应一个具体策略类。 ​ 策略模式的主要目的是将算法的定义与使用分开，也就是将算法的行为和环境分开，将算法的定义放在专门的策略类中，每一个策略类封装了一种实现算法，使用算法的环境类针对抽象策略类进行编程，符合依赖倒转原则。在出现新的算法时，只需要增加一个新的实现了抽象策略类的具体策略类即可。 策略模式定义​ 定义了一系列算法类，将每一个算法封装起来，并让他们可以互相替换。策略模式让算法独立于使用它的客户而变化，也称为政策模式。策略模式是一种对象行为型模式。 策略模式结构图 ​ 由结构图可以看到包含以下3个角色： Context（环境类）：环境类是使用算法的角色，它在解决某个问题（即实现某个方法）时可以采用多种策略。在环境类中维持一个对抽象策略类的引用实例，用于定义所采用的策略。 Strategy（抽象类）：它为所支持的算法声明了抽象方法，是所有策略类的父类，它可以是抽象类或具体类，也可以是接口。环境类通过抽象策略中声明的方法在运行时调用具体策略类中实现的算法。 ConcreteStrategy（具体策略类）：它实现了在抽象策略类中声明的算法，在运行时，具体策略类将覆盖在环境类中定义的抽象策略类对象，使用一种具体的算法实现某个业务处理。 策略模式主要优缺点主要优点 策略模式提供了对开闭原则的完美支持，用户可以在不修改原有系统的基础上选择算法或行为，也可以灵活地增加新的算法或行为。 策略模式提供了管理相关的算法族的办法，策略类的等级结构定义了一个算法或行为族，恰当使用继承可以把公共的代码移到抽象策略类中，从而避免重复的代码。 策略模式提供了一种可以替换继承关系的办法。如果不使用策略模式，那么使用算法的环境类就可能会有一些子类，每个子类提供一种不同的算法。但是，这样一来算法的使用就和算法本身混在一起，不符合单一职责原则，决定使用哪一种算法的逻辑和该算法本身混合在一起，从而不可能再独立演化；而且使用继承无法实现算法或行为在程序运行时的动态切换。 使用策略模式可以避免多重条件选择语句。多重条件选择语句不易维护，它把采取哪一种算法或行为的逻辑与算法或行为本身的实现逻辑混合在一起，将他们全部硬编码在一个庞大的多重条件选择语句中，比直接集成环境类的办法还要原始和落后。 策略模式提供了一种算法的复用机制，由于将算法单独提取出来封装在策略类中，因此不同的环境类可以方便地复用这些策略类。 主要缺点 客户端必须知道所有的策略类，并且自行决定使用哪一个策略类。这就意味着客户端必要理解这些算法的区别，以便实时选择恰当的算法。换言之，策略模式只适用于客户端知道所有的算法或行为的情况。 策略模式将造成系统产生很多具体策略类，任何细小的变化都将导致系统要增加一个新的具体策略类。 无法同时在客户端使用多个策略类，也就是说，在使用策略模式时，客户端每次只能使用一个策略类，不支持使用一个策略类完成部分功能后再使用另一个策略类来完成剩余功能的情况。 策略模式适用场景​ 在以下情况下可以考虑使用策略模式： 一个系统需要动态地在几种算法中选择一种，那么可以将这些算法封装到一个个的具体算法类中，而这些算法类都是一个抽象算法类的子类。换言之，这些具体算法类均有统一的接口，根据里氏代换原则和面向对象的多态性，客户端可以选择使用任何一个具体算法类，并且只要维持一个数据类型是抽象算法类的对象。 一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重条件选择语句来实现。此时，使用策略模式，把这些行为转移到相应的具体策略类里面，就可以避免使用难以维护的多重条件选择语句。 不希望客户端知道复制的、与算法相关的数据结构，在具体策略类中封装算法与相关的数据结构，可以提高算法的保密性与安全性。 具体事例案例1​ Sunny软件公司为某电影院开发了一套影院售票系统，在改系统中需要为不同类型的用户提供不同的电影票打折方式，具体打折方案如下： 学生凭学生证可以享受8折优惠。 年龄在10周岁及以下的儿童可享受每张票减免10元的优惠（原始票价需大于等于20元） 影院VIP用户除享受票价半价优惠外还可以积累积分，积分累计到一定额度可以兑换电影院赠送的奖品。 ​ 改系统在将来可能还需要个根据需要引入新的打折方式。 传统的实现方案大概会如下： 123456789101112131415161718192021222324public double calculate() &#123; //学生票折后票价计算 if(this.type.equalsIgnoreCase("student")) &#123; System.out.println("学生票： "); return this.price * 0.8; &#125; //儿童票折后票价计算 else if(this.type.equalsIgnoreCase("children") &amp;&amp; this.price &gt;= 20) &#123; System.out.println("儿童票："); return this.price - 10; &#125; //VIP票折价后票价计算 else if(this.type.equalsIgnoreCase("vip")) &#123; System.out.println("VIP票："); System.out.println("增加积分！"); return this.price * 0.5; &#125; else &#123; return this.price; &#125; &#125; ​ 使用策略模式实现的代码已上传至github上。 案例2​ 使用策略模式和自定义注解去掉大量的if-else https://mp.weixin.qq.com/s/sa_MMAzYg6jpy9s_rtvcCQ]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产环境中redis的数据备份和灾难恢复策略]]></title>
    <url>%2FCKING.github.io%2F2019%2F09%2F22%2F%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%ADredis%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E5%92%8C%E7%81%BE%E9%9A%BE%E6%81%A2%E5%A4%8D%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[本节思维导图 ![生产环境中的数据 备份和灾难恢复](生产环境中redis的数据备份和灾难恢复策略/生产环境中的数据 备份和灾难恢复.png) ​ 我们生产环境中redis的数据备份和灾难恢复策略简单地说就是：开启AOF机制，并用RDB做冷备。 数据备份方案​ 具体的数据备份方案如下： 写crontab定时调度脚本去做数据备份。 每小时都备份一份rdb，可以copy到一个目录中去，并且只保留最近48小时的备份。 每天都保留一份当日的rdb备份到一个目录中去，仅仅保留最近一个月的备份。 每次copy备份的时候，把最旧的备份删掉。 每天晚上将当前服务器上所有的数据备份，发送一份到远程的云服务上去。 ​ 首先先创建一个目录，/usr/local/redis，然后在redis的目录中创建copy文件夹，用来存储复制快照文件的脚本。然后在redis目录中创建snapshotting文件夹，用来存储备份的rdb快照。 ​ 执行vi redis_rdb_copy_hourly.sh，编写每小时复制一份rdb的shell脚本。 123456789#!/bin/sh cur_date=`date +%Y%m%d%k`rm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_datedel_date=`date -d -48hour +%Y%m%d%k`rm -rf /usr/local/redis/snapshotting/$del_date ​ 执行vi redis_rdb_copy_daily.sh，编写每天复制一份rdb的shell脚本。 123456789#!/bin/sh cur_date=`date +%Y%m%d`rm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_datedel_date=`date -d -1month +%Y%m%d`rm -rf /usr/local/redis/snapshotting/$del_date ​ 上面cp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_date中的/var/redis/6379/dump.rdb是redis生成的快照文件的存储地址，可以根据具体情况进行修改。 ​ 最后再执行crontab -e新建调度任务，命令如下： 120 * * * * sh /usr/local/redis/copy/redis_rdb_copy_hourly.sh0 0 * * * sh /usr/local/redis/copy/redis_rdb_copy_daily.sh ​ 然后再每天一次将所有的数据上传一次到远程的云服务器上去，这样一个数据备份方案就算完成了。 数据恢复方案 如果是redis进程挂掉了，那么重启redis进程即可，直接基于AOF日志文件恢复数据。 如果是redis进程所在机器宕机了，那么重启机器后，尝试重启redis进程，尝试直接基于AOF日志文件进行数据恢复。如果AOF没有破损，那么久直接基于AOF恢复，如果AOF文件损坏，那么用redis-check-aof fix修复日志文件。 如果redis当前最新的AOF文件和RDB文件出现了丢失，那么可以尝试基于该机器上当前的某个最新的RDB数据副本进行数据恢复。具体操作步骤为：停掉redis，关闭AOF，拷贝AOF备份，重启redis，确认数据恢复，直接在命令行热修改redis配置，即在redis-cli中执行config set appendonly yes打开AOF。这个时候redis就会将内存中的数据对应的日志，写入AOF文件中，此时AOF和RDB两份数据文件的数据就同步了。用redis config set热修改配置参数，可能配置文件中的实际参数没有被持久化的修改，需要再停止redis，手动修改配置文件，打开AOF，然后再重启redis。 如果当前机器上的所有RDB文件全部损坏，那么从远程的云服务上拉取最新的RDB快照来恢复数据。 如果是发现有重大的数据错误，比如某个小时上线的程序一下子将数据全部污染了，数据全错了，那么可以选择某个更早的时间点，对数据进行恢复 举个例子，12点上线了代码，发现代码有bug，导致代码生成的所有的缓存数据，写入redis，全部错了 找到一份11点的rdb的冷备，然后按照上面的步骤，去恢复到11点的数据，就可以了。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM中的一些参数介绍]]></title>
    <url>%2FCKING.github.io%2F2019%2F09%2F16%2FJVM%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[JVM配置常用参数堆参数 参数 描述 -Xms 设置JVM启动时堆内存的初始化大小 -Xmx 设置堆内存最大值 -Xmn 设置年轻代的空间大小，剩下的为老年代的空间大小 -XX:PermGen 设置永久代内存的初始化大小（JDK1.8开始废弃了永久代） -XX:MaxPermGen 设置永久代的最大值 -XX:SurvivorRatio 设置Eden区和Survivor区的空间比例：Eden/S0 = Eden/S1 默认为8 -XX:NewRatio 设置老年代与年轻代的比例大小，默认值为2 回收器参数 参数 描述 -XX:+UseSerialGC 串行，Young区和Old区都使用串行，使用复制算法回收，逻辑简单高效，无线程切换开销 -XX:+UseParallelGC 并行，Young区：使用Parallel scavenge回收算法，会产生多个线程并行回收。通过-XX:ParallelGCThreads=n参数指定有线程数，默认是CPU核数；Old区：单线程 -XX:+UseParallelOldGC 并行，和UseParallelGCC一样，Young区和Old区的垃圾回收时都使用多线程手机 -XX:+UseConcMarkSweepGC 并发，短暂停顿的并发收集。Young区：可以使用普通的或者parallel垃圾回收算法，由参数 -XX:+UseParNewGC来控制；Old区：只能使用Concurrent Mark Sweep -XX:+UseG1GC 并行的、并发的和增量式压缩短暂停顿的垃圾收集器。不区分Young区和Old区空间。它把堆空间划分为多个大小相等的区域。当进行垃圾收集时，它会优先收集存活对象较少的区域，因此叫“Garbage First” ​ 如上表所示，目前主要有串行、并行和并发三种。对于大内存的应用而言，串行的性能太低，因此使用到的主要是并行和并发两种。并行和并发GC的策略通过UseParallelGCC和UseCon从MarkSweepGC来指定，还有一些细节的配置参数用来配置策略的执行。例如：XX:ParallelGCThreads、XX:CMSInitiatingOccupancyFraction等。通常：Young区对象回收只可选择并行（耗时间），Old区选择并发（耗CPU）。 项目中常用配置 参数设置 描述 -Xms4800m 初始化堆空间大小 -Xmx4800m 最大堆空间大小 -Xmn1800m 年轻代的空间大小 -Xss512k 设置线程栈空间大小 -XX:PermSize=256m 永久区空间大小（jdk1.8开始废弃了永久代） -XX:MaxPermSize=256m 最大永久区空间大小 -XX:+UseStringCache 默认开启，启用缓存常用的字符串 -XX:+UseConcMarkSewwpGC 老年代使用CMS收集器 -XX:UseParNewGC 新生代使用并行收集器 -XX:ParallelGCThreads=4 并行线程数量4 -XX:+CMSClassUnloadingEnabled 允许对类的元数据进行清理 XX:+DisableExplicitGC 禁止显示的GC -XX:+UseCMSInitiatingOccupancyOnly 表示只有达到阈值之后才进行CMS回收 -XX:CMSInitiatingOccupancyFraction=68 设置CMS在老年代回收的阈值为68% -verbose:gc 输出虚拟机GC详情 -XX:+PrintGCDetails 打印GC详情日志 -XX:+PrintGCDataStamps 打印GC的耗时 -XX:+PrintTenuringDistribution 打印Tenuring年龄信息 -XX:+HeapDumpOnOutOfMemoryError 当抛出OOM时进行HeapDump -XX:HeapDumpPath=/home/admin/logs 指定HeapDump的文件路径或目录 常用组合 Young Old JVM Options Serial Serial -XX:+UseSerialGC Parallel scavenge Parallel Old/Serial -XX:+UseParallelGC-XX:+UseParallelOldGC Serial/Parallel scavenge CMS -XX:+UseParNewGC-XX:+UseConcMarkSweepGC G1 -XX:+UseG1GC 常用GC调优策略GC调优原则​ 在调优之前，我们需要记住下面的原则： 多数的Java应用不需要在服务器上进行GC优化。 多数导致GC问题的Java应用，都不是因为我们参数设置错误，而是代码问题。 在应用上线之前，先考虑将机器的JVM参数设置到最优（最适合）。 减少创建对象的数量。 减少使用全局变量和大对象。 GC优化是到最后不得已才 采用的手段。 在实际使用中，分析GC情况优化代码比优化GC参数要多得多。 GC调优目的 将转移到老年代的对象数量降低到最小。 减少GC的执行时间 GC调优策略 策略1：将新对象预留在新生代，由于Full GC的成本远高于Minor GC，因此尽可能将对象分配在新生代是明智的做法，实际项目中根据GC日志分析新生代空间大小分配是否合理，适当通过“-Xmn”命令调节新生代大小，最低限度降低新对象直接进入老年代的情况。 策略2：大对象进入老年代，虽然在大部分情况下，将对象分配在新生代是合理的。但是对于大对象这种做法却值得商榷，大对象如果首次在新生代分配可能会出现空间不足导致很多年龄不够的小对象被分配到老年代，破坏新生代的对象结构，可能会出现频繁的full gc。因此，对于大对象，可以设置直接进入老年代（当然短命的大对象对于垃圾回收来说就是噩梦）。-XX:PretenureSizeThreshold可以设置直接进入老年代的对象大小。 策略3：合理设置进入老年代对象的年龄，-XX:MaxTenuingThreshold设置对象进入老年代的年龄大小，减少老年代的内存占用，降低full GC发送的频率。 策略4：设置稳定的堆大小，堆打下设置有两个参数：-Xms初始化堆大小，-Xms最大堆大小。 策略5：如果满足下面的指标，则一般不需要进行GC优化： MinorGC执行时间不到50ms MinorGC执行不频繁，约10秒一次 Full GC执行时间不到1s Full GC执行频率不算频繁，不低于10分钟1次。 参考资料Java应用如何调优]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单讲一下分库分表]]></title>
    <url>%2FCKING.github.io%2F2019%2F09%2F10%2F%E7%AE%80%E5%8D%95%E8%AE%B2%E4%B8%80%E4%B8%8B%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[分库分表​ 分库分表是两回事，可能是光分库不分表，也可能是光分表不分库，都有可能。 分表​ 如果你单表都几千万数据了，单表数据量太大，会极大影响你的Sql执行的性能，到了后面sql可能就跑的很慢了。一般来说，单表到了几百万的时候，性能就会相对差一些，你就得分表了。 ​ 分表，就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户ID来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就可以了。这样可以控制每个表的数据量在可控的方范围内，比如每个表就固定在200万以内。 分库​ 分库，就是一般而言我们一个库，最多支撑到2000并发，就一定要扩容了，而且一个健康的单库并发值最好保持在每秒1000左右，不要太大。可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 分库分表前 分库分表后 并发支撑情况 MySQL单机部署，扛不住高并发 MySQL从单机到多机，能承受的并发增加了多倍 磁盘使用情况 MySQL单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低 SQL执行性能 单表数据量太大，SQL越跑越慢 单表数据库量减少，SQL执行效率明显提升 分库分表的中间件​ 比较常见的分库分表的中间件包括： Cobar TDDL Atlas Sharding-jdbc Mycat ​ 目前市场上比较主流的就是sharding-jdbc和Mycat，这两个都可以考虑去使用。Sharding-jdbc这种属于Client层方案，优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合Sharding-jdbc的依赖。Mycat属于proxy层方案，缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间间那里处理即可。 ​ 通常来说，这两个方案其实都可以选用，但是大佬建议中小型公司选用 Sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 Mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 Mycat，然后大量项目直接透明使用即可。 如何对数据库进行水平拆分和垂直拆分水平拆分​ 水平拆分，就是把一个表的数据弄个多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的表里，然后用多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分​ 垂直拆分，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 ​ 还有表层面的拆分，就是分表，将一个表变成N个表，就是让每个表的数据量控制在一定范围内，保证SQL的性能。否则单表的数据量越大，SQL性能就越差。 ​ 无论分库还是分表，上面说的那些数据库中间件都是可以支持的。就是基本上那些中间件可以做到你分库分表之后，中间件可以根据你指定的某个字段值，比如说 userid，自动路由到对应的库上去，然后再自动路由到对应的表里去。 ​ 你就得考虑一下，你的项目里该如何分库分表？一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都 ok 了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。 ​ 有两种分库分表的方式： 按照range来分，就是每个库一段连续的数据，这个一般是按照，例如时间范围来的。但是这种一般较少使用，因为很容易产生热点问题，大量的流量都打在最新的数据上了 按照某个子段hash一下均匀分散，这个较为常用。 ​ range来分，好处在于说，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。 ​ hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。]]></content>
      <categories>
        <category>分布式</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[保证缓存与数据库双写的一致性]]></title>
    <url>%2FCKING.github.io%2F2019%2F09%2F08%2F%E4%BF%9D%E8%AF%81%E7%BC%93%E5%AD%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8C%E5%86%99%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%2F</url>
    <content type="text"><![CDATA[本节思维导图 Cache Aside Pattern​ 最经典的缓存+ 数据库读写的模式，就是这个Cache Aside Pattern 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回请求。 更新的时候，先更新数据库，然后再删除缓存。 ​ 至于为什么是删除缓存，而不是更新缓存。原因在于在复杂的缓存场景，缓存不单单是数据库中直接拉取出来的值。比如更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据进行运算，才能计算出缓存最新的值的。 ​ 而且更新缓存的代价有时候是很高的。对于复杂的缓存数据计算场景，如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新，但这个数据有可能只需要被访问一次呢？例如一个缓存涉及的表的字段，在一分钟内就修改了一百次，而缓存也更新了一百次，但是这个缓存在一分钟内只被读取一次，有大量的冷数据。如果只是删除缓存的话，那么在1分钟内，这个缓存也就重新计算一次，大幅度降低开销。用到缓存才去算缓存。 最初级的缓存不一致问题及解决方案​ 如果先更新数据库，在删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。 解决思路​ 先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存是空的，那么数据不会不一致。因为读的时候缓存没有了，所以读了数据库中的旧数据，然后更新到缓存中。 高并发场景下数据不一致问题分析​ 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改，一个请求过来，先去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。这样就会发生数据库与缓存的数据不一致了。 什么场景下会发生上述情况​ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。如果并发量很低的话，特别是读并发很低，每天访问量就一万，那么很少的情况才会出现上述情况。如果每天是上亿的流量，每秒并发读是几万，每秒只要有数据更新的情况，就可坑出现上述的数据库和缓存不一致的情况。 解决方案​ 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据 + 更新缓存的操作，根据唯一标识路由之后，也发到同一个jvm内部队列中。 ​ 一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没有完成更新。此时如果一个读请求过来，没有读到缓存，可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。 ​ 这里有一个可以优化的地方。一个队列中，多个更新缓存请求串在一起是没意义的。可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新缓存的请求操作进去，直接等前面的更新操作请求完成即可。 ​ 等那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库读取最新的值，然后写入缓存中。 ​ 如果请求还在等待时间范围内，不断轮询，发现可以取到值了，那么久直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。 存在的问题 读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以要注意读超时的问题，每个读请求必须在超时时间范围内返回。 ​ 解决方案，或者最大的风险点在于可能数据更新很频繁。导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。因此务必通过一些模拟真实的测试，看看数据更新的频率是怎样的。 ​ 另外，因为一个队列中，可能会积压对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务。每个服务分摊一些数据的更新操作。如果一个内存队列路积压100个商品的库存修改操作，每个库存修改操作需要耗费10ms去完成，那么最后一个商品的读请求，可能等待10 * 100ms = 1s，才能得到数据，这个时候就导致读请求的长时阻塞。 读请求并发量过大 ​ 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时hang在服务上，看服务能不能抗的住，需要多少机器才能抗住最大的极限情况的峰值。 ​ 但是因为不是所有的数据都在同一时间更新，缓存也不会再同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。 多服务实例部署的请求路由 ​ 可能这个服务部署了多个实例，那么必须保证，执行数据更新操作，以及执行缓存更新操作的请求，都通过Nginx服务器路由到相同的服务实例上。 ​ 例如对同一个商品的读写请求，全部路由到同一台服务器上。可以自己去做服务间的按照某个请求参数的hash路由，也可以用Nginx的hash路由功能等。 热点商品的路由问题，导致请求的倾斜 ​ 如果某个商品的读写请求特别高，全部打到相同机器相同的队列里，可能会造成某台机器的压力过大。就是说，因为只有在商品更新的时候才会清空缓存，然后才会导致读写并发。所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响不是特别大，但是的确可能某些机器的负载会高一些。 总结​ 一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说系统不是严格要求“缓存 + 数据库”必须保持一致性的话，最好不要做“读请求和写请求串行化”，串到一个内存队列里去。 ​ 串行化可以保证一定不会出现不一致的情况，但是它会导致系统的吞吐量大幅度降低。]]></content>
      <categories>
        <category>分布式</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis集群]]></title>
    <url>%2FCKING.github.io%2F2019%2F09%2F04%2FRedis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[本节思维导图 ![Redis Cluster](Redis集群/Redis Cluster.png) ​ Redis cluster，主要是针对海量数据 + 高并发 + 高可用的场景。Redis cluster支撑N个redis master node，每个master node都可以挂载多个slave node。这样整个redis就可以横向扩容了。如果要支撑更大数据量的缓存，那就横向扩容更多的master节点。 Redis cluster介绍 自动将数据进行分片，每个master上放一部分数据 提供内置的高可用支持，部分master不可用时，还是可以继续工作的 ​ 在redis cluster架构下，每个redis要开放两个端口号，比如一个是6379，另一个就是加1W的端口号，比如16379. ​ 16379端口号是用来进行节点间通信的，也就是cluster bus的东西。cluster bus的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus使用一种二进制的协议，gossip协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制基本通信原理​ 集群元数据的维护有两种方式：集中式。Gossip协议。redis cluster节点间采用gossip协议进行通信。 ​ 集中式是将集群元数据（节点信息、故障等等）几种存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的storm。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于ZooKeeper（分布式协调的中间件）对所有元数据进行存储维护。 ​ redis维护集群元数据采用另一个方式，gossip协议，所有节点都持有一份元数据，不同节点如果出现了元数据的变更，就不断将元数据发送给其他的节点，让其他节点也进行元数据的并更。 ​ 集中式的好处在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其他节点读取的时候就可以感知到；不好在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。 ​ gossip好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆续打到所有节点上去更新，降低了压力；缺点是元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。 1000端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+ 10000。每个节点每隔一段时间都会往另外几个节点发送ping消息，同时其他几个节点接收到ping之后返回pong。 交换的信息：信息包括故障信息，节点的增加和删除，hash slot信息等等。 gossip协议​ gossip协议包含多种消息，包含ping、pong、meet、fail等等。 meet：某个节点发送meet给新加入的节点，让新节点加入集群中，然后新节点就会开始与其他节点进行通信。 1redis-trib.rb add-node 其实内部就是发送了一个gossip meet消息给新加入的节点，通知那个节点去加入我们的集群。 ping：每个节点都会频繁给其它节点发送ping，其中包含自己的状态还有自己维护的集群元数据，互相通过ping交换元数据。 pong：返回ping和meet，包含自己的状态和其它信息，也用于消息广播和更新。 fail：某个节点判断另一个节点fail之后，就发送fail给其它节点，通知其它节点某个节点宕机了。 ping消息深入​ ping时要携带一些元数据，如果很频繁，可能会加重网络负担。 ​ 每个节点每秒会执行10次ping，每次会选择5个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了cluster_node_timeout / 2，那么立即发送ping，避免数据交换延时过长，落后的时间过长。例如，两个节点之间都10分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以cluster_node_timeout可以调节，如果调的比较大，那么会降低ping的频率。 ​ 每次ping，都会带上自己节点的信息，还有就是带上1/10其它节点的信息，发送出去，进行交换。至少包含3个其它节点的信息，最多包含总结点减2个其它节点的信息。 分布式寻址算法 hash算法 一致性hash算法（自动缓存迁移） + 虚拟节点（自动负载均衡） redis cluster的hash slot算法 hash算法​ 来了一个key，首先计算hash值，然后对节点数取模。然后打在不同的master节点上，一旦某一个master节点宕机，所有请求过来，都会基于最新的master节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。 一致性hash算法​ 一致性hash算法将整个hash值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织。下一步将各个master节点（使用服务器的ip或主机名）进行hash。这样就能确定每个节点在其哈希环上的位置。 ​ 来了一个key，首先计算hash值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个master节点就是可以所在位置。 ​ 在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其他不收影响，增加一个节点也同理。 ​ 但是如果一致性哈希算法在节点太少是，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性hash算法引入了虚拟节点机制，即对每一个节点计算多个hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。 redis cluster的hash slot算法​ redis cluster有固定的16384个hash slot，对每个key计算CRC16值，然后对16384取模，可以获取key对应的hash slot。 ​ redis cluster中每个master都会持有部分slot，比如有3个master，那么可能每个master持有5000多个hash slot。hash slot让node的增加和移除都很简单，增加一个master，就将其他master的hash slot移动部分过去，减少一个master，就将它的hash slot移动到其他master上去。移动hash slot的成本是非常低的。客户端的API，可以对指定的数据，让他们走同一个hash slot，同时hash tag来实现。 ​ 任何一台机器宕机，redis的寻址都不受影响。因为key找的是hash slot，不是机器。 ![hash slot](Redis集群/hash slot.png) Redis Cluster的高可用与主备切换原理​ redis cluster的高可用的原理，几乎跟哨兵是类似的。 判断节点宕机​ 如果一个节点认为另一个节点宕机，那么就是pfail，主观宕机。如果多个节点都认为一个节点宕机了，那么就是fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown。 ​ 在cluster-node-timeout内，某个节点一直没有返回pong，那么就会认为fail。 ​ 如果一个节点认为某个节点fail，那么会在gossip ping消息中，ping给其他节点，如果超过半数的节点都认为pfail了，那么就会变成fail. 从节点过滤​ 对宕机的master node，从其所有的slave node中，选择一个切换成master node。 ​ 检查每个slave node与master node断开连接的时间，如果超过了cluster-node-timeout * cluster-salve-validity-factor，那么就没有资格切换成master。 从节点选举​ 每个从节点，都根据自己对master复制数据的offset，来设置一个选举时间，offset越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。 ​ 所有的master node开始slave选举投票，给要进行选举的slave进行投票，如果大部分master node (N / 2 + 1)都投票给某个从节点，那么选举通过，那个从节点 可以切换成master。 ​ 从节点执行主备切换，从节点切换为主节点。 与哨兵比较​ 整个流程跟哨兵相比，非常类似。所以redis cluster相当于直接集成了replication和sentinel的功能。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis持久化]]></title>
    <url>%2FCKING.github.io%2F2019%2F09%2F01%2FRedis%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本节思维导图 ​ Redis持久化的意义在于数据备份和灾难恢复。Redis如果仅仅只是将数据缓存在内存里面，如果Redis宕机了再重启，内存里的数据就全部弄丢了。所以得用Redis的持久化机制，将数据写入内存的同时，异步的慢慢的将数据写入磁盘文件里，进行持久化。如果Redis宕机重启，自动从磁盘上加载之前持久化的一些数据即可，也许会丢失少许数据，但是至少不会将所有数据都弄丢。 Redis持久化的两种方式 RDB：RDB持久化机制，是对Redis中的数据进行周期性的持久化。 AOF：AOF机制对每条写入命令作为日志，以append-only的模式写入一个日志文件中，在Redis重启的时候，可以通过回放AOF日志中的写入指令来重新构建整个数据集。 ​ 通过RDB或AOF，都可以将Redis内存中的数据持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如阿里云等云服务等。 ​ 如果同时使用RDB和AOF两种持久化机制，那么redis重启的时候，会使用AOF来重构新数据，因为AOF中的数据更加完整。 RDB优缺点 RDB会生成多个数据文件，每个数据文件都代表某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完备的数据文件发送到一些远程的安全存储上去，比如国内的阿里云的ODPS分布式存储上，以预定好的备份策略来定期备份redis中的数据。 RDB对redis对外提供的读写服务，影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可。 相对于AOF持久化机制来说，直接基于RDB数据文件来重启和恢复redis进程，更加快速。 如果想要在redis故障时，尽可能少的丢失数据，那么RDB没有AOF好。一般来说，RDB数据快照文件，一般都是每隔5分钟，或者更长时间生成一次，这个时候就得接受一旦redis进程宕机，那么会丢失最近5分钟的数据。 RDB每次在fork子进程来执行RDB快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF优缺点 AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据。 AOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使尾部破损，也很容易修复。 AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewritelog的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的merge后日志文件ready的时候，再交换新老日志文件即可。 AOF日志文件的命令通过非常可读的方式进行记录，这个特定非常适合做灾难性的误删除的紧急操作。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据。 对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大。 AOF开启后，支持的QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然每秒一次fsync性能也还是很高的。但如果实时写入，那么QPS会大降，redis性能会大大降低。 以前AOF发生过bug，就是通过AOF记录的日志，进行数据恢复的时候，没有恢复一模一样的数据。因此，类似AOF这种较为复杂的基于命令/merge/回放的方式，比基于RDB每次持久化一份完整的数据快照的方式，更加脆弱一些，容易有bug。不过AOF就是为了rewrite过程导致的bug，因此每次rewrite并不是基于旧的指令日志进行merge，而是基于当时内存中的数据进行指令的重新构建，这样健壮性好很多。 补充：rewrite类似于普通数据库管理系统日志恢复点，当AOF文件随着写命令的运行膨胀时，当文件大小触碰到临界时，rewrite会被运行。 rewrite会像replication一样，fork出一个子进程，创建一个临时文件，遍历数据库，将每个key、value对输出到临时文件。输出格式就是Redis的命令，但是为了减小文件大小，会将多个key、value对集合起来用一条命令表达。在rewrite期间的写操作会保存在内存的rewrite buffer中，rewrite成功后这些操作也会复制到临时文件中，在最后临时文件会代替AOF文件。 RDB和AOF到底该如何选择 不要仅仅使用RDB，因为那样会导致你丢失很多数据。 也不要仅仅使用AOF，因为那样有两个问题：第一，通过AOF做冷备，没有RDB做冷备来得恢复速度更快；第二，RDB每次简单粗暴生成数据快照，更加健壮，可以避免AOF这种复杂的备份和恢复机制的bug； redis支持同时开启两种持久化方式，我们可以综合使用AOF和RDB两种持久化机制，用AOF来保证数据不丢失，作为数据恢复的第一选择，用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速的数据恢复。 如何配置RDB持久化的配置​ 打开redis的配置文件，搜索save，如图所示： ​ save 60 10000表示，每隔60秒，如果有超过10000个key发生了变化，那么就生成一个新的dump.rdb文件，就是当前redis内存中完整的数据快照，这个操作也被称为snapshotting。save可以设置多个，就是多个snapshotting检查点，每到一个检查点，就会去check一下，是否有指定的key数量发生了变更，如果有，就生成一个新的dump。 ​ 也可以手动调用save或者bgsave命令，同步或异步执行rdb快照生成。 ​ 如果你通过redis-cli SHUTDOWN的方式去停掉redis，这其实是一种安全退出的模式，redis在退出的时候会将内存中的数据立即生成一份完整的快照。如果用kill -9粗暴杀死redis进程，则相当于redis故障异常退出，不会生成dump快照文件。 AOF持久化的配置​ AOF持久化默认是关闭的，默认是打开RDB持久化。要开启AOF持久化配置，在redis配置文件中搜索appendonly，如下所示，将no改为yes即可。打开AOF持久化机制之后，redis每收到一条写指令，就会写入日志文件中。会先写入os cache，然后每隔一定时间再fsync一下。 ​ AOF的fsync总共有三种策略： appendfsync always：每次写入一条数据，立即将这个数据对应的写日志fsync到磁盘上去，性能很差，吞吐量很低，但确保了redis里的数据一条不丢。 appendfsync everysec：每秒执行一次fsync。这个最常用，生产环境一般这么配置，性能很高，QPS可以上万。 appendfsync no：不主动执行fsync，由操作系统自行判断。不可控。 AOF rewrite的配置​ 这里主要讲两个rewrite的配置 12auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 举个例子，比如上一次AOF rewrite之后，日志大小是128MB。此时就会接着128MB继续写AOF日志，如果发现增长的比例已经超过了之前的100%，256MB，就可能会去触发一次rewrite。但是此时还要去跟min-size， 64mb去比较，256 &gt; 64时，才会触发rewrite。 AOF破损文件的修复如果redis在append数据到AOF文件时，机器宕机了，可能会导致AOF文件破损 用redis-check-aof –fix命令来修复破损的AOF文件]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计核心接口的防重幂等性]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F26%2F%E8%AE%BE%E8%AE%A1%E6%A0%B8%E5%BF%83%E6%8E%A5%E5%8F%A3%E7%9A%84%E9%98%B2%E9%87%8D%E5%B9%82%E7%AD%89%E6%80%A7%2F</url>
    <content type="text"><![CDATA[本节思维导图 ​ 在分布式系统中，一般都会有重试机制。但重复机制又有一定几率出现重复的数据。例如订单系统消费了消息，但是由于网络等问题消息系统未收到反馈是否已成功处理，此时消息系统会根据配置的规则隔断时间就retry一次。但如果此时网络恢复正常，我第一次收到的消息成功处理了，这是又收到一条消息，如果没有防护措施，就有可能出现重复数据。 接口幂等性​ 幂等性指任意多次执行所产生的影响均与一次执行的影响相同。多次调用对系统的产生的影响是一样的，即对资源的作用是一样的，但是返回值允许不同。在我们编程中主要操作就是CURD，其中读取（Retrieve）操作和删除（Delete）操作是天然幂等的，受影响的就是创建（Create）、更新（Update）。 对于业务中需要考虑幂等性的地方一般都是接口的重复请求，重复请求是指同一个请求因为某些原因被多次提交。导致这个情况会有几种场景： 前端重复提交：提交订单，用户快速重复点击多次，造成后端生成多个内容重复的订单。 接口超时重试：对于给第三方调用的接口，为了防止网络抖动或其他原因造成请求丢失，这样的接口一般都会设计成超时重试多次。 消息重复消费：MQ消息中间件，消息重复消费。 幂等性实现方式Token机制 服务端提供了发送token的接口，我们在分析业务的时候，哪些是存在幂等问题的，就必须在执行业务前，前去获取token，服务器会把token保存到redis中； 然后调用业务接口请求时，把token携带过去，一般反正请求头部； 服务器判断token是否存在redis中，存在表示第一次请求，可以继续执行业务，业务完成后，需要把redis中的token删掉； 如果判断token不存在redis中，就表示是重复操作，直接返回重复标记给client，这样就保证了业务代码，不被重复执行。 这就是token+redis的幂等方案。适用于绝大部分场景。主要针对前端重复连续多次点击的情况，网上也有另一个版本的Token方案，不同的地方是：网上方案检验token存在后，就立刻删除token，再进行业务处理。而上面的方式是检验token存在后，先进行业务处理，再删除token。 网上方案的缺点是先删除token，这是出现系统问题导致业务处理出现异常，业务处理没有成功，接口调用方也没有获取到明确的结果，然后进行重试，但token已经删除掉了，服务端判断token不存在，认为是重复请求，就直接返回了，无法进行业务处理了。 而上面的方案后删除token也是会存在问题的，如果进行业务处理成功后，删除redis中的token失败了，这样就导致了有可能会发生重复请求，因为token没有被删除。 token机制缺点业务请求每次请求，都会有额外的请求（一次获取token请求、判断token是否存在的业务）。其实真实的生产环境中，1万请求也许只会存在10个左右的请求会发生重试，为了这10个请求，我们让9990个请求都发生了额外的请求。（当然redis性能很好，耗时不会太明显） 去重表机制往去重表里插入数据的时候，利用数据库的唯一索引特性，保证唯一的逻辑。唯一序列号可以是一个字段，也可以是多字段的唯一性组合。 这里要注意的是，去重表和业务表应该在同一库中，这样就保证了在同一个事务，即使业务操作失败了，也会把去重表的数据回滚。这个很好的保证了数据一致性。 另外，使用数据库防重表的方式它有个严重的缺点，那就是系统容错性不高，如果幂等表所在的数据库连接异常或所在的服务器异常，则会导致整个系统幂等性校验出问题。 乐观锁机制乐观锁解决了计算赋值型的修改场景。例如： 123456update user set point = point + 20, version = version + 1 whereuserid=1 and version=1 加上了版本号后，就让此计算赋值型业务，具备了幂等性。 乐观锁缺点在操作业务前，需要先查询出当前的version版本。 唯一主键机制这个机制是利用了数据库的主键唯一约束的特性，解决了在insert场景时幂等问题。但主键的要求不是自增的主键，这样就需要业务生成全局唯一的主键，之前老顾的文章也介绍过分布式唯一主键ID的生成，可自行查阅。如果是分库分表场景下，路由规则要保证相同请求下，落地在同一个数据库和同一表中，要不然数据库主键约束就不起效果了，因为是不同的数据库和表主键不相关。因为对主键有一定的要求，这个方案就跟业务有点耦合了，无法用自增主键了。 Redis实现Redis实现的方式就是将唯一序列号作为Key，唯一序列号可以拿几个字段MD5加密生产的密文，value可以是你想填的任何信息。唯一序列号也可以是一个字段，例如订单的订单号，也可以是多字段的唯一性组合。当然这里需要设置一个 key 的过期时间，否则 Redis 中会存在过多的 key。 状态机对于很多业务有一个业务流转状态的，每个状态都有前置状态和后置状态，以及最后的结束状态。例如流程的待审批，审批中，驳回，重新发起，审批通过，审批拒绝。订单的待提交，待支付，已支付，取消。 以订单为例，已支付的状态的前置状态只能是待支付，而取消状态的前置状态只能是待支付，通过这种状态机的流转我们就可以控制请求的幂等。 123456789101112131415161718192021222324252627public enum OrderStatusEnum &#123; UN_SUBMIT(0, 0, "待提交"), UN_PADING(0, 1, "待支付"), PAYED(1, 2, "已支付待发货"), DELIVERING(2, 3, "已发货"), COMPLETE(3, 4, "已完成"), CANCEL(0, 5, "已取消"), ; //前置状态 private int preStatus; //状态值 private int status; //状态描述 private String desc; OrderStatusEnum(int preStatus, int status, String desc) &#123; this.preStatus = preStatus; this.status = status; this.desc = desc; &#125; //...&#125; 假设当前状态是已支付，如果支付接口又收到了支付请求，则会抛出异常会拒绝此处处理。 参考资料https://juejin.im/post/5ceb4c4f51882572a206d174 https://juejin.im/post/5d1e01aaf265da1bbc6ff400]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud生产环境配置服务的配置超时和重试参数]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F26%2FSpringCloud%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%85%8D%E7%BD%AE%E8%B6%85%E6%97%B6%E5%92%8C%E9%87%8D%E8%AF%95%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[启动Ribbon的饥饿加载​ 在生产环境中，系统第一次启动时，调用其他服务经常会出现timeout，经过查阅资料得知：每个服务第一次被请求的时候，他会去初始化一个Ribbon的组件，初始化这些组件需要耗费一定的时间，所以很容易会导致timeout问题。解决方案是让每个服务启动的时候直接初始化Ribbon相关的组件，避免第一次请求的时候初始化。 123ribbon: eager-load: enabled: true ​ 上面只是解决了内部服务之间的调用，但还有一个问题就是：网关到内部服务的访问。由于Spring Cloud Zuul的路由转发也是通过Ribbon实现负载均衡的，所以也会存在第一次调用时比较慢的情况。 ​ 此时可以通过以下配置 12345zuul: ignored-services: '*' ribbon: eager-load: enabled: true ​ Spring Cloud Zuul的饥饿加载中没有设计专门的参数来配置，而是直接采用了读取路由配置来进行饥饿加载的做法。所以，如果我们使用默认路由，而没有通过配置的方式制定具体路由规则，那么zuul.ribbon.eager-load.enabled=true的配置就没有作用了。 ​ 因此，在真正使用的时候，可以通过zuul.ignored-services=*来忽略所有的默认路由，让所有的路由配置均维护在配置文件中，以达到网关启动时就默认初始化好各个路由转发的负载均衡对象。 Ribbon配置超时和重试参数​ 以下配置是用来配置Ribbon的超时时间和重试次数的： 12345678910111213ribbon: ConnectTimeout: 250 # 连接超时时间(ms) ReadTimeout: 2000 # 通信超时时间(ms) OkToRetryOnAllOperations: true # 是否对所有操作重试 MaxAutoRetriesNextServer: 1 # 同一服务不同实例的重试次数 MaxAutoRetries: 1 # 同一实例的重试次数hystrix: command: default: execution: isolation: thread: timeoutInMillisecond: 10000 # 熔断超时时长：10000ms ​ 假设在网关Zuul配置了以上参数，MaxAutoRetriesNextServer和MaxAutoRetries的意思是如果Zuul认为某个服务超时了，此时会先重试一下该服务对应的这台机器，如果还是不行就会重试一下该服务的其他机器。 ​ 重试机制除了上面的参数配置的方式之外，还可以使用Spring-Retry实现。相比配置参数配置的方式，灵活性和扩展性更强。详情可以看大佬的这一篇Spring Retry重试机制]]></content>
      <categories>
        <category>分布式</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里分布式事务框架seata的使用和介绍]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F24%2F%E9%98%BF%E9%87%8C%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%A1%86%E6%9E%B6seata%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[​ 在分布式系统中，分布式事务是一个必须要解决的问题，目前使用较多的是最终一致性方案。自年初阿里开源了Fescar（四月初更名为Seata）后，该项目受到了极大的关注，目前已接近 8000 Star。Seata以高性能和零侵入的特性为目标解决微服务领域的分布式事务难题，目前正处于快速迭代中。 seata的几个概念​ 在讲解seata的原理之前，我们先了解几个Seata的相关概念。 XID：全局事务的唯一标识，由 ip:port:sequence 组成； Transaction Coordinator (TC)：事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚； Transaction Manager (TM )：控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议； Resource Manager (RM)：控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚； seata的简单使用​ 本文主要基于springcloud + Eureka + mysql + seata的结构搭建一个分布式系统的demo。具体步骤如下： 下载Eureka的demo https://github.com/seata/seata-samples/tree/master/springcloud-eureka-seata 下载seata-server 0.8.0 https://github.com/seata/seata/releases 创建数据库fescar，并用Navicat执行一个SQL文件创建相应测试用的表格和数据，内容如下：（这一步其实可以省略，demo中配置文件的数据库地址其实是有效的） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/* Navicat Premium Data Transfer Source Server : seata Source Server Type : MySQL Source Server Version : 50616 Source Host : 47.95.78.215:3306 Source Schema : fescar Target Server Type : MySQL Target Server Version : 50616 File Encoding : 65001 Date: 23/08/2019 11:22:20*/SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for account_tbl-- ----------------------------DROP TABLE IF EXISTS `account_tbl`;CREATE TABLE `account_tbl` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_id` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `money` int(11) NULL DEFAULT 0, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 214 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;-- ------------------------------ Records of account_tbl-- ----------------------------INSERT INTO `account_tbl` VALUES (213, &apos;U100000&apos;, 10000);-- ------------------------------ Table structure for order_tbl-- ----------------------------DROP TABLE IF EXISTS `order_tbl`;CREATE TABLE `order_tbl` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_id` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `commodity_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `count` int(11) NULL DEFAULT 0, `money` int(11) NULL DEFAULT 0, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 247 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;-- ------------------------------ Table structure for storage_tbl-- ----------------------------DROP TABLE IF EXISTS `storage_tbl`;CREATE TABLE `storage_tbl` ( `id` int(11) NOT NULL AUTO_INCREMENT, `commodity_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `count` int(11) NULL DEFAULT 0, PRIMARY KEY (`id`) USING BTREE, UNIQUE INDEX `commodity_code`(`commodity_code`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 1135 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;-- ------------------------------ Records of storage_tbl-- ----------------------------INSERT INTO `storage_tbl` VALUES (1134, &apos;C100000&apos;, 200);-- ------------------------------ Table structure for undo_log-- ----------------------------DROP TABLE IF EXISTS `undo_log`;CREATE TABLE `undo_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20) NOT NULL, `xid` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `context` varchar(128) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `rollback_info` longblob NOT NULL, `log_status` int(11) NOT NULL, `log_created` datetime(0) NOT NULL, `log_modified` datetime(0) NOT NULL, `ext` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE, UNIQUE INDEX `ux_undo_log`(`xid`, `branch_id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 619 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;SET FOREIGN_KEY_CHECKS = 1; 修改demo中配置文件中数据库的账号和密码（这一步其实也可以省略，理由同上） 修改seata-server中的配置文件registry.conf，将registry的方式type改为“euraka”。如果有需要，你可以在下面修改eureka的配置，指定相应的serviceUrl和application。 修改demo中所有服务resources文件夹下的registry.conf，将注册方式type改为“file”。 先运行demo中的eureka服务，然后在seata-server的bin文件下运行命令seata-server.bat -h 127.0.0.1 -p 8091 -m file启动seata-server，然后再运行demo中的其他服务。若无明显错误信息，则启动成功。 测试demo的分布式事务功能，主要的事务发起者是business-service，测试地址如下： 提交：http://localhost:8084/purchase/commit 回滚：http://localhost:8084/purchase/rollback 修改后的源码下载地址：https://github.com/GD-CKING/demo demo解析引入依赖​ 通过分析demo，如果要使用分布式事务架构Seata，在需要引入seata的服务中引入以下依赖： 12345678&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-seata&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.seata&lt;/groupId&gt; &lt;artifactId&gt;seata-all&lt;/artifactId&gt;&lt;/dependency&gt; ​ demo中除了eureka之外其他服务都引入了这些依赖。 配置文件​ seata的配置文件主要有两个：registry.conf和file.conf。其中registry.conf是seata的配置入口文件。在registry中可以指定具体配置的形式，默认使用file类型，在file.conf配置文件中有一下配置内容： transport​ transport部分的配置对用NettyServerConfig类，用于定义Netty相关的参数。TM、RM和seata-server之间使用Netty进行通信。 service​ service中主要要注意service.vgroup_mapping这个配置，service.vgroup_mapping后面跟的内容要跟在配置文件中的spring.cloud.alibaba.seata.tx-service-group设置的属性一致，否则会提示no available server to connect.这个属性主要是为了定义一个tx-server-group名称 ，这个名称就是file.conf中的service.vgroup_mapping.${spring.cloud.alibaba.seata.tx-service-group}。 ​ 而file.conf中vgroup_mapping.my_test_tx_group = &quot;default&quot;指定seata-server的地址是下面default.grouplist设定的地址： 12345678910service &#123; #vgroup-&gt;rgroup #配置Client连接TC的地址 vgroup_mapping.my_test_tx_group = "default" default.grouplist = "127.0.0.1:8091" #degrade current not support enableDegrade = false #disable 是否启用seata的分布式事务 disableGlobalTransaction = false &#125; client1234567client &#123; #RM接收TC的commit通知后缓冲上限 async.commit.buffer.limit = 10000 lock &#123; retry.internal = 10 retry.times = 30 &#125; &#125; 表undo-log​ 要使用seata必须创建一个undo-log表。undo_log 是需要在业务库上创建的一个表，seata 依赖该表记录每笔分支事务的状态及二阶段 rollback 的回放数据。不用担心该表的数据量过大形成单点问题，在全局事务 commit 的场景下事务对应的 undo_log 会异步删除。 123456789101112CREATE TABLE `undo_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20) NOT NULL, `xid` varchar(100) NOT NULL, `rollback_info` longblob NOT NULL, `log_status` int(11) NOT NULL, `log_created` datetime NOT NULL, `log_modified` datetime NOT NULL, `ext` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 使用@GlobalTransactional开启事务​ 是开启分布式事务非常简单，只需要在要开启事务的业务方式上加上@GlobalTransactional注解开启事务即可。Seata 会将事务的 xid 通过拦截器添加到调用其他服务的请求中，实现分布式事务 TM处理流程​ 在本例中，TM 的角色是 business-service, BusinessService 的 purchase 方法标注了 @GlobalTransactional 注解。 ​ 方法调用后将会创建一个全局事务，首先关注 @GlobalTransactional 注解的作用，在GlobalTransactionalInterceptor中被拦截处理。 ​ 全局事务创建后，就开始执行 business.execute()，即业务代码storageFeignClient.deduct(commodityCode, orderCount)进入 RM 处理流程，此处的业务逻辑为调用 storage-service 的扣减库存接口。 RM处理流程 获取business-service传来的XID 绑定XID到当前上下文中 执行业务逻辑sql 向TC创建本次RM的Netty连接 向TC发送分支事务的相关信息 获得TC返回的branchId 记录Undo Log数据 向TC发送本次事务PhaseOne阶段的处理结果 从当前上下文中解绑XID 事务提交​ 各分支事务执行完成后，TC 对各 RM 的汇报结果进行汇总，给各 RM 发送 commit 或 rollback 的指令。 ​ 对于commit动作的处理，RM只需删除xid、branchId对应的undo_log即可。 事务回滚​ 对于rollback场景的触发有两种情况 分支事务处理异常，即ConnectionProxy中report(false)的情况。 TM捕获到下游系统上抛的异常，即发起全局事务标有@GlobalTransactional注解的方法捕获到的异常。在前面TransactionalTemplate类的execute模版方法中，对business.execute()的调用进行了catch，catch后会调用rollback，由TM通知TC对应XID需要回滚事务。 参考资料https://zhuanlan.zhihu.com/p/63381854]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis的主从复制架构]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F22%2FRedis%E7%9A%84%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[本节思维导图 Redis主从架构​ 单机的Redis，能够承载的QPS大概在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构设计成主从（master-slave）架构，一主多从，主负责写，并且将数据复制到其他的slave节点，从节点复制读。所有的读请求全部走从节点。这样也可以轻松实现水平扩容，支撑读高并发。 ​ redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication的核心机制 redis采用异步方式复制数据到slave节点，不过redis2.8开始，slave node会周期性地确认自己每次复制的数据量 一个master node是可以配置多个slave node的 slave node也可以连接其他的slave node slave node做复制的时候，不会block master node的正常工作 slave node做复制的时候，也不会block对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候会暂停对外服务了 slave node主要用来进行横向扩容，做读写分离，扩容的slave node可以提高读的吞吐量 ​ 如果采用了主从架构，那么建议必须开启master nod的持久化，不建议用slave node作为master node的数据热备，因为那样的话，你关掉了master的持久化，可能在master宕机重启的时候数据是空的，然后可能一经过复制，slave node的数据也丢了。 ​ 另外，master的各种备份方案 也需要做。如果本地的所有文件丢弃，从备份中挑选一份rdb去恢复master，这样才能确保启动的时候，是有数据的。即使采用了高可用机制，slave node可以自动接管master node，但也可能哨兵（sentinel）还没检测到masterfailure，master node自动重启了，还是可能导致上面的slave node数据被清空。 redis主从复制的核心原理​ 当启动一个slave node的时候，它会发送一个PSYNC命令给master node。 ​ 如果是slave node初次连接到master node，那么会触发一次full resynchronization全量复制。此时master会启动一个后台线程，开始生成一份RDB快照文件，同时还会将从客户端新收到的所有命令缓存在内存中。RDB文件生产完毕后，master会将这个RDB发送给slave，slave会先写入本地磁盘，然后再从本地磁盘加载到内存中，接着master会将内存中缓存的命令发送给slave，slave也会同步这些数据。slave node如果跟master node有网络故障，断开了连接，会自动重连，连接之后master node仅会复制给slave部分缺失的数据。 主从复制的断点续传​ 从redis2.8开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么就可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。 ​ master node会在内存中维护一个backlog，master和slave都会保存一个replica offset，还有一个master run id，offset就是保存在backlog中的。如果master和slave网络连接断掉了，slave会让master从上次的replica offset开始继续复制，如果没有找到对应的offset，就会执行一次resynchronization。 ​ 使用master run id，是为了定位到上次传输数据的master。如果是根据host + ip定位master node，是不靠谱的，如果master node重启或者数据出现了变化，那么slave node应该根据不同的run id区分。 无磁盘化复制​ master在内存中直接创建RDB，然后发送给slave，不会在本地落地磁盘。要想开启这个功能，只需要在配置文件中国开启repl-diskless-syc yes即可。 1234repl-diskless-sync yes# 等待 5s 后再开始复制，因为要等更多 slave 重新连接过来repl-diskless-sync-delay 5 过期key处理​ slave不会过期key，只会等待master过期key。如果master过期了一个key，或者通过LRU淘汰了一个key，那么会模拟一条del命令发送给slave。 复制的完整流程​ slave node启动时，会在自己本地保存master node的信息，包括master node的host和ip，但是复制流程没开始。 ​ slave node内部有个定时任务，每秒检查是否有新的master node要连接和复制，如果发现，就跟master node建立socket网络连接。然后master node发送ping命令给master node。如果master设置了requirepass，那么slave node必须发送masterauth的口令过去进行认证。master node第一次执行全量复制，将所有数据发送给slave node，而在后续，master node持续将写命令，异步复制给slave node。 数据同步相关的核心机制​ 数据同步相关的核心机制指的就是第一次slave连接master的时候，执行的全量复制，这个过程里面的一些细节的机制。 master和slave都会维护一个offset​ master会在自身不断累加offset，slave也会在自身不断累加offset。slave每秒都会上报自己的offset给master，同时master也会保存每个slave的offset。 ​ 这个不是特定就用在全量复制的，主要是master和slave都要知道各自的数据的offset，才能知道互相之间的数据不一致的情况。 backlog​ master node有一个backlog，默认是1MB大小。master node在给slave node复制数据时，也会将数据在backlog中同步写一份。backlog主要是用来做全量复制中断开后的增量复制的。 master run id​ info server可以看到master run id。 ​ 上面说过，根据host+ip定位master node是不靠谱的，如果master node重启或者数据发生了变化，那么slave node应该根据不同的run id区分，run id不同就做全量复制。如果需要不更改run id重启redis，可以使用redis-cli debug reload命令。 ![run id](Redis的主从复制架构/run id.png) psync​ 从节点使用psync从master node进行复制，psync runid offset ​ master node会根据自身的情况返回相应信息，可能是FULLRESYNC runid offset触发全量复制，可能是CONTINUE触发增量复制。 全量复制 master执行bgsave，在本地生成一份RDB快照文件 master node将RDB快照文件发送给slave node，如果RDB复制时间超过60秒（repl-timeout），那么slave node就会认为复制失败，可以适当调大这个参数。 master node在生成RDB时，会将所有新的写命令缓存在内存中，在slave node保存了RDB之后，再将新的写命令复制给slave node 如果在复制期间，内存缓冲区持续消耗超过64MB，会在一次性超过256MB，那么停止复制，复制失败。 1client-output-buffer-limit slave 256MB 64MB 60 slave node接收到RDB之后，清空自己的旧数据，然后重新加载RDB到自己内存中，同时基于旧的数据版本对外提供服务。 如果slave node开启了AOF，那么会立即执行BGREWAITEAOF，重写AOF 增量复制 如果全量复制过程中，master-slave网络连接断掉了，那么slave重新连接master时，会触发增量复制 master会直接从自己的backlog中获取部分丢失的数据，发送给slave node，默认backlog就是1MB master就是根据slave发送的psync中的offset来从backlog中获取数据的。 heartbeat​ 主从节点互相都会发送heartbeat信息 ​ master默认每隔10秒发送一次heartbeat，slave node每隔1秒发送一个heartbeat。 异步复制​ master每次接收到写命令之后，现在内部写入数据，然后异步发送给slave node redis如何才能做到高可用​ 一个slave故障了，并不会影响可用性，还有其他的slave在提供服务。但master node死掉了，会导致无法写数据。没有master可以写数据，slave也就没用了，系统就不可用了。 ​ redis的高可用架构，叫做failover故障转移，也可以叫做主备切换。 ​ master node在故障时，自动检测，并且将某个slave node自动切换为master node的过程，叫做主备切换。这个过程就实现了redis的主从架构下的高可用。 主从复制的配置​ 讲了那么多，我们来看看如何配置，从而实现主从架构。 ​ 首先先配置从节点: 打开从节点的配置文件，搜索replicaof （低版本的有些是slaveof），去配置从节点要连接的主节点。如replicaof 192.168.1.1 6379，其中192.168.1.1是我们主节点的IP地址。 在配置文件中搜索replica-read-only（低版本的有些是slave-read-only），将该属性配置为也是：replica-read-only yes，这样就开启了只读redis从节点，它会拒绝所有的写操作，这样可以强制搭建读写分离的架构，从而实现读写分离。 在配置文件中搜索masterauth，来配置主节点redis的连接口令。如masterauth redis-pass，其中redis-pass就是主节点的认证口令。 在配置文件中搜索bind，将bind 127.0.0.1改成bind 自己的IP地址。bind 127.0.0.1是本地的开发调式的模式，就只有127.0.0.1本地才能访问到6379的端口。 强制开启6379端口iptables -A INPUT -ptcp --dport 6379 -j ACCEPT。（这一步有时可以省略） ​ 配置主节点： 打开主节点的配置文件，搜索requirepass，配置主节点的认证口令，使其与从节点配置的masterauth保持一致。 在配置文件中搜索bind，将bind 127.0.0.1改成bind 自己的IP地址。 强制开启6379端口iptables -A INPUT -ptcp --dport 6379 -j ACCEPT。（这一步有时可以省略） ​ 这样主从架构就配置好了，我们测试一下，先启动主节点，再启动从节点。进入主节点的redis中，执行info replication查看相关信息 ​ 同样的，进入从节点的redis，执行info replication查看相关信息]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis的线程模型及和mencached的区别]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F20%2FRedis%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%92%8Cmencached%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[本节思维导图 Redis和Memcached的区别Redis支持复杂的数据结构​ redis相比于memcached来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作，redis相对来说比较好。 Redis原生支持集群模式​ redis 3.X便能支持cluster模式，而memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比​ 由于redis只使用单核，而memcached可以使用多核，所以平均每一个核上redis存储小数据时比memcached性能更高。而在100K以上的数据中，memcached性能要高于redis。 Redis的线程模型​ redis内部使用文件事件处理器file event handler，这个文件事件处理器是单线程的，所以redis才叫做单线程的模型。它采用IO多路复用机制同时监听多个socket，将产生事件的socket压入内存队列中，事件分派器根据socket上的事件类型来选择对应的事件处理器进行处理。 ​ 文件事件处理器的结构包含4个部分： 多个socket IO多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 客户端与redis的一次通信过程如下： 首先，redis服务端进程初始化的时候，会将server socket的AE_READABLE事件与连接应答处理器关联。 ​ 客户端socket01向redis进程的server socket请求建立连接，此时server socket会产生一个AE_READABLE事件，IO多路复用程序监听到server socket产生的事件后，将该socket压入队列中。文件事件分派器从队列中获取socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的socket01，并将该socket01的AE_READABLE事件与命令请求处理器关联。 ​ 假设客户端发送了一个set key value请求，此时redis中的socket01会产生AE_READABLE事件，IO多路复用程序将socket01压入队列，此时事件分派器从队列中获取到socket01产生的AE_READABLE事件，由于前面的socket01的AE_READABLE事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取socket01的key value并在自己内存中完成key value的设置，操作完成后，它会将socket01的AE_WRITABLE事件与命令回复处理器关联。 ​ 如果此时客户端准备好接受返回结果了，那么redis中的socket01会产生一个AE_WRITABLE事件，同样压入队列，事件分派器找到相关联的命令回复处理器，由命令回复处理器对socket01输入本次操作的一个结果，之后解除socket01的AE_WRITABLE事件与命令回复处理器的关联。 Redis单线程效率高的原因 纯内存操作 核心是基于非阻塞的IO多路复用机制 C语言实现，一般来说，C语言实现的程序更接近操作系统，执行速度相对会快 单线程反而避免了多线程的频繁上下文切换，预防了多线程可能产生的竞争问题]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zuul-实现灰度发布]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F20%2FZuul-%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%2F</url>
    <content type="text"><![CDATA[​ 一般情况下，我们要发布新版本了，在不确定正确性的情况下，我们会选择先部分节点升级，然后再让一些特定的流量进入到这些新节点，完成测试后再全量发布。这就是灰度发布。 ​ 在Eureka中注册多个服务后，如果一个服务有多个实例，那么默认会走ribbon的软负载均衡来进行分发请求。而要完成灰度发布，要做的就是修改ribbon的负载策略。在SpringCloud体系中，完成这件事，一般都是根据Eureka的metadata进行自定义元数据，然后修改Ribbon的规则。 ​ 我们可以用数据库来动态开启灰度发布和指定灰度发布的请求，当然你也可以用Apollo配置中心、Redis、ZooKeeper，其实都可以。先创建一个灰度发布启用表： 1234567CREATE TABLE `gray_release_config` ( `id` int(11) NOT NULL AUTO_INCREMENT, `service_id` varchar(255) DEFAULT NULL, `path` varchar(255) DEFAULT NULL, `enable_gray_release` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 ​ 其中“enable_gray_release”表示是否启用灰度发布，默认数字0是不启动，1启动。然后插入一条数据，方便我们测试： 1INSERT INTO gray_release_config VALUES(1, 'order-service', '/order', 0) ​ 首先，我们需要在Zuul项目里添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;io.jmnarloch&lt;/groupId&gt; &lt;artifactId&gt;ribbon-discovery-filter-spring-cloud-starter&lt;/artifactId&gt; version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt; ​ 接着在网关中新建给表的实体类： 1234567891011121314151617181920212223242526272829303132333435package com.zhss.demo.zuul.gateway;public class GrayReleaseConfig &#123; private int id; private String serviceId; private String path; private int enableGrayRelease; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getServiceId() &#123; return serviceId; &#125; public void setServiceId(String serviceId) &#123; this.serviceId = serviceId; &#125; public String getPath() &#123; return path; &#125; public void setPath(String path) &#123; this.path = path; &#125; public int getEnableGrayRelease() &#123; return enableGrayRelease; &#125; public void setEnableGrayRelease(int enableGrayRelease) &#123; this.enableGrayRelease = enableGrayRelease; &#125; &#125; ​ 然后我们可以编写一个定时器，定时获取灰度表的信息，看哪些服务需要灰度发布，新建类GrayReleaseConfigManager： 1234567891011121314151617181920212223242526272829303132333435363738394041package com.zhss.demo.zuul.gateway;import java.util.List;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Configuration;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.scheduling.annotation.EnableScheduling;import org.springframework.scheduling.annotation.Scheduled;import org.springframework.stereotype.Component;@Component@Configuration @EnableScheduling public class GrayReleaseConfigManager &#123; private Map&lt;String, GrayReleaseConfig&gt; grayReleaseConfigs = new ConcurrentHashMap&lt;String, GrayReleaseConfig&gt;(); @Autowired private JdbcTemplate jdbcTemplate; @Scheduled(fixedRate = 1000) private void refreshRoute() &#123; List&lt;GrayReleaseConfig&gt; results = jdbcTemplate.query( "select * from gray_release_config", new BeanPropertyRowMapper&lt;&gt;(GrayReleaseConfig.class)); for(GrayReleaseConfig grayReleaseConfig : results) &#123; grayReleaseConfigs.put(grayReleaseConfig.getPath(), grayReleaseConfig); &#125; &#125; public Map&lt;String, GrayReleaseConfig&gt; getGrayReleaseConfigs() &#123; return grayReleaseConfigs; &#125;&#125; ​ 然后再编写一个Zuul的过滤器，实现灰度发布的逻辑： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package com.zhss.demo.zuul.gateway;import org.springframework.context.annotation.Configuration;import com.netflix.zuul.ZuulFilter;import com.netflix.zuul.context.RequestContext;import io.jmnarloch.spring.cloud.ribbon.support.RibbonFilterContextHolder;import static org.springframework.cloud.netflix.zuul.filters.support.FilterConstants.*;import java.util.Map;import java.util.Random;import javax.annotation.Resource;import javax.servlet.http.HttpServletRequest;@SuppressWarnings("unused")@Configurationpublic class GrayReleaseFilter extends ZuulFilter &#123; @Resource private GrayReleaseConfigManager grayReleaseConfigManager; /** * 过滤的优先级，数字越大，级别越低 * @return */ @Override public int filterOrder() &#123; return PRE_DECORATION_FILTER_ORDER - 1; &#125; @Override public String filterType() &#123; return PRE_TYPE; &#125; /** * 是否执行该过滤器 * @return */ @Override public boolean shouldFilter() &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); String requestURI = request.getRequestURI(); // http://localhost:9000/order/order?xxxx Map&lt;String, GrayReleaseConfig&gt; grayReleaseConfigs = grayReleaseConfigManager.getGrayReleaseConfigs(); for(String path : grayReleaseConfigs.keySet()) &#123; if(requestURI.contains(path)) &#123; GrayReleaseConfig grayReleaseConfig = grayReleaseConfigs.get(path); if(grayReleaseConfig.getEnableGrayRelease() == 1) &#123; System.out.println("启用灰度发布功能"); return true; &#125; &#125; &#125; System.out.println("不启用灰度发布功能"); return false; &#125; /** * 过滤器的具体逻辑 * @return */ @Override public Object run() &#123;// RequestContext ctx = RequestContext.getCurrentContext();// HttpServletRequest request = ctx.getRequest();// String gray = request.getParameter("gray");//// if("true".equals(gray)) &#123;// RibbonFilterContextHolder.getCurrentContext().add("version", "new");// &#125; else &#123;// RibbonFilterContextHolder.getCurrentContext().add("version", "current");// &#125; Random random = new Random(); int seed = random.nextInt(100); if (seed == 50) &#123; // put the serviceId in `RequestContext` RibbonFilterContextHolder.getCurrentContext() .add("version", "new"); &#125; else &#123; RibbonFilterContextHolder.getCurrentContext() .add("version", "old"); &#125; return null; &#125;&#125; ​ 上面的代码主要还是看run()方法的实现。注释掉的代码是通过判断请求连接中是否包含“gray”参数，如果包含gray参数并且它的值为“true”，则将流量引到新的节点。而没有注释的代码则是根据随机数seed的值来引流。当你希望有10%的流量引到新节点时，可以将if(seed == 50)改成 seed &gt;= 90或者其他。 ​ 最后，就是在要升级的服务配置上增加metadata的自定义数据即可，根据上述的代码，我们应该在要升级的服务的配置文件中增加：eureka: instance: metadata-map: version: new。在没升级的服务的配置文件中增加：eureka: instance: metadata-map: version: old ​ 这样，基于Zuul的灰度发布功能就实现了。当然，基于灰度发布这块，国内有了更强大的开源框架Nepxion Discovery。Nepxion Discovery是一款对Spring Cloud Discovery服务注册发现、Ribbon负载均衡、Feign和RestTemplate调用的增强中间件，感兴趣的朋友可以去官方的github上查看：https://github.com/Nepxion/Discovery]]></content>
      <categories>
        <category>分布式</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis分布式锁的实现原理]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F20%2FRedis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[本节思维导图 ​ 目前基于Redis实现的分布式锁常用的框架是Redisson,它的使用比较简单，在项目中引入Redisson的依赖，然后基于Redis实现分布式锁的加锁与释放锁，如下所示： ​ 接下来我们就说一下Redisson这个框架对于Redis分布式锁的实现原理。 Redis分布式锁的底层原理​ Redisson这个框架对Redis分布式锁的实现原理图如下： 加锁机制​ 某个客户端要加锁。如果该客户端面对的是一个Redis Cluster集群，它首先会根据hash节点选择一台机器，这里注意，仅仅只是选择一台机器。紧接着就会发送一段lua脚本到redis上，lua脚本如下所示： ​ 使用lua脚本，可以把一大堆业务逻辑通过封装在lua脚本发送给redis，保证这段赋值业务逻辑执行的原子性。在这段脚本中，这里KEYS[1]代表的是你加锁的那个key，比如说：RLock lock = redisson.getLock(“myLock”);这里你自己设置了加锁的那个锁key就是“myLock”。 ​ ARGV[1]代表的就是锁key的默认生存时间，默认30秒。ARGV[2]代表的是加锁的客户端的ID，类似于下面这样：8743c9c0-0795-4907-87fd-6c719a6b4586:1。 ​ 脚本的意思大概是：第一段if判断语句，就是用“exists myLock”命令判断一下，如果你要加锁的那个key不存在，就可以进行加锁。加锁就是用“hset myLock 8743c9c0-0795-4907-87fd-6c719a6b4586:1 1”命令。通过这个命令设置一个hash数据结构，这个命令执行后，会出现一个类似下面的数据结构： ​ 上述就代表“8743c9c0-0795-4907-87fd-6c719a6b4586:1”这个客户端对“myLock”这个锁key完成了加锁。接着会执行“pexpire myLock 30000”命令，设置myLock这个锁key的生存时间是30秒。好了，到此为止，ok，加锁完成了。 锁互斥机制​ 如果这个时候客户端B来尝试加锁，执行了同样的一段lua脚本。第一个if判断会执行“exists myLock”，发现myLock这个锁key已经存在。接着第二个if判断，判断myLock锁key的hash数据结构中，是否包含客户端B的ID，但明显没有，那么客户端B会获取到pttl myLock返回的一个数字，代表myLock这个锁key的剩余生存时间。此时客户端B会进入一个while循环，不听的尝试加锁。 watch dog自动延期机制​ 客户端A加锁的锁key默认生存时间只有30秒，如果超过了30秒，客户端A还想一直持有这把锁，怎么办？其实只要客户端A一旦加锁成功，就会启动一个watch dog看门狗，它是一个后台线程，会每隔10秒检查一下，如果客户端A还持有锁key，那么就会不断的延长锁key的生存时间。 可重入加锁机制​ 客户端A已经持有锁了，然后可重入加锁，如下代码所示： ​ 这个时候lua脚本是这样执行的：第一个if判断不成立，“exists myLock”会显示锁key已经存在了。第二个if判断会成立，因为myLock的hash数据结构中包含的那个ID，就是客户端A的ID，此时就会执行可重入加锁的逻辑，它会用“incrby myLock 8743c9c0-0795-4907-87fd-6c71a6b4586:1 1 ”这个命令对客户端A的加锁次数，累加1，此时myLock的数据结构变成下面这样： ​ 即myLock的hash数据结构中的那个客户端ID，就对应着加锁的次数。 释放锁机制​ 执行lock.unlock()，就可以释放分布式锁。释放逻辑是：每次对myLock数据结构中的那个加锁次数减1，如果加锁次数为0了，说明客户端已经不再持有锁了，此时就会用“del MyLock”命令，从redis里删除了这个key。然后另外的客户端B就可以尝试完成加锁了。 上述Redis分布式锁的缺点​ 上面方案的最大问题，就是如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的master slave实例，但是这个过程中如果发送redis master宕机，主备切换，redis slave变为了redis master。 ​ 这就会导致客户端B来尝试加锁的时候，在新的redis master上完成了加锁，而客户端A也以为自己成功加了锁，此时就会导致多个客户端对一个分布式锁完成了加锁。这时就会导致各种脏数据的产生。 ​ 所以这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：在redis master实例宕机的时候，可能导致多个客户端同时完成加锁。]]></content>
      <categories>
        <category>分布式</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper分布式锁的实现原理]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F19%2FZooKeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[ZooKeeper分布式锁机制​ 本文将基于常用的ZooKeeper分布锁实现框架–Curator，说一下这个框架对ZooKeeper分布式锁的实现。 ​ 首先模拟一下两个客户端一起争抢ZK上的一把分布式锁的场景： ​ ZK里有一把锁，这个锁就是ZK上的一个节点。然后两个客户端都要来获取这个锁。假设客户端A抢先一步，对ZK发起了加分布式锁的请求，这个加锁请求是用到了ZK的“临时顺序节点”。简答来说就是直接在“my_lock”这个锁节点下，创建一个顺序节点，这个节点有ZK内部自行维护的一个节点序号。 ​ 例如第一个客户端来搞一个顺序节点，ZK内部会起个名字叫xxx-00001。然后第二个客户端搞一个顺序节点，ZK可能会起个名字叫xxx-00002。规律就是最后一个数字都是依次递增的，从1开始递增，ZK会维护这个顺序。 ​ 所以这个时候，假如客户端A先发起请求，就会搞出一个顺序节点，如图所示： ​ 客户端A发起一个加锁请求，先会在你要加锁的node下搞一个临时顺序节点，节点名字由Curator框架自己生成出来，但最后一个数字是“1”，因为客户端是第一个发起请求的。 ​ 客户端A常见完一个节点后，它会查一下“my_lock”这个锁节点下的所有子节点，并且这些子节点都是按照序号排序的，这个时候他大概会拿到一个集合： ​ 接着客户端A会走一个关键性的判断：我创建的那个顺序节点，是不是排在第一个？如果是的话，那我就可以加锁了。因为我是第一个创建顺序节点的人，所以我是第一个尝试加分布式锁的人。 ​ 客户端A加完锁了，客户端B过来想要加锁，这时它会先在“my_lock”这个锁节点下创建一个临时顺序节点，此时名字大概会是“xxx-00002” ​ 客户端B因为是第二个来创建顺序节点的，所以ZK内部会维护序号为“2”。接着客户端B会走加锁判断逻辑，查询“my_lock”锁节点下的所有子节点，按照顺序排列，类似于： ​ 同时检查自己创建的顺序节点，是不是集合中的第一个？如果不是，那就加锁失败。失败之后，客户端B就会通过ZK的API对他的顺序节点的上一个顺序节点加一个监听器 ​ 接着，客户端A加锁之后，逻辑处理完后就会释放锁，释放锁实际就是把ZK里创建的顺序节点“xxx-00001”给删除掉。删除了节点之后，ZK会负责通知监听这个节点的监听器，也就是客户端B的监听器说锁释放了。 ​ 此时客户端B的监听器感知到了上一个顺序节点被删除，也就是排在他之前的某个客户单释放了锁，此时客户端B重新尝试去获取锁，也就是获取“my_lock”节点下的子节点集合： ​ 然后客户端B判断自己是否是集合中的第一个顺序节点，如果是，直接完成加锁，运行完业务代码后，再次释放锁。 总结​ 总结一下多个客户端争抢一个ZK分布式锁的原理： 客户端上来直接创建一个锁节点下的一个接一个的临时顺序节点 如果自己不是第一个节点，就对自己上一个节点加监听器 只要上一个节点释放锁，自己就排到前面去，相当于一个排队机制。 ​ 而且用临时加节点的另一个好处就是，如果某个客户端创建临时顺序节点之后，自己宕机了也没关系，ZK感知到那个客户端宕机，会自动删除对应的临时顺序节点，相当于自动释放锁。 ​ 最后看一下用Curator框架进行加锁和释放锁的一个过程：]]></content>
      <categories>
        <category>分布式</category>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单了解ZooKeeper]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F19%2F%E7%AE%80%E5%8D%95%E4%BA%86%E8%A7%A3ZooKeeper%2F</url>
    <content type="text"><![CDATA[本节思维导图 ZooKeeper的数据结构​ ZooKeeper的数据结构，跟Unix文件系统非常类似，可以看做是一颗树，每个节点叫做ZNode，每一个节点可以通过路径来标识： ​ ZooKeeper的节点我们称之为ZNode，ZNode分为两种类型： 短暂/临时：当客户端和服务端断开连接后，所创建的ZNode（节点）会自动删除 持久：当客户端和服务端断开连接后，所创建的ZNode不会删除 ​ 这些节点由可以分成另外两种类型： 普通节点 带顺序号节点 监听器​ ZooKeeper之所以能实现那么多功能，最主要还是配合了监听器。 ​ 常见的监听器有以下两个功能： 监听ZNode节点的数据变化 监听子节点的增减变化 ​ 通过监听+ZNode节点，Zookeeper就可以实现比较多的功能了 ZooKeeper的作用统一配置管理​ 比如现在有三个系统A、B、C，他们有三份配置ASystem.yml、BSystem.yml、CSystem.yml，然后，这三份配置又非常类似，很多配置项几乎一样。此时如果我们要改变其中一份配置项的信息，很可能另外两份都要改，并且改了配置项的系统很能就要重启系统。 ​ 于是我们希望把ASystem.yml、BSystem.yml、CSystem.yml相同的配置项抽取出来成一份公用的配置common.yml，并且即使common.yml改了，也不需要系统A、B、C重启。 ​ 解决方案是我们可以把common.yml这份配置放在ZooKeeper的ZNode节点中，系统A B C监听这个节点有无变更，变更了就及时响应。 ​ 具体实现可以大佬写的 基于zookeeper实现统一配置管理 统一命名服务​ 统一命名服务的理解其实跟域名一样，是我们为这某一部分的资源给它取另一个名字，别人通过这个名字就可以拿到对应的资源。 ​ 例如我们有一个域名叫www.test.com。但这个域名下有多台机器： 192.168.1.1 192.168.1.2 192.168.1.3 192.168.1.4 别人访问www.test.com即可访问到我的机器，而不是通过IP去访问。 分布式锁​ 详情请参考这篇 ZooKeeper的分布式锁的实现原理 集群管理​ 还是以三个系统A B C为例，在ZooKeeper中创建临时节点即可， ​ 只要系统A挂了，那么/groupMember/A这个节点就会删除，通过监听groupMember下的子节点，系统B和C就能感知到系统A挂了，新增也是同理。 ​ 除了能感知节点的上下线变化，Zookeeper还可以实现动态选举Master的功能（如果集群是主从结构模式下）。原理也很简单，如果想要实现这个功能，只要ZNode节点的类型是带顺序号的临时节点就好了。ZooKeeper会每次选举最小编号的作为Master，如果Master挂了，自然对应的ZNode节点就会删除，然后让新的最小编号作为Master，这样就可以实现动态选举的功能。]]></content>
      <categories>
        <category>分布式</category>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务方案]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F16%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[本节思维导图 目前分布式事务的实现方案主要有以下5种： XA方案 TCC方案 本地消息表 可靠消息最终一致性方案 最大努力通知方案 两阶段提交方案/XA方案​ 所谓的XA方案，就是两阶段提交。有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先询问各个数据库是否准备好了，如果数据库都准备好了，就正式提交事务，在各个数据库上执行。如果任何其中一个数据库回答不OK，那么就回滚事务。 ​ 这种分布式方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复制的事务，效率很低。绝对不适合高并发的场景。如果要实现，可以基于Spring+JTA就可以实现。 ​ 这个方案，一般很少用。一般来说某个系统内部如果出现跨多个库的操作，是不合规的。即便是现在的微服务，一个大的系统分成十几个甚至几百个服务。一般来说，都是要求每个服务只能操作自己对应的一个数据库。如果要操作别的服务对应的库，不允许直接连接，违反微服务架构的规范，你随便交叉胡乱访问，几百个服务的话，全体乱套，这样的一套服务是没法管理的，没法治理的，可能会出现数据被别人改错，自己的库被别人写挂等情况。 如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许交叉访问别人的数据库。 TCC方案​ tcc全称是：try、confirm、cancel Try阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预定。 Confirm：这个阶段说的是在各个服务中执行实际的操作。 Cancel：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）。 ​ 这种方案也用的比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于自己写的代码来回滚和补偿的，会造成补偿代码巨大。一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，会使用TCC，严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性。而且最好是你的各个业务执行的时间都比较短。但是一般情况下尽量不要使用TCC方案，自己手写回滚逻辑或者是补偿代码，都是很恶心的，业务代码很难维护。 本地消息表​ 本地消息表的大概意思如下： A系统在自己本地一个事务里操作同时，插入一条数据到消息表； 接着A系统将这个消息发送到MQ中去； B系统接收到消息之后，在一个事务里，往自己本地消息表插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过，那么此时这个事务会回滚，这样保证不会重复处理消息； B系统执行成功之后，就会更新自己本地信息表的状态以及A系统信息表的状态； 如果B系统处理失败，那么久不会更新信息表状态，那么此时A系统会定时扫描自己的消息表，如果有未处理的消息，则会发送到MQ中去，让B再次处理； 这个方案保证了最终一致性，哪怕B事务失败了，但是A会不断重发信息一致到B那边成功为止。 这个方案最大的问题是就是严重依赖于数据库的消息表来管理事务，如果是高并发场景，很难扩展，所以一般比较少用。 可靠消息最终一致性方案​ 这个的意思，就是干脆不用本地消息表了，直接基于MQ来实现事务，比如阿里的RocketMQ就支持消息事务，大概的思路如下： A系统先发送一个prepared消息到mq，如果这个prepared消息发送失败那么就直接取消操作别执行了； 如果这个消息发送成功了，那么接着执行本地事务，如果成功就告诉MQ发送确认信息，如果失败就告诉mq回滚消息； 如果发送了确认消息，那么此时B系统会接收到确认信息，然后执行本地事务； MQ会自动定时轮询所有prepared消息回调你的接口，问你这个消息是不是本地事务处理失败了，所有没发送确认消息的信息，是继续重试还是回滚？一般来说这里你就可以查下数据库之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。 这个方案里，要是系统B的事务失败了，那就重试，自动不断地重试直到成功，如果实在不行，那就针对重要的资金业务进行回滚，比如B系统本地回滚后，想办法通知系统A也回滚，或者是发送警报由人工来手工回滚和补偿 ​ 这个方案还是比较合适的，目前国内的互联网公司大部分都是这样设计。你可以使用RocketMQ，也可以使用其他消息队列封装一套类似的逻辑出来。 最大努力通知方案​ 这个方案的大概思路就是： 系统A本地事务执行完之后，发送个消息到MQ； 这里会有个专门消费MQ的最大努力通知服务，这个服务会消费MQ然后写入数据中记录下来，或者是放入个内存队列里，接着调用系统B的接口； 要是系统B执行成功就OK了，要是系统B执行失败了，那么最大努力同时服务就定时尝试重新调用系统B，反复N次，最后还是不行就放弃。 总结​ 基本上，一些特别严格的场景，用的是TCC来保证强一致性，例如严格要求资金绝对不能错的场景；其他的一些场景基于阿里的RocketMQ来实现分布式事务，例如一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么敏感，可以用可靠消息最终一致性方案。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计一个能抗住上万服务实例的注册中心]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F15%2F%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%83%BD%E6%8A%97%E4%BD%8F%E4%B8%8A%E4%B8%87%E6%9C%8D%E5%8A%A1%E5%AE%9E%E4%BE%8B%E7%9A%84%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%2F</url>
    <content type="text"><![CDATA[​ 之前说过ZooKeeper和Eureka由于自己的特性，都不太适合支撑大规模的服务实例。Eureka是peer-to-peer模式，每台机器都是高并发请求的话会有瓶颈。而ZooKeeper是每次服务上下线，就会全量通知其他服务，导致网络宽带被打满，这也是一个瓶颈。具体可以查看服务注册中心的选型调研这篇文章。那么怎样才能实现一个能抗住上万服务实例的注册中心呢？ ​ 目前大公司的服务注册中心为了能支撑大规模的服务实例，基本都是自研服务注册中心。基本的思路就是实现一个分布式服务注册中心。主要设计逻辑包括：分片存储服务注册表、支持横向扩容、每台机器均摊高并发请求、各个服务主动拉取注册表信息，避免方向通知网卡被打爆等等。 ​ 简单的原理图如下所示：]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zuul-实现动态路由]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F13%2FZuul-%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1%2F</url>
    <content type="text"><![CDATA[​ 一般情况下，Zuul需要在配置文件里写好路由信息，这样zuul才可以通过这些路由信息根据连接转发到相应的服务上去。但每增加一个服务，就需要停下网关去重新编写配置文件，这样就比较麻烦了。因此，就有人提出了动态路由的方法。 ​ 动态路由有很多方式实现，这里主要讲一下用数据库去实现动态路由。 ​ 首先，先创建一个表，用于存储路由信息： 1234567891011CREATE TABLE `gateway_api_route` ( `id` varchar(50) NOT NULL, `path` varchar(255) NOT NULL, `service_id` varchar(50) DEFAULT NULL, `url` varchar(255) DEFAULT NULL, `retryable` tinyint(1) DEFAULT NULL, `enabled` tinyint(1) NOT NULL, `strip_prefix` int(11) DEFAULT NULL, `api_name` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 ​ 该表结构主要是按照Zuul的ZuulProperties.ZuulRoute类设计的： ​ 插入一条数据，方便测试： 1INSERT INTO gateway_api_route (id, path, service_id, retryable, strip_prefix, url, enabled) VALUES ('order-service', '/order/**', 'order-service',0,1, NULL, 1); ​ 然后编写表gateway_api_route相应的实体类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class GatewayApiRoute &#123; private String id; private String path; private String serviceId; private String url; private boolean stripPrefix = true; private Boolean retryable; private Boolean enabled; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getPath() &#123; return path; &#125; public void setPath(String path) &#123; this.path = path; &#125; public String getServiceId() &#123; return serviceId; &#125; public void setServiceId(String serviceId) &#123; this.serviceId = serviceId; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public boolean isStripPrefix() &#123; return stripPrefix; &#125; public void setStripPrefix(boolean stripPrefix) &#123; this.stripPrefix = stripPrefix; &#125; public Boolean getRetryable() &#123; return retryable; &#125; public void setRetryable(Boolean retryable) &#123; this.retryable = retryable; &#125; public Boolean getEnabled() &#123; return enabled; &#125; public void setEnabled(Boolean enabled) &#123; this.enabled = enabled; &#125; &#125; ​ 接下来就开始编写动态路由的实现逻辑，其实基本逻辑就是从数据库里取出路由数据，然后封装成ZuulProperties.ZuulRoute。主要代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.zhss.demo.zuul.gateway;import org.springframework.beans.BeanUtils;import org.springframework.cloud.netflix.zuul.filters.RefreshableRouteLocator;import org.springframework.cloud.netflix.zuul.filters.SimpleRouteLocator;import org.springframework.cloud.netflix.zuul.filters.ZuulProperties;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.util.StringUtils; import java.util.LinkedHashMap;import java.util.List;import java.util.Map;public class DynamicRouteLocator extends SimpleRouteLocator implements RefreshableRouteLocator &#123; private JdbcTemplate jdbcTemplate; private ZuulProperties properties; public void setJdbcTemplate(JdbcTemplate jdbcTemplate) &#123; this.jdbcTemplate = jdbcTemplate; &#125; public DynamicRouteLocator(String servletPath, ZuulProperties properties) &#123; super(servletPath, properties); this.properties = properties; &#125; @Override public void refresh() &#123; doRefresh(); &#125; @Override protected Map&lt;String, ZuulProperties.ZuulRoute&gt; locateRoutes() &#123; LinkedHashMap&lt;String, ZuulProperties.ZuulRoute&gt; routesMap = new LinkedHashMap&lt;String, ZuulProperties.ZuulRoute&gt;(); // 加载application.yml中的路由表 routesMap.putAll(super.locateRoutes()); // 加载db中的路由表 routesMap.putAll(locateRoutesFromDB()); // 统一处理一下路由path的格式 LinkedHashMap&lt;String, ZuulProperties.ZuulRoute&gt; values = new LinkedHashMap&lt;&gt;(); for (Map.Entry&lt;String, ZuulProperties.ZuulRoute&gt; entry : routesMap.entrySet()) &#123; String path = entry.getKey(); if (!path.startsWith("/")) &#123; path = "/" + path; &#125; if (StringUtils.hasText(this.properties.getPrefix())) &#123; path = this.properties.getPrefix() + path; if (!path.startsWith("/")) &#123; path = "/" + path; &#125; &#125; values.put(path, entry.getValue()); &#125; System.out.println("路由表：" + values); return values; &#125; private Map&lt;String, ZuulProperties.ZuulRoute&gt; locateRoutesFromDB() &#123; Map&lt;String, ZuulProperties.ZuulRoute&gt; routes = new LinkedHashMap&lt;&gt;(); List&lt;GatewayApiRoute&gt; results = jdbcTemplate.query( "select * from gateway_api_route where enabled = true ", new BeanPropertyRowMapper&lt;&gt;(GatewayApiRoute.class)); for (GatewayApiRoute result : results) &#123; if (StringUtils.isEmpty(result.getPath()) ) &#123; continue; &#125; if (StringUtils.isEmpty(result.getServiceId()) &amp;&amp; StringUtils.isEmpty(result.getUrl())) &#123; continue; &#125; ZuulProperties.ZuulRoute zuulRoute = new ZuulProperties.ZuulRoute(); try &#123; BeanUtils.copyProperties(result, zuulRoute); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; routes.put(zuulRoute.getPath(), zuulRoute); &#125; return routes; &#125; &#125; 然后在新建一个配置类DynamicRouteConfiguration 12345678910111213141516171819202122232425262728package com.zhss.demo.zuul.gateway;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.autoconfigure.web.ServerProperties;import org.springframework.cloud.netflix.zuul.filters.ZuulProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.jdbc.core.JdbcTemplate; @Configurationpublic class DynamicRouteConfiguration &#123; @Autowired private ZuulProperties zuulProperties; @Autowired private ServerProperties server; @Autowired private JdbcTemplate jdbcTemplate; @Bean public DynamicRouteLocator routeLocator() &#123; DynamicRouteLocator routeLocator = new DynamicRouteLocator( this.server.getServletPrefix(), this.zuulProperties); routeLocator.setJdbcTemplate(jdbcTemplate); return routeLocator; &#125; &#125; 这样就差不多，最后再实现一个定时器，定时刷新路由信息： 1234567891011121314151617181920212223242526272829package com.zhss.demo.zuul.gateway;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.cloud.netflix.zuul.RoutesRefreshedEvent;import org.springframework.cloud.netflix.zuul.filters.RouteLocator;import org.springframework.context.ApplicationEventPublisher;import org.springframework.context.annotation.Configuration;import org.springframework.scheduling.annotation.EnableScheduling;import org.springframework.scheduling.annotation.Scheduled;import org.springframework.stereotype.Component;@Component@Configuration @EnableScheduling public class RefreshRouteTask &#123; @Autowired private ApplicationEventPublisher publisher; @Autowired private RouteLocator routeLocator; @Scheduled(fixedRate = 5000) private void refreshRoute() &#123; System.out.println("定时刷新路由表"); RoutesRefreshedEvent routesRefreshedEvent = new RoutesRefreshedEvent(routeLocator); publisher.publishEvent(routesRefreshedEvent); &#125; &#125; 这样一个基于zuul的动态路由功能就完成了，代码跑起来后，可以看到定时器在工作，定数刷新路由信息：]]></content>
      <categories>
        <category>分布式</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eureka的一些参数配置优化]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F13%2FEureka%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本节思维导图 ​ Eureka的默认配置是比较糟糕的，一般服务的上线和下线极端情况下需要一分多钟才能感知到，服务故障极端情况下需要两到三分钟才能感知到，这相对于ZooKeeper的秒级感知来说实在是太慢了。因此我们可以通过修改Eureka的一些配置参数来达到秒级通知的效果。 Eureka-Server端的配置eureka.server.responseCacheUpdateIntervalMs​ 这个参数表示的是Eureka中ReadWriteCacheMap的缓存数据多久会更新到ReadOnlyCacheMap中去，应为Eureka-Client是从ReadOnlyCacheMap拉取数据的。这个参数默认是30秒更新一次ReadOnlyCacheMap，我们可以改为3秒更新一次：eureka.server.response-cache-update-interval-ms = 3000 eureka.server.evictionIntervalTimerInMs​ 这个参数表示的是Eureka-Server中的缓存数据每隔多少秒主动失效。默认是60秒主动清空服务列表，我们可以改为6秒：eureka.server.eviction-interval-timer-in-ms = 6000 eureka.instance.leaseExpirationDurationInSeconds​ 服务过期时间配置，超过这个时间没有接收到心跳就会认为该服务实例已经挂了。并将该服务实例从注册表中剔除掉。默认情况下是90秒，我们可以设置为9秒：eureka.instance.lease-expiration-duration-in-seconds = 9 Eureka-Client端的配置eureka.client.registryFetchIntervalSeconds​ 这个参数表示的是Eureka-Client拉取数据，刷新本地缓存的时间，默认是每30秒拉取一次数据，我们可以将速度提高10倍，改为3秒：eureka.client.registry-fetch-interval-seconds = 3 eureka.instant.leaseRenewalIntervalInSeconds​ 这个参数表示的是Eureka-Client每隔多久发送一次心跳，默认是30秒发送一次心跳到Eureka-Server上。我们可以改成3秒：eureka.instant.lease-renewal-interval-in-seconds = 30]]></content>
      <categories>
        <category>分布式</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务注册中心的选型调研]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F11%2F%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E9%80%89%E5%9E%8B%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[本节思维导图 ​ 目前市场上使用最多的服务注册中心应该是Eureka和Zookeeper，当然Consul和Nacos，也在慢慢崛起。本文主要从集群模式、一致性保障、时效性和容量这几个角度来讨论Eureka和ZooKeeper的区别。 服务注册发现的集群模式Eureka​ Eureka的集群模式，简单地说就是peer-to-peer。部署一个集群，但是集群里每个机器的地位是对等的，各个服务可以向任何一个Eureka实例进行服务注册和发现，集群里的任何一个Eureka实例收到请求后，会自动同步给其他所有的Eureka实例。除了注册信息，服务发送的心跳信息也会同步到其他Eureka实例上。如图所示： ZooKeeper​ ZooKeeper的集群模式，简单就是Leader+Follower，其中只有Leader可以负责写，即服务注册，领完，它还负责把数据同步给Follow。服务发现的时候，Leader/Follow都可以读。 一致性保障：CP or AP​ CAP原则包含如下三个元素： C（Consistency）：一致性。在分布式系统中的所有数据备份，在同一时刻具有同样的值，所有节点在同一时刻读取的数据都是最新的数据副本。 A（Availability）：可用性。好的相应性能。完全的可用性指的是在任何故障模型下，服务都会在有限的时间内处理完成并进行相应。 P（Partition tolerance）：分区容错性。尽管网络上有部分消息丢失，但系统仍然可以继续工作。 CAP原理证明，任何分布式系统只可同时满足以上两点，无法三者兼顾。由于关系型数据库是单节点无复制的，因此不具有分区容忍性，但是具有一致性和可用性；而分布式的服务化系统都需要满足分区容忍性，那么我们必须在一致性和可用性之间进行权衡。 Eureka​ Eureka是AP模式的，即它牺牲了一致性，而实现可用性和分区容错性。因为Eureka是peer-to-peer模式，可能数据还没有同步互过去，自己就挂掉了，但服务实例依然可以从别的Eureka实例上拉去注册表，但是看到的数据就不是最新的收据了。但Eureka保证了最终一致性。例如服务A除了注册服务之外还会发送心跳信息，当服务A发现Eureka1实例挂掉之后，会向另一个活着的Eureka2实例发送心跳信息，Eureka2就能感知到服务A的存在并更新注册表的数据，从而实现最终一致性。 ZooKeeper​ ZooKeeper是CP模式的。ZooKeeper是有一个Leader节点会接收数据，然后同步其他节点，一旦Leader挂掉了，就要重新选举Leader，这个过程为了一致性，就会牺牲看可用性，会不可用一段时间，那么就可以继续写数据了，保证了一致性。即ZooKeeper是同步数据期间和Leader选举期间，都处于不可用阶段，等结束之后就可以继续使用，但这样却保证了强一致性。 服务注册发现的时效性​ ZooKeeper的时效性更好，注册或者是挂了，一般秒级就能感知到。 ​ Eureka，默认配置非常糟糕。服务发现感知要到几十秒，甚至分钟级别。上线一个新的服务，到其他服务可以发现它，极端情况下可能要一分钟的时间。（30秒ReadWriteCache更新ReadOnlyCacheMap数据，再30秒服务实例去拉取ReadOnlyCacheMap的数据）。 ​ 在默认情况下，服务故障，隔60秒才去检查心跳，发现这个服务上一次心跳是在30秒之前。在隔60秒去检查心跳，超过90秒没有心跳，才会认为这个服务已经挂了，这样子就已经过去两分钟了。 ​ 因此极端情况下，你的服务挂掉了，到其他服务感知到，可能需要两三分钟时间，比较漫长。 容量​ Eureka很难支撑大规模的服务实例，因为每个Eureka实例都要接受所有服务的注册请求信息和心跳信息，实例多了压力太大扛不住，很难做到几千服务实例。比如服务实例太多，达到上千个，每秒钟的有上千个心跳信息，那要同时同步到其余心跳信息。压力会比较大。 ​ ZooKeeper同样不适合大规模的服务实例，因为服务上线的时候，需要瞬间推送数据通知到所有的其他服务实例，所以一旦服务规模太大，到了几千个服务实例的时候，会导致网络带宽被大量占用。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>服务注册</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud常用组件原理]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F11%2FSpringCloud%E5%B8%B8%E7%94%A8%E7%BB%84%E4%BB%B6%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[​ Spring Cloud是一个全家桶式的技术栈，包含了很多组件。本文主要简单介绍下最核心的几个组件的底层原理。包括Eureka、Ribbon、Feign、Hystrix和Zuul这几个组件。 业务场景介绍​ 文章先假定一个业务场景：现在开发一个电商系统，要实现支付订单的功能，流程如下： 创建一个订单之后，如果用户立刻支付了这个订单，我们需要将订单状态更新为“已支付” 扣减相应的商品库存 通知仓储中心，进行发货 给用户的这次购物增加相应的积分 针对上述流程，我们需要有订单服务、库存服务、仓储服务、积分服务。整个流程的大体思路如下： 用户针对一个订单完成支付之后，就会去找订单服务，更新订单状态 订单服务调用库存服务，完成相应功能 订单服务调用仓储服务，完成相应功能 订单服务调用积分服务，完成相应功能 如图所示： SpringCloud核心组件：Eureka​ Eureka是微服务架构中的注册中心，主要功能是服务注册与发现和心跳检测与故障。在上述场景中，订单服务不知道其他其他服务在哪台机器上，此时就需要一个注册中心，来管理各个服务的地址，如图所示： ​ 如上图所示，所有服务都有一个Eureka Client组件，这个组件专门负责将这个服务的信息注册到Eureka Server中，也就相当于告诉了Eureka Server自己在哪台服务器上，监听这哪个端口。而Eureka中维护了一个注册表，保存着各个服务的机器和端口号。 ​ 新增服务、下线服务都是直接操作Eureka-Server的注册表的，而注册表变更时为了并发安全是会加锁操作的（使用ReentrantReadWriteLock）然后注册表一变更，立刻清楚掉ReadWrite缓存的数据，并重新写入新数据。服务从ReadOnlyCache上拉取服务 ，并缓存到本地。而Eureka-Server采用两个缓存，是为了避免并发冲突。 ​ 假设没有ReadOnlyCacheMap，万一刚好注册表发生变更的时候，ReadWriteCacheMap会被失效掉，所以客户端的请求也就直接来读注册表了，会涉及到锁的操作，弄了个ReadOnlyCacheMap可以大大减少锁操作发生的概率。 ​ 假设没有ReadWriteCacheMap，那么ReadOnlyCacheMap每隔30秒刷新的时候就只能跟注册表比较了，如果此时注册表也发生了变更，也会涉及到锁的操作，因为ReadWriteCacheMap的存在（因为ReadWriteCacheMap是每隔180秒才会主动失效一次）也可以大大减少这个锁操作发生的概率。 ​ 除了服务注册与发现之外，Eureka还有检测心跳的功能，以此来判断那台机器出现故障。Eureka-Client默认每30秒想Eureka发送一次心跳，而Eureka-Server会有专门的线程来检测心跳。 ​ 总结一下：Eureka拥有服务注册与发现、心跳检测与故障等功能。其中： Eureka-Client：负责将这个服务的信息注册到Eureka Server中 Eureka-Server：注册中心，里面有注册表和两个缓存，保存了各个服务所在的机器和端口。 SpringCloud核心组件：Feign​ 通过Eureka我们知道了各个服务在哪里，但如何向其他服务发起请求呢，这个就是Feign的作用。如下所示： 1234567@Component@FeignClient("tensquare-user")public interface UserClient &#123; @RequestMapping(value = "/user/incfollow/&#123;userid&#125;/&#123;x&#125;", method = RequestMethod.POST) public void incFollowcount(@PathVariable("userid")String userid, @PathVariable("x") int x);&#125; ​ 通过使用Feign，直接就是用注解定义一个FeignClient接口，然后调用那个接口就可以了，FeignClient会在底层根据你的注解，跟你指定的服务建立连接、构造请求、发起请求、获取响应、解析响应等等。 ​ 而Feign之所以能实现这些功能，关键的机制是使用了动态代理。我们根据下图来分析： 首先，如果你对某个接口定义了@FeignClient注解，Feign就会针对这个接口创建一个动态代理 接着你要是调用按个接口，本质上就是调用Feign创建的动态代理，这是核心中的核心 Feign的动态代理会根据你在接口上的@RequestMapping等注解，来动态构造出你要请求的地址。 最后针对这个地址，发起请求，解析响应 SpringCloud核心组件：Ribbon​ 如果库存系统部署子在了五台机器上，Feign怎么知道该请求哪台机器呢。这时SpringCloud Ribbon就派上永昌路 。它的作用是负载均衡，会帮你在每次请求时选择一台机器，均匀的把请求分发到各个机器上。 ​ Ribbon的负载均衡默认使用的是Round Robin轮询算法。就是说如果订单服务对库存系统发起10次请求，那就先让你请求第1台机器。然后是第2台、第3台，第4、第5，然后再来一个循环，第1、第2。。。以此类推。 ​ 此外，Ribbon和Feign以及Eureka紧密协作而完成工作的，具体如下： 首先Ribbon会从Eureka-Client获取到对应的服务注册表，也就知道了所有的服务都部署在了哪些机器上，在监听哪些端口。 然后Ribbon就可以使用默认的Round Robin算法，从中选择一台。 Feign就会针对这台机器，构造并发起请求。 SpringCloud核心组件：Hystrix​ 在微服务架构里，一个系统会有很多的服务，以本文的业务场景为例：订单服务在一个业务流程里需要调用三个服务。现在假设订单服务有100个线程可以处理请求，然后积分服务不幸挂了，每次订单服务调用积分服务的时候，都会卡住几秒，然后抛出一个超时异常。这样会导致几个问题： 1、如果系统处于高并发的场景下，大量请求涌过来的时候，订单服务的100个线程都会卡在请求积分这块，导致订单服务没有一个线程可以处理请求。 2、然后就会导致别人请求订单服务的时候，发现订单服务也挂了，不响应任何请求了。 这就是微服务架构中的服务雪崩问题。这么多服务互相调用，要是不做任何保护的话，某一个服务挂了，就会引起连锁反应，导致别的服务也挂了。 ​ 但就算积分系统挂了，订单服务也可以不用挂啊。结合业务来看，支付订单的时候，只要把库存减了，然后通知仓库发货就可以了；如果积分系统挂了，大不了恢复之后，再手工恢复数据，不应该因为一个积分服务挂了，就直接导致订单服务也挂了。 ​ 这个时候就要使用Hystrix了。Hystrix是隔离、熔断以及降级的一个框架。就是Hystrix会搞很多个小小的线程池，例如订单服务请求库存服务是一个线程池，请求仓储服务是一个线程池，请求积分服务是一个线程池，每个线程池里的线程就仅仅用于请求哪个服务。 ​ 比如积分系统挂了，会导致订单服务里的那个调用积分服务的线程都卡死不能工作了，但是由于订单服务调用库存系统、仓储系统的这两个线程池都是正常工作的，所以这两个服务不会受到任何影响。 ​ 这个时候如果别人请求订单服务，订单服务还是可以正常调用库存服务扣减库存，调用仓储服务通知发货。只不过调用积分服务的时候，每次都会报错。但是如果积分服务都挂了，每次调用都要去卡住几秒钟干啥呢？有意义吗？当然没有！所以我们直接对积分服务熔断不就得了，比如在5分钟内请求积分服务直接就返回了，不要去走网络请求卡住几秒钟，这个过程，就是所谓的熔断！ ​ 而且积分系统挂了，我们还可以来个降级：每次调用积分服务，你就在数据库里记录一条消息，说给某某用户增加了多少积分，因为积分服务挂了，导致没增加成功！这样等积分服务恢复了，你可以根据这些记录手工加一下积分。这个过程，就是所谓的降级。 SpringCloud的核心组件：Zuul​ 说完了Hystrix，接着给大家说说最后一个组件：Zuul，也就是微服务网关。这个组件是负责网络路由的。不懂网络路由？行，那我给你说说，如果没有Zuul的日常工作会怎样？ ​ 假设你后台部署了几百个服务，现在有个前端兄弟，人家请求是直接从浏览器那儿发过来的。打个比方：人家要请求一下库存服务，你难道还让人家记着这服务的名字叫做inventory-service？部署在5台机器上？就算人家肯记住这一个，你后台可有几百个服务的名称和地址呢？难不成人家请求一个，就得记住一个？你要这样玩儿，那真是友谊的小船，说翻就翻！ ​ 上面这种情况，压根儿是不现实的。所以一般微服务架构中都必然会设计一个网关在里面，像android、ios、pc前端、微信小程序、H5等等，不用去关心后端有几百个服务，就知道有一个网关，所有请求都往网关走，网关会根据请求中的一些特征，将请求转发给后端的各个服务。 ​ 而且有一个网关之后，还有很多好处，比如可以做统一的降级、限流、认证授权、安全，等等。 总结最后再来总结一下，上述几个Spring Cloud核心组件，在微服务架构中，分别扮演的角色： Eureka：各个服务启动时，Eureka Client都会将服务注册到Eureka Server，并且Eureka Client还可以反过来从Eureka Server拉取注册表，从而知道其他服务在哪里 Ribbon：服务间发起请求的时候，基于Ribbon做负载均衡，从一个服务的多台机器中选择一台 Feign：基于Feign的动态代理机制，根据注解和选择的机器，拼接请求URL地址，发起请求 Hystrix：发起请求是通过Hystrix的线程池来走的，不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题 Zuul：如果前端、移动端要调用后端系统，统一从Zuul网关进入，由Zuul网关转发请求给对应的服务]]></content>
      <categories>
        <category>分布式</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存区域]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F09%2FJVM%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[JVM内存布局 存放类的方法区​ 这个方法区是在JDK1.8以前的版本里，代表JVM中的一块区域。主要是放从“.class”文件里加载进来的类，还会有一些类似常量池的东西放在这个区域。JDK1.8以后，这个区域改了名字，叫“Metaspace”，也叫“元空间”。 ​ 还是拿之前的代码举例，如下： 123456public class Test &#123; public static void main(String[] args) &#123; ReplicaManager replicaManager = new ReplicaManager(); &#125;&#125; ​ 这两个类加载后，就会放在这个方法区中， 执行代码指令用的程序计数器​ 假设我们的代码是这样： 1234567public class Test &#123; public static void main(String[] args) &#123; ReplicaManager replicaManager = new ReplicaManager(); replicaManager.loadReplicaFromDish(); &#125;&#125; ​ 实际上这段代码先存在于“.java”后缀的文件里，但为了能让计算机看懂这段代码，需要将这个文件经过编译器编译，把“.java”后缀的源文件编译为“.class”后缀的字节码文件。这个“.class”后缀的字节码文件里，存放的就是编译好的字节码了，字节码才是计算机可以理解的一种语言。字节码大概如下： ​ 所以首先明白一点：我们写好的Java代码会被翻译成字节码，对应各种字节码指令 ​ 现在Java代码通过JVM跑起来的第一件事情就确定了，首先Java代码被编译成字节码指令，然后字节码指令一定会被一条一条地执行，这样才能实现我们写好的代码执行的效果。当JVM加载类信息到内存之后，实际就会使用自己的字节码执行引擎，去执行我们写的代码编译出来的代码指令，那么在执行字节码指令的时候，JVM就需要一个特殊的内存区域，“程序计数器”。它是用来记录当前的字节码指令位置的，也就是记录目前执行到了哪一条字节码指令。 ​ JVM是支持多个线程的，所以你写好的代码可能会开启多个线程并发执行不同的代码，所以就会有各个线程来并发的执行不同的代码指令，因此每个线程都会有自己的一个程序计数器，专门记录当前这个线程目前执行到了哪一条字节码指令。 Java虚拟机栈​ Java代码在执行的时候，一定是线程来执行某个方法中的代码，即使是下面的代码，也会有一个main线程来执行main()方法里的代码。在main线程执行main()方法的代码指令的时候，就会通过main线程对应的程序计数器记录自己执行的指令位置。 1234567public class Test &#123; public static void main(String[] args) &#123; ReplicaManager replicaManager = new ReplicaManager(); replicaManager.loadReplicaFromDish(); &#125;&#125; ​ 但在方法里，一般会定义一些方法内的局部变量，例如上面的代码中就有一个“replicaManager”局部变量。因此JVM必须有一块保存每个方法内的局部变量等数据的，这个区域就是Java虚拟机栈。每个线程都有自己的Java虚拟机栈，比如这里的main线程就会有自己的一个Java虚拟机栈，用来存放自己执行的那些方法的局部变量。 ​ 如果线程执行了一个方法，就会对这个方法调用创建对应的一个栈帧。栈帧就有这个方法的局部变量表、操作数栈。动态链接、方法出口等信息。 ​ 比如main线程执行了main()方法，那么就会给main()方法创建一个栈帧，压入main线程的Java虚拟机栈，同时在main()方法的栈帧里，存放对应的“replicaManager”局部变量。 ​ 然后假设main()线程继续执行ReplicaManager对象里的方法，比如下面，就在“loadReplicasFromDisk”方法里定义了一个局部变量：“hasFinishedLoad”。 123456public class ReplicaManager &#123; public void loadReplicasFromDish() &#123; Boolean hasFinishedLoad = false; &#125;&#125; ​ 那么main线程执行上面的“loadReplicasFromDish”方法时，就会为“loadReplicasFromDish”方法创建一个栈帧压入线程自己的Java虚拟机栈里面去。 ​ 接着如果“loadReplicasFromDish”方法调用了另外一个“isLocalDataCorrupt()”方法，这个方法里也有自己的局部变量，如下： 123456789101112131415public class ReplicaManager &#123; public void loadReplicasFromDish() &#123; Boolean hasFinishedLoad = false; if(isLocalDataCorrupt()) &#123; &#125; &#125; public Boolean isLocalDataCorrupt() &#123; Boolean isCorrupt = false; return isCorrupt &#125;&#125; ​ 这个时候会给“isLocalDataCorrupt”方法又创建一个栈帧，压入线程的Java虚拟机里，而且“isLocalDataCorrupt”方法的栈帧的局部变量表里会有一个“isCorrupt”变量，这个“isLocalDataCorrupt”的局部变量，整个过程如下： ​ 接着如果“isLocalDataCorrupt”方法执行完毕，就会把“isLocalDataCorrupt”方法对应的栈帧从Java虚拟机栈里出栈；然后如果“loadReplicasFromDisk”方法也执行完毕，就会把“loadReplicasFromDisk”方法也从Java虚拟机栈里出栈、 ​ “JAVA虚拟机栈”这个组件的作用：调用执行任何方法时，都会给方法创建栈帧然后入栈，在栈帧里存放了这个方法对应的局部变量之类的数据，包括这个方法执行的其他相关信息，方法执行完毕之后出栈。 Java堆内存​ Java堆主要是存放我们在代码中创建的各种对象。 1234567public class Test &#123; public static void main(String[] args) &#123; ReplicaManager replicaManager = new ReplicaManager(); replicaManager.loadReplicaFromDish(); &#125;&#125; ​ 上面的“new ReplicaManager()”这个代码就是创建了一个ReplicaManager类的对象实例，这个对象实例里面会包含一些数据，如下代码所示：这个“ReplicaManager”类里的“replicaCount”就是属于这个对象实例的一个数据。而类似ReplicaManager这样的对象实例就会存放在Java堆内存里。 12345678910111213141516public class ReplicaManager &#123; private long replicaCount; public void loadReplicasFromDish() &#123; Boolean hasFinishedLoad = false; if(isLocalDataCorrupt()) &#123; &#125; &#125; public Boolean isLocalDataCorrupt() &#123; Boolean isCorrupt = false; return isCorrupt &#125;&#125; ​ Java堆内存区域里会放入类似ReplicaManager的对象，然后我们因为在main方法里创建了ReplicaManager对象，那么在线程执行main方法代码的时候，就会在main方法对应的栈帧的局部变量表里，让一个引用类型的“replicaManager”局部变量来存放ReplicaManager对象的地址。 ​ 相当于你可以认为局部变量表的“replicaManager”指向了Java堆内存里的ReplicaManager对象。 核心内存区域的全流程串讲​ 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; ReplicaManager replicaManager = new ReplicaManager(); replicaManager.loadReplicaFromDish(); &#125;&#125;public class ReplicaManager &#123; private long replicaCount; public void loadReplicasFromDish() &#123; Boolean hasFinishedLoad = false; if(isLocalDataCorrupt()) &#123; &#125; &#125; public Boolean isLocalDataCorrupt() &#123; Boolean isCorrupt = false; return isCorrupt &#125;&#125; ​ 首先，你的JVM进程会启动，就会先加载Test类到内存里，然后有一个main线程，开始执行你的Test中的main()方法。main线程是关联了一个程序计数器的，他执行到哪一行指令，就会记录在这里。 ​ 其次，就是main线程执行main()方法的时候，会在main线程相关的Java虚拟机栈里，压入一个main()方法的栈帧，接着会发现需要创建一个ReplicaManager类的实例对象，此时会加载ReplicaManager类到内存里来。 ​ 然后会创建一个ReplicaManager的对象实例分配在堆内存里，并且在main()方法的栈帧里的局部变量表引入一个“replicaManager”变量，让他引用ReplicaManager对象在Java堆内存中的地址。 ​ 接着，main线程开始执行ReplicaManager对象中的方法，会依次把自己执行到的方法对应的栈帧压入自己的Java虚拟机栈。 ​ 执行完方法之后再把方法对应的栈帧从Java虚拟机栈里出栈。 其他内存区域​ 在JDK很多底层API里，比如IO相关、网络Socket相关的，很多地方都不是JAVA代码了，而是走的native方法去调用本地操作系统里面的一些方法，可能调用的都是C语言写的方法，或者一些底层类库。在调用这种native方法时，就会有线程对应的本地方法栈，这个跟Java虚拟机栈类似的，也是存放各种native方法的局部变量表之类的信息。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM类加载机制]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F07%2FJVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本节思维导图 JVM什么情况下会加载一个类​ 首先，我们应该清楚一个类从加载到使用，一般会经过加载–&gt;链接–&gt;初始化–&gt;使用–&gt;卸载这几个过程，其实链接阶段又可以细分为三个：验证–&gt;准备–&gt;解析。所以首先要明白的一个问题就是，JVM在执行我们代码的时候，什么时候去加载一个类呢？即什么时间会从“.class”字节码文件中加载这个类到JVM内存中？ ​ 答案就是你的代码中用到这个类的时候。 ​ 比如下面有一个类，里面有一个“main()”方法作为入口，那么一旦你的JVM进程启动之后，它一定会先把这个类加载到内存里，然后从main()方法入口的代码开始执行。 12345public class Test &#123; public static void main(String[] args) &#123; &#125;&#125; 接着上面的代码中，出现了这么一行代码： 123456public class Test &#123; public static void main(String[] args) &#123; ReplicaManager replicaManager = new ReplicaManager(); &#125;&#125; 这个时候，你的代码中明显需要使用“ReplicaManager”这个对象，因此会从“ReplicaManager.class”字节码中加载对应的类到内存中使用。 简单概括就是：首先你的代码中包含“main()”方法的主类一定会在JVM启动后加载到内存中，开始执行你的“main()”方法中的代码，接着遇到你使用了别的类，此时就会从对应的“.class”字节码文件中加载对应的类到内存里来。 从使用角度出发，来看验证、准备和初始化的过程1、验证阶段​ 简单来说，这一步即使根据JAVA虚拟机规范，来检验你加载进来的“.class”文件中的内容，是否符合指定的规范。 2、准备阶段​ 一般情况下，我们写好的类，都有一些类变量，如下： 1234public class ReplicaManager &#123; public static int flushInterval;&#125; ​ 假设有这么一个类“ReplicaManager”，它的“ReplicaManager.class”，刚被加载到内存之后，会被进行验证，确认这个验证码是符合规范的。接着就会进行准备工作。这个准备工作，就是给这个“ReplicaManager”类分配一定的内存空间，然后给他里面的类变量（也就是static修饰的变量）分配内存空间，来一个默认的初始值。比如上面的“flushInterval”这个类变量分配的内存空间，会给一个“0”初始值。 3、解析阶段​ 这个阶段，实际上就是把符合引用替换为直接引用的过程 4、三个阶段的小结​ 这三个阶段中，最核心的就是“准备阶段”，这个阶段是给加载进来的类分配好了内存空间，类变量也分配好了内存空间，并且给了默认的初始值。 核心阶段：初始化​ 上面说过，在准备阶段，会把我们的“ReplicaManager”类给分配好内存空间。另外的一个类变量“flushInterval”也会给一个默认的初始值“0”。那么接下来，在初始化阶段，就会正式执行我们的类初始化的代码了。 ​ 那什么是类初始化代码呢？看看以下代码 12345public class ReplicaManager &#123; public static int flushInterval = Configuration.getInt("replica.flush.interval");&#125; 通过以上代码我们可以知道，这个类变量，我们是通过Configuration.getInt(“replica.flush.interval”)这段代码来获取一个值，并且赋值给他的。但是这个赋值逻辑并不在准备阶段执行，在准备阶段，仅仅是给这个类变量开辟一个内存空间，然后给个初始值“0”而已。 ​ 而这段赋值的代码，则是在“初始化”阶段来执行。在该阶段，代码Configuration.getInt(“replica.flush.interval”)会在这里执行，完成一个配置项的读取，然后赋值给这个类变量“flushInterval”。 另外比如下面的static静态代码块，也会在这个阶段执行。 123456789101112131415public class ReplicaManager &#123; public static int flushInterval = Configuration.getInt("replica.flush.interval"); private static Map&lt;String, Object&gt; replicas; static &#123; loadReplicaFromDish(); &#125; public static void loadReplicaFromDish() &#123; this.replicas = new HashMap&lt;String, Object&gt;(); &#125;&#125; 什么时候会初始化一个类​ 一般来说有一下时机：比如“new ReplicaManager()”来实例化类的对象，此时就会触发类的加载到初始化全过程，把这个类准备好，然后再实例化一个对象出来； ​ 或者是包含“main”方法的主类，必须是立马初始化的。 ​ 这里还有一个非常重要的规则，就是如果初始化一个类的时候，发现它的父类还没初始化，那么先必须初始化它的父类。 类加载器和双亲委派机制​ 上述的过程中，都必须依赖类加载器来实现，Java里主要有几种类加载器 1、启动类加载器（Bootstrap ClassLoader）​ 它主要负责加载我们机器上安装的java目录下的核心类，比如Object、System、String等。 2、扩展类加载器（Extension ClassLoader）​ 用于加载一些扩展的系统类，比如XML、加密、压缩相关的功能类等。JDK9之后变成了平台类加载器，即Platform ClassLoader。 3、应用类加载器（Application ClassLoader）​ 主要是加载用户定义的CLASSPATH路径下的类。 4、自定义类加载器​ 除了上面几种之外，还可以自定义类加载器，去根据你自己的需求加载你的类。 双亲委派机制​ 低层次的当前类加载器，不能覆盖更高层次类加载器已经加载的类。如果低层次的类加载器想加载一个未知类，要礼貌地向上级询问：“请问这个类已经加载了吗”？被询问的高层次类加载器会自问两个问题：第一，我是否已加载过此类？第二，如果没有，是否可以加载此类？只有当所有高层次类加载器在两个问题上的答案均为“否”时，才可以让当前类加载器加载这个未知类。 ​ 简单地讲，所谓的双亲委派模型：先找父亲去加载，不行的话再由儿子来加载。 最后总结]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo的负载均衡策略、集群容错策略和动态代理策略]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F07%2Fdubbo%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AD%96%E7%95%A5%E5%92%8C%E9%9B%86%E7%BE%A4%E5%AE%B9%E9%94%99%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[本节思维导图 dubbo的负载均衡策略random loadbalance​ 随机调用实现负载均衡。这是dubbo默认的负载均衡策略。可以对provider设置不同的权重，会按照权重来负载均衡，权重大分配流量越高。一般使用这个策略即可。 roundrobin loadbalance​ 均匀地将流量打到各个机器上，但如果各个机器性能不一样，容易导致性能差的机器负载过高，所以此时需要调整权重，让性能差的机器承载比较小的流量。 leastactive loadbalance​ 这个就是自动感知一下，某个机器的性能越差，接收的流量就越小，就越不活跃，此时就会给不活跃性能差的机器更小的请求。 consistentHash loadbalance​ 一致性哈希算法，相同参数的请求一定分发到一个provider上去，provider挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就使用这个一致性哈希算法。 dubbo集群容错策略failover cluster模式失败自动切换，自动重试其他机器，默认使用这个，常见于读操作。（失败重试其他机器） 可以通过以下几种方式配置重试次数： 1&lt;dubbo:service retries="2" /&gt; 或者 1&lt;dubbo:reference retries="2" /&gt; 或者 123&lt;dubbo:reference&gt; &lt;dubbo:method name="findFoo" retries="2" /&gt;&lt;/dubbo:reference&gt; failfast cluster模式一次调用失败就立即失败，常用与非幂等性的写操作，比如新增一条记录（调用失败就立即失败） failsafe cluster模式出现异常时忽略掉，常用与不重要的接口调用，比如日志记录。 配置示例如下： 1&lt;dubbo:service cluster="failsafe" /&gt; failsafe cluster模式 或者 1&lt;dubbo:reference cluster="failsafe" /&gt; failback cluster模式失败了后台自动记录请求，然后定时重发，比较适合于写消息队列。 forking cluster模式并行调用多个provider，只要一个成功立即返回，常用于实时性要求比较高的读操作，但是会浪费更多的服务资源，可以通过forks=”2”来设置最大并行数。 broadcast cluster模式逐个调用所有的provider，任何一个provider出错则报错。通用用于通知所有provider更新缓存或日志等本地资源信息。 dubbo动态代理策略默认使用javassist动态字节码生成，创建代理类。但是可以通过spi扩展机制配置自己的动态代理策略。]]></content>
      <categories>
        <category>分布式</category>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo支持的通信协议、序列化协议以及hession的数据结构]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F06%2Fdubbo%E6%94%AF%E6%8C%81%E7%9A%84%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE%E3%80%81%E5%BA%8F%E5%88%97%E5%8C%96%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8Ahession%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[本节思维导图 dubbo支持的通信协议dubbo协议​ 默认就是走dubbo协议，单一长连接，进行的是NIO异步通信，基于hession作为序列化协议。使用的场景是：传输数据量小（每次请求在100kb以内），但是并发量高。 ​ 为了支持高并发场景，一般是服务提供者就几台机器，但是服务消费者有上百台，可能每天调用量达到上亿次，此时用长连接是最合适的，就是跟每个服务消费者维持一个长连接即可，可能总共就100个连接，然后后面直接基于长连接NIO异步通信，可以支撑高并发请求。 ​ 长连接，通俗讲就是建立连接后可以持续发送请求，无须再建立连接。 ​ 而短连接，每次要发送请求之前，需要先重新建立一次连接。 rmi协议​ 走JAVA二进制序列化，多个短连接，适合消费者和提供者数量差不多的情况，适用于文件的传输，一般较少用。 hession协议​ 走hession序列化协议，多个短连接，适用于提供者数量比消费者数量还多的情况，适用于文件传输，一般较少用。 http协议​ 走JSON序列化 webService​ 走SOAP文本序列化 dubbo支持的序列化协议​ dubbo默认的序列化协议是hession序列化协议，除此之外还支持java二进制协议、json和SOAP文本序列化等多种序列化协议。 hession数据结构​ hession的数据结构可以分为三类型：8种基本原始类型、3种递归类型和1中特殊类型 8种基本原始类型 原始二进制数据 64-bit date（64位毫秒值日期） 64-bit double 64–bit long 32-bit int boolean null UTF-8 编码的string 3种递归类型 list for lists and arrays map for maps and dictionaries object for objects 1种特殊类型 ref：用于表示对共享对象的引用 为什么PB的效率是最高的​ protocol buffer是Google出品的一种轻量并且高效的结构化数据存储格式，性能要比JSON和XML高得多。它性能高主要有两个原因：一是，它使用protocol编译器，自动进行序列化和反序列化，速度非常快，差不多比XML和JSON快上了20~100倍；第二，它的数据压缩效果好，它序列化后的数据量体积小，因为体积小，传输起来带宽和速度上会有优化。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis的缓存穿透、缓存击穿、缓存雪崩、热点数据失效问题及解决方案]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F05%2FRedis%E7%9A%84%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E3%80%81%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E3%80%81%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E3%80%81%E7%83%AD%E7%82%B9%E6%95%B0%E6%8D%AE%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[本节思维导图 ​ 在我们的平常的项目中多多少少都会使用到缓存，因为一些数据我们没有必要每次查询的时候都去查询到数据库。特别是高 QPS 的系统，每次都去查询数据库，对于你的数据库来说将是灾难。但缓存使用不当，也会引起灾难。 缓存穿透什么是缓存穿透​ 正常情况下，我们去查询的数据都是存在。但如果请求去查询一条数据库根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去。这种查询不存在数据的现象称为缓存穿透。 缓存穿透带来的问题​ 如果有黑客对你的系统进行攻击，拿一个不存在的id 去查询数据，会产生大量的请求到数据库中，可能会导致你的数据库由于压力太大而宕机。 解决方案1、缓存空值​ 之所以会穿透，是因为缓存中没有存储这些空数据的key，从而导致每次查询都到数据库去了。因此我们可以为这些key对应的值设置null丢到缓存里面去，后面再出现查询这个key的请求的时候，就直接返回null。不过要设置过期时间。 2、布隆过滤器​ 这种方式在大数据场景应用比较多，比如Hbase中使用它去判断数据是否在磁盘上，还有在爬虫场景判断URL是否已经被爬取。 ​ 这种方案可以加在第一种方案中，在缓存之前再加一层布隆过滤器，在查询的时候先去布隆过滤器查询key是否存在，如果不存在就直接返回，存在再走查缓存和数据库。 3、用户鉴权​ 这种情况有可能是黑客进行恶意攻击，因此我们可以在系统中增加用户鉴权校验或者在接口层增加校验，直接拦截不正常的请求。 方案选择​ 对于一些恶意攻击，攻击带过来的大量的key是不存在的，那么我们采用第一种方案就会缓存大量不存在key的数据，此时第一种方案就不合适了，我们可以先使用第二种方案过滤掉这些key。即针对这种key异常多、请求重复率比较低的数据，我们没有必要进行缓存，使用第二种方案直接过滤掉。 缓存击穿什么是缓存击穿​ 在平常高并发的系统中，大量的请求同时查询一个key时，此时这个key刚好失效了，就会导致大量的请求打到数据库上面去，这种现象我们成为缓存击穿。 缓存击穿带来的问题​ 会造成某一时刻数据库请求里过大，压力剧增。 解决方案1、设置热点数据永不过期2、加互斥锁​ 多个线程同时去查询数据库的这条数据时，我们可以在第一个查询数据的请求上使用一个互斥锁来锁住它，其他线程走到这一步拿不到锁就等着，等第一个线程查询到了数据，然后做缓存。后面的线程进来了，就可以直接走缓存了。 缓存雪崩什么是缓存雪崩​ 缓存雪崩的情况是，在某一时刻发生大规模的缓存失效的情况，比如你的缓存服务宕机了，会有大量的请求进来直接打到DB上。结果就是DB撑不住宕机了。 解决方案1、事前，使用集群缓存，保证缓存服务的高可用​ 这种方案是在发生雪崩前对缓存集群实现高可用，如果是使用Redis，可以使用 主从+哨兵或者Redis Cluster来避免Redis全盘崩溃的情况。 2、事中：ehcache本地缓存 + Hystrix限流&amp;降级，避免数据库被打死​ 使用ehcache本地缓存的目的也是考虑在Redis Cluster完全不可用的时候，ehcache本地缓存可以支撑一阵。 ​ 使用Hystrix进行限流&amp;降级，例如一秒来了3000个请求，我们可以设置只能有一秒1000个请求能通过这个组件，那么其他剩余的2000请求就会走限流逻辑。然后去调用我们自己开发的降级组件，比如设置一些默认值之类的，以此来保护数据库不会被大量的请求给打死。 3、事后：开启Redis持久化机制，尽快恢复缓存集群​ 一旦重启，就能从磁盘上自动加载数据恢复内存中的数据。 解决热点数据集中失效问题什么是热点数据集中失效​ 我们在设置缓存的时候，一般会给缓存设置一个失效时间，过了这个时间，缓存就失效。对于一些热点的数据来说，当缓存失效以后会存在大量的请求过来，然后打到数据库中去，从而可能导致数据库崩溃的情况。 解决方案1、设置不同的过期时间​ 为了避免这些热点数据集中失效，那么我们在设置缓存过期时间的时候，尽量让他们失效的时间错开。例如在一个基础的时间上加上或减去一个范围内的随机值。 2、互斥锁​ 结合上面击穿的情况，在第一个请求去查询数据库的时候加一个互斥锁，其余的查询都会被阻塞住，知道锁释放，从而保护数据库。 ​ 但是因为它会阻塞其他线程，此时系统吞吐量会下降，需要结合实际的业务去考虑是否要这么做。 参考资料https://juejin.im/post/5c9a67ac6fb9a070cb24bf34]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的IO模型]]></title>
    <url>%2FCKING.github.io%2F2019%2F08%2F02%2FJava%E7%9A%84IO%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本节思维导图 同步和异步的概念：针对接口调用，API调用，服务调用等。同步调用者必须等待这个接口的磁盘读写或网络通信的操作执行完毕，调用者才可以返回，如图所示： 异步调用者调用接口后，直接就返回，不用等待磁盘读写或网络通信操作完成，而是可以去做其他事情。而这个接口如果干完了某件事，会反过来通知调用者，之前的调用成功了。这个可以通过内部机制来通知，或者通过回调函数来通知。 阻塞和非阻塞：针对的是底层底层IO操作阻塞比如我们的程序现在想要通过网络读取数据，如果是阻塞IO模式，一旦发起请求到操作系统内核去从网络中读取数据，就会阻塞在那里，必须等待网络中的数据到达之后，才能从网络中读取数据到内核，再从内核返回给程序。 非阻塞程序发送请求给内核要从网络读取数据，但是此时网络中的数据还没到，此时不会阻塞住，内核会返回一个异常信息给程序，程序可以干别的，然后不断去轮询去访问内核，看请求的数据是否读取到了。如图所示： BIO，NIO，多路复用IO，信号驱动式IO和AIOBIO​ 主要是同步阻塞IO模型，在JAVA里叫做BIO，在JDK1.4之前，在JAVA代码里调用IO相关接口，发起IO操作之后，JAVA程序就会同步等待，这个同步指的是JAVA程序调用IO API接口的层面而言。 ​ 而IO API在底层的IO操作是基于阻塞IO来的，向操作系统内核发起IO请求，系统内核会等待数据就位之后，才会执行IO操作，执行完毕了才会返回。 NIO​ 在JDK1.4之后提供了NIO，他的概念是同步非阻塞，也就是说如果你调用NIO接口去执行IO操作，其实还是同步等待，但是在底层的IO操作上，会对系统内核发起非阻塞IO请求，以非阻塞的形式来执行IO。 ​ 也就是说，如果底层数据没到位，那么内核会返回异常信息，不会阻塞住，但是NIO接口内部会采用非阻塞方式过一会儿再次调用内核发起IO请求，知道成功为止。 ​ 之所以说是同步非阻塞的，这里的“同步”指的就是因为在你的JAVA代码调用NIO接口层面是同步的，你还是要同步等待底层IO操作真正完成了才可以返回，只不过在执行底层IO的时候采用了非阻塞的方式来执行罢了。 IO多路复用模型​ 实际上，如果基于NIO进行网络通信，采取的就是多路复用的IO模型，这个多路复用IO模型针对的是网络通信中的IO场景来说的。就是在基于Socket进行网络通信的时候，如果有多个客户端跟你的服务端建立了Socket连接，你就需要维护多个Socket连接。而所谓的多路复用IO模型，就是说你的JAVA代码直接通过一个select函数（一般都是系统内核级别的函数，除此还有poll,epoll）调用，直接进入一个同步等待的状态。 ​ 这也是为什么说NIO一定是“同步”的，因为你必须在这里同步等待某个Socket连接有请求到来。接着你就要同步等着select函数去对底层的多个Socket连接进行轮询，不断地查看各个Socket连接谁有请求到达，就可以让select函数返回，交给我们的java程序处理。 ​ select函数在底层会通过非阻塞的方式轮询各个Socket，任何一个Socket如果没有数据到达，那么非阻塞的特性会立即返回一个信息。然后select函数可以轮询下一个Socket，不会阻塞在某个Socket上，所以底层是基于这种非阻塞的模式来“监视”各个Socket谁有数据到达的。 ​ 这就是所谓的“同步非阻塞”，但是因为操作系统把上述工作都封装在一个select函数调用里，可以对多路Socket连接同时进行监控，所以就把这种模型称为“IO多路复用”模型。 ​ 通过这个模型，就可以用一个线程，调用一个select函数，然后监视大量的客户端连接，如下图： 信号驱动式IO​ 首先我们允许Socket进行信号驱动IO，并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，会收到一个SIGIO信号，可以在信号处理函数中调用IO操作函数处理数据，如下图所示： ​ 相比于非阻塞式IO的轮询方式，信号驱动IO的CPU利用率更高。 AIO​ 在JDK1.7之后，又支持了AIO，也就做NIO2.0，他就支持异步IO模型。 ​ 异步IO模型，就是你的Java程序可以基于AIO API发起一个请求，比如接收网络数据，AIO API底层会基于异步IO模型来调用操作系统内核。此时不惜要去管这个IO是否成功，AIO接口会直接返回，你的Java程序也会直接返回。然后，你的Java程序就可以去干别的事情了。 ​ BIO，NIO都是同步的，你发起IO请求，都必须同步等待IO操作完成，但是这里你发起一个请求，直接AIO接口就返回了，你可以干别的事情了，纯异步方法，不过需要你提供一个回调函数给AIO接口，一旦底层系统内核完成了具体的IO请求，比如网络读写之类的，就会回调你提供的回调函数。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo工作原理]]></title>
    <url>%2FCKING.github.io%2F2019%2F07%2F31%2Fdubbo%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[本节思维导图 dubbo工作原理 第一层：service层，接口层：有服务提供者和服务消费者实现 第二层：config层，配置层：主要对dubbo进行各种配置 第三层：proxy层，服务代理层，为provider、consumer生成代理，代理之间进行网络通信 第四层：registry层，服务注册层，负责服务的注册与发现 第五层：cluster层，集群层，封装多个服务提供者的路由和负载均衡，将多个实例组合成一个服务 第六层：monitor层，监控层，对rpc接口的调用时间和调用次数进行监控 第七层：protocal层，远程调用层，封装rpc调用 第八层：exchange层，信息交换层，封装请求相应模式，同步转异步 第九层：transport层，网络传输层，抽象mina和Netty为统一接口 第十层：serialize层，数据序列化层 工作流程 第一步：provider向注册中心注册 第二步：consumer从注册中心订阅服务，注册中心通知consumer注册好的服务 第三步：consumer调用provider 第四步：consumer和provider都异步通知监控中心 dubbo架构图 注册中心挂了可以继续通信吗可以，因为刚开始初始化的时候，消费者会将提供服务的地址信息拉取到本地缓存，所以注册中心挂了可以继续通信]]></content>
      <categories>
        <category>分布式</category>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[观察者模式]]></title>
    <url>%2FCKING.github.io%2F2019%2F07%2F30%2F%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[​ “红灯停，绿灯行”。在这个过程中，信号灯是汽车的观察目标，汽车是观察者。随着信号灯的变化，汽车的行为也随着变化。在软件系统中，一个对象的状态或行为的变化将导致其他对象的行为或状态也发生改变，它们之间将产生联动。为了更好地描述对象之间存在的这种一对多（包括一对一）的联动，观察者模式应运而生。 ​ 观察者模式是使用频率最高的设计模式之一，用于建立一种对象与对象之间的依赖关系，一个对象发生改变时将自动通知其他对象，其他对象将相应作出反应。在观察者模式中，发生改变的对象成为观察目标，而被通知的对象成为观察者。这些观察者之间可以没有任何相互联系，可以根据需要增加和删除观察者，使得系统更易于扩展。 观察者模式定义：观察者模式：定义对象之间的一种一对多依赖关系，使得每当一个对象状态发生改变时，其相关依赖对象皆得到通知并被自动更新。观察者模式的别名包括发布-订阅（Publish/Subscribe）模式、模型-视图（Model/View）模式、源-监听器（Source/Listener）模式或从属者（Dependents）模式。观察者模式是一种对象行为型模式。 观察者模式结构图： 在结构图中包含以下四个角色： （1）Subject（目标）：目标又称为主题，它是指被观察的对象。在目标中定义了一个观察者集合，一个观察目标可以接受任意数量的观察者来观察，它提供一系列方法来增加和删除观察者对象，同时定义了通知方法notify()。目标类可以是接口，也可以是抽象类或具体类。 （2）ConcreteSubject（具体目标）：具体目标是目标类的子类，通常包含有经常发生改变的数据，当他的状态改变时，向其各个观察者发出通知；同时它还实现了在目标类中定义的抽象业务逻辑方法（如果有的话）。如果无须扩展目标类，则目标具体类可以省略。 （3）Observer（观察者）：观察者将对观察目标的改变做出反应，观察者一般定义为接口，改接口声明了更新数据的方法update()，因此又称为抽象观察者。 （4）ConcreteObserver（具体观察者）：在具体观察者中维护一个指向具体目标对象的引用，它存储具体观察者的有关状态，这些状态需要和具体目标的状态保持一致。它实现了在抽象观察者Observer中声明的update()方法。通常在实现时，可以调用具体的目标类的attach()方法将自己添加到目标类的集合或通过detach()方法将自己从目标类的集合中删除。 观察者模式主要优缺点：1、主要优点（1）观察者模式可以实现表示层和数据逻辑层的分离，定义了稳定的消息更新传递机制，并抽象了更新接口，使得可以有各种各样不同的表示层充当具体观察者角色。 （2）观察者模式在观察目标和观察者之间建立一个抽象的耦合。观察目标只需要维持一个抽象观察者的集合，无须了解其具体观察者。由于观察目标和观察者没有紧密地耦合在一起，因此它们可以不同的抽象化层次。 （3）观察者支持广播通信，观察目标会向所有已注册的观察者对象发送通知，简化了一对多系统设计的难度。 （4）观察者模式满足开闭原则的要求，增加新的具体观察者无须修改原有系统代码，在具体观察者与观察目标之间不存在关联关系的情况下，增加新的观察目标也很方便。 2、主要缺点（1）如果一个观察目标对象有很多直接或间接观察者，将所有的观察者都通知到会花费很多时间 （2）如果在观察者和观察目标之间存在循环依赖，观察目标会触发它们之间进行循环调用，可能导致系统崩溃。 （3）观察者模式没有相应的机制让观察者知道所观察的目标是怎么发生变化的，而仅仅只是知道观察目标发生了变化。 观察者模式使用场景：（1）一个抽象模型有两个方面，其中一个方面依赖于另一个方面，将这两个方面封装在独立的对象中使它们可以各自独立地改变和复用。 （2）一个对象的改变将导致一个或多个其他对象也发生改变，而并不知道具体有多少对象将发生改变，也不知道这些对象是谁。 （3）需要在系统中创建一个触发链，A对象的行为将影响B对象，B对象的行为将影响C对象……，可以使用观察者模式创建一种链式触发机制。 观察者模式与MVC的关系：​ MVC是一种架构模式，它包含3个角色：模型（Model）、视图（View）和控制器（Controller）。其中，模型可应对于观察者模式中的观察目标，而视图对应于观察者，控制器可充当两者之间的中介者，当模型层的数据发生改变时，视图层将自动显示内容。 案例​ Sunny软件公司欲开发一款多人联机对战游戏（类似魔兽世界），在该游戏中，多个玩家可以加入同一战队组成联盟，当战队中某一成员受到敌人攻击时将给所有其他盟友通知，盟友收到通知后将做出响应。 ​ Sunny公司通过对系统功能需求进行分析，发现在改系统中战队成员之间的联动过程可以简单描述如下：联盟成员受到攻击–&gt;发送通知给盟友–&gt;盟友做出响应。如果按照此思路来设计系统，因为成员在受到攻击时需要通知他的每位盟友，所以每个成员都需要持有其他所有盟友的信息，这将导致系统开销较大。因此开发人员决定引入一个新的角色–“战队控制中心”来负责维护和管理每个战队所有成员的信息。当一个成员受到攻击时，向相应的战队控制中心发送求助信息，战队控制中心再逐一通知每个盟友，盟友再做出相应。如图所示： 为了实现对象之间的联动，Sunny公司决定使用观察者模式来进行多人联机对战游戏的设计，其基本结构图如图所示： 相关代码实现已上传]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis哨兵集群实现高可用]]></title>
    <url>%2FCKING.github.io%2F2019%2F07%2F29%2FRedis%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[哨兵的介绍​ sentinel，也叫哨兵。哨兵是Redis集群机构中非常重要的一个组件，有以下功能： 集群监控：负责监控redis master和slave进程是否正常工作。 消息通知：如果某个redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果master node挂掉了，会自动转移到slave node上。 配置中心：如果故障转移发生了，通知client客户端新的master地址。 ​ 哨兵用于实现Redis集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。 故障转移时，判断一个master node是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作。 哨兵的核心知识sdown和odown转换机制 sdown是主观宕机，就一个哨兵如果自己觉得一个master宕机了，那么就是主观宕机。 odown是客观宕机，如果quorum数量的哨兵都觉得一个master宕机了，那么就是客观宕机。 sdown达成的条件比较简单，如果一个哨兵ping一个master，超过了is-master-down-after-milliseconds指定的毫秒数之后，就主观认为master宕机了；如果一个哨兵在指定的时间内，收到了quorum数量的其他哨兵也认为那个master是sdown，那么就认为是odown。 quorum（法定人数）和majority​ 每次一个哨兵要做主备切换，首先需要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，这个哨兵还需要得到majority哨兵的授权，才能正式执行切换。 ​ 如果quorum &lt; majority，比如5个哨兵，majority就是3， quorum设置为2，那么就需要3个哨兵授权就可以执行切换。 ​ 如果quorum &gt;= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵，quorum是5，那么必须5个哨兵都同意授权，才能执行切换。 核心知识 哨兵至少需要3个实例，来保证自己的健壮性。 哨兵 + Redis主从的部署架构，是不保证数据零丢失的，只能保证redis集群的高可用性。 对于哨兵 + redis主从这种复制的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 哨兵集群必须部署2个以上节点，如果哨兵集群仅仅部署了2个哨兵实例，quorum = 1。 ​ 配置quorum = 1，如果master宕机，两个哨兵只要有1个哨兵认为master宕机了，就可以进行切换，同时会选举出一个哨兵来执行故障转移，但是同时这个时候，需要majority个哨兵，也就是大多数哨兵是运行的。 123452 个哨兵，majority=23 个哨兵，majority=24 个哨兵，majority=25 个哨兵，majority=3... ​ 如果此时只是Master宕机，哨兵1正常运行，那么故障转移时OK的，如果是Master和哨兵1运行的机器宕机了，那么哨兵只有一个，此时就没有majority数量个哨兵来执行故障转移，虽然另外一台机器上还有一个哨兵，但是故障转移不会执行。 ​ 经典的3节点哨兵集群是这样的： ​ 配置quorum = 2，如果M1所在机器宕机了，那么三个哨兵还剩下2个，S2和S3可以一致认为master宕机了，然后选举一个来执行故障转移，同时3个哨兵的majority是2，所以还剩下2个哨兵运行着，就可以允许执行故障转移。 Redis哨兵主备切换的数据丢失问题异步复制导致的数据丢失​ 因为master -&gt; slave的复制是异步的，所以可能有部分数据还没复制到slave，master就宕机了，此时这部分数据就丢失了。 脑裂导致的数据丢失​ 脑裂，即某个master所在机器突然脱离了正常的网络，跟其他slave机器不能连接，但是实际上master还运行着，此时哨兵可能就会认为master宕机了，然后开启选举，将其他slave切换成了master。这个时候，集群里就会有两个master，这就是所谓的脑裂。 ​ 此时虽然某个slave被切换成了master，但是可能client还没来得及切换到新的master，还继续向旧的master写数据。因此旧master再次恢复的时候，会被作为一个master挂到新的master上去，自己的数据会清空，重新从新的master复制数据，而新的master并没有后来client写入的数据，因此这部分数据也就丢失了。 数据丢失问题的解决方案可行进行如下配置： 12min-slaves-to-write 1min-slaves-max-lag 10 表示，要求至少有1个slave，数据复制和同步的延迟不能超过10秒。 减少异步复制数据的丢失 ​ 有了min-slaves-max-lag这个配置，就可以确保说，一旦slave复制数据和ack延时太长，就认为可能master宕机后损失的数据太多了，那么就拒绝写请求，这样可以把master宕机时由于部分数据未同步到slave导致的数据丢失降低到可控范围内。 减少脑裂的数据丢失 ​ 如果一个master出现了脑裂，跟其他slave丢了连接，那么上面两个配置可以确保说，如果不能继续给指定的slave发送数据，而且slave超过了10秒没有给自己（master）ack消息，那么就直接拒绝客户端的写请求，因此在脑裂的情况下，最多就丢失10秒的数据。 哨兵集群的自动发现机制​ 哨兵互相之间的发现，是通过redis的pub/sub系统实现的，每个哨兵都会往_sentine__:hello这个channel里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知其他哨兵的存在。 ​ 每隔两秒钟，每个哨兵都会往自己监控的某个maser + slave对应的_sentinel__:hellochannel里发送一个消息，内容是自己的Host、ip和runid还有对这个master的监控配置。 ​ 每个哨兵也会去监听自己监控的每个 master+slaves 对应的 __sentinel__:hello channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。 ​ 每个哨兵还会跟其他哨兵交换对 master 的监控配置，互相进行监控配置的同步。 slave配置的自动纠正​ 哨兵会负责自动纠正slave的一些配置，比如slave如果要成为潜在的master候选人，哨兵会确保复制现有的master数据；如果slave连接到了一个错误的master上。比如故障转移后，那么哨兵会确保它们连接到正确的master上。 slave -&gt; master选举算法​ 如果一个master被认为odown，而且majority数量的哨兵都允许主备切换，那么某个哨兵就会执行住别切换操作，此时需要选举一个slave来当master，会考虑slave的一些信息： 跟master断开连接的时长 slave优先级 复制offset run id ​ 如果一个slave跟master断开连接的时间已经超过了down-after-milliseconds的10倍，外加master的宕机的时长，那么slave就被认为不适合选举为master 1(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state ​ 接下来会对slave进行排序： 按照slave优先级进行排序，slave priority越低，优先级越高。 如果slave priority相同，那么看replica offset，哪个slave复制了越多的数据，offset越靠后，优先级就越高。 如果上面两个条件都相同，那么选一个run id比较小的那个。 configuration epoch​ 哨兵会对一套redis master + slaves进行监控，有相应的监控配置。 ​ 执行切换的那个哨兵，会从要切换到新的master（slave -&gt; master）那里得到一个configuration epoch，这就是一个version号，每次切换的version号都必须是唯一。 ​ 如果第一个选举出的哨兵切换失败，那么其他哨兵，就会等待failover-timeout时间，然后接替继续执行切换，此时会重新获取一个新的configuration epoch，作为新的version号。 configuration传播​ 哨兵完成切换之后，会在自己本地更新生成最新的master配置，然后同步给其他的哨兵，就是通过之前说的pub/sub消息机制。 ​ 这里之前的version号就很重要了，以为各种消息都是通过一个channel去发布和监听的，所以一个哨兵完成一次新的切换之后，新的master配置是跟着新的version号的。其他的哨兵都是根据版本号的大小来跟新自己的master配置的。]]></content>
      <categories>
        <category>分布式</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UML之类之间的关系]]></title>
    <url>%2FCKING.github.io%2F2019%2F07%2F27%2FUML%E4%B9%8B%E7%B1%BB%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[​ 在软件系统中国，类并不是孤立存在的，类与类之间存在各种关系，对于不同类型的关系，UML提供了不同的表示方式。 关联关系​ 关联关系是类与类之间最常用的一种关系，它是一种结构化关系，用于表示一类对象与另一类对象之间有联系，如汽车和轮胎，师傅和徒弟，班级和学生等。在UML类图中，用实现连接有关联关系的对象所对应的类。 ​ 例如现在一个登陆界面类LoginForm中包含一个JButton类型的注册按钮loginButton，它们之间可以表示为关联关系，代码实现时可以在LoginForm中定义一个名为loginButton的属性对象，其类型为JButton，如图所示： 对应的JAVA代码片段如下： 1234567891011public class LoginForm &#123; //定义成员变量 private JButton loginButton; ...&#125;public class JButton &#123; ...&#125; ​ 在UML中，关联关系又包含如下几种形式。 双向关联​ 默认情况下，关联是双向的。例如顾客购买商品并拥有商品，反之，卖出的商品总有某个顾客与之关联。因此，Customer类和Product类之间具有双向关联关系，如图所示： ​ 对应的代码片段如下： 123456789101112public class Customer &#123; private Product[] products; ...&#125;public class Product &#123; private Customer customer; ...&#125; 单向关联​ 类的关联关系也可以是单向的，在UML中单向关联关系用带箭头的实线表示。例如，顾客拥有地址，则Customer类与Address类具有单向关联关系，如图所示： ​ 对应的代码如下： 1234567891011public class Customer &#123; private Address address; ...&#125;public class Address &#123; ...&#125; 自关联​ 在系统中可能会存在一些类的属性对象类型为该类本身，这种特殊的关联关系成为自关联。例如，一个节点类（Node）的成员又是节点Node类型的对象，如图所示： ​ 对应的代码如下： 123456public class Node &#123; private Node subNode; ...&#125; 多重性关联​ 多重性关联关系又称为重数性（Multiplicity）关联关系，表示两个关联对象在数量上的对应关系。在UML中，对象之间的多重性可以直接在关联直线上用一个数字或一个数字范围表示。 ​ 对象之间可以存在多种多重关联惯性，常见的多重性表示方法如下表： 表示方式 多重性说明 1.. 1 表示另一个类的一个对象只与改类的一个对象有关系 0.. * 表示另一个类的一个对象与该类的零个或多个对象有关系 1.. * 表示另一个类的一个对象与该类的一个或多个对象有关系 0.. 1 表示另一个类的一个对象没有或只与改类的一个对象有关系 m.. n 表示另一个类的一个对象与该类最少m，最多n个对象有关系（m &lt;= n） ​ 例如，一个界面可以拥有零个或多个按钮，但是一个按钮只能属于一个界面。因此，一个Form类的对象可以与零个或多个Button类的对象相关联，但一个Button类的对象只能与一个Form类的对象关联，如图所示： ​ 对应的代码如下： 123456789101112public class Form &#123; //定义一个集合对象 private Button[] buttons; ...&#125;public class Button &#123; ...&#125; 聚合关系​ 聚合关系表示整体与部分的关系。在聚合关系中，成员对象是整体对象的一部分，但是成员对象可以脱离整体对象独立存在。在UML中，聚合关系用带空心菱形的直线表示。例如，汽车发送机是汽车的组成部分，但是汽车发送机可以独立存在，因此，汽车和发送机是聚合关系，如图所示： ​ 在代码实现聚合关系时，成员对象通常作为构造方法、Setter方法或业务方法的参数注入到整体对象中，如下所示： 123456789101112131415161718192021public class Car &#123; private Engine engine; //构造注入 public Car(Engine engine) &#123; this.engine = engine; &#125; //设值注入 public void setEngine(Engine engine) &#123; this.engine = engine; &#125; ...&#125;public class Engine &#123; ...&#125; 组合关系​ 组合关系也表示类之间整体和部分的关系，但是在组合关系中整体对象可以控制成员对象的生命周期，一旦整体对象不存在，成员对象也将不存在，成员对象与整体对象之间具有同生共死的关系。在UML中，组合关系用带实心菱形的直线表示。例如，人的头（Head）与嘴巴（Mouth），嘴巴是头的组成部分之一，而且如果头没了，嘴巴也就没了，因此头和嘴巴是组合关系，如图所示： ​ 在代码实现组合关系时，通常在整体类的构造方法中直接实例化成员类，如下所示： 123456789101112131415public class Head &#123; private Mouth mouth; public Head() &#123; mouth = new Mouth(); &#125; ...&#125;public class Mouth &#123; ...&#125; 依赖关系​ 依赖关系是一种使用关系，特定事物的改变有可能会影响到使用该事物的其他的事物，在需要表示一个事物使用另一个事物时使用依赖关系。大多数情况下，依赖关系体现在某个类的方法使用另一个类的对象作为参数。在UML中，依赖关系用带箭头的虚线表示，由依赖的一方指向被依赖的一方。例如，驾驶员开车，在Driver类的drive()方法中将Car类型的对象作为一个参数传递，以便在drive()方法中能调用Car类的move()方法，且驾驶员的drive()方法依赖车的move()方法，因此类Drive依赖类Car，如图所示： ​ 在系统实施阶段，依赖关系通常通过3种方式来实现。第1种也是最常用的一种方式是上面讲的将一个类的对象作为另一个类中方法的参数；第2种方式是在一个类的方法中将另一个类的对象作为其局部变量；第3种方式是在一个类的方法中调用另一个类的静态方法。 ​ 相应的代码如下： 12345678910111213141516public class Driver &#123; public void drive(Car car) &#123; car.move(); &#125; ...&#125;public class Car &#123; public void move() &#123; ... &#125; ...&#125; 泛化关系​ 泛化关系也就是继承关系,用于描述父类与子类之间的关系，父类又称作基类或超类，子类又称作派生类。在UML中,泛化关系用带空心三角形的直线来表示。在代码实现时,使用面向对象的继承机制来实现泛化关系，如在Java语言中使用extends关键字、在C++/C#中使用冒号“:”来实现。 ​ 相应的代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637//父类public class Person &#123; protected String name; protected int age; public void move() &#123; ... &#125; public void say() &#123; ... &#125;&#125;//子类public class Student extends Person &#123; private String stuedentNo; public void study() &#123; ... &#125;&#125;//子类public class Teacher extends Person&#123; private String teacherNo; public void teach() &#123; ... &#125;&#125; 接口与实现关系​ 在很多面向对象语言中都引人了接口的概念，如Java、C#等。在接口中,通常没有属性，而且所有的操作都是抽象的，只有操作的声明，没有操作的实现。UML中用与类的表示法类似的方式表示接口，接口之间也可以有与类之间关系类似的继承关系和依赖关系，但是接口和类之间还存在一种实现(Realization)关系。在这种关系中，类实现了接口，类中的操作实现了接口中所声明的操作。在UML中,类与接口之间的实现关系用带空心三角形的虚线来表示。 ​ 相应的代码如下所示： 1234567891011121314151617public interface Vehicle &#123; public void move();&#125;public class Ship implements Vehicle &#123; public void move() &#123; ... &#125;&#125;public class Car implements Vehicle &#123; public void move() &#123; ... &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP的基础概念]]></title>
    <url>%2FCKING.github.io%2F2019%2F07%2F25%2FHTTP%E7%9A%84%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[本节思维导图 URIURI(Uniform Resource Identifier)统一资源标识符，其中包括两个部分：URL(Uniform Resource Locator)统一资源定位符和URN(Uniform Resource Name)统一资源名称。 HTTP方法客户端发送的请求报文的第一行为请求行，包含了方法字段。 GET 获取资源 HEAD 获取报文首部 和GET方法类似，但不返回报文实体主体部分。主要用于确认URL的有效性和资源更新的日期时间等。 POST 传输实体主体 POST主要用来传输数据，GET只要用来获取资源 PUT 上传文件 由于自身不带验证机制，任何人都可以上传文件，因此存在安全性问题，一般不使用该方法 PATCH 对资源进行部分修改 PUT也可以用于修改资源，但是只能完全替代原始资源，PATCH允许部分修改。 DELETE 删除文件 与PUT功能相反，并且同样不带验证机制 OPTIONS 查询支持的方法 查询指定的URL能够支持的方法。会返回Allow:GET, POST, HEAD, OPTIONS 这样的内容 CONNECT TRACE]]></content>
      <categories>
        <category>计算机网络</category>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>计算机网络 HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2FCKING.github.io%2F2019%2F07%2F24%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Hello</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列优缺点]]></title>
    <url>%2FCKING.github.io%2F2019%2F07%2F24%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%BC%98%E7%BC%BA%E7%82%B9%2F</url>
    <content type="text"><![CDATA[本节思维导图 消息队列的优点消息队列主要有三个优点：解耦、异步和削峰 削峰假设一个业务场景如图所示，A系统分别发送信息给B C D系统，此时系统的耦合性比较高，当需求改变要求不给D系统发送消息，或者增加一个E系统也需要A系统发送数据，那么我们就不得不去修改A系统的代码。除此之外如果其他系统挂了，同样也会影响到A系统，如果引入MQ，那么A系统就只需要把数据放进MQ中，由需要这个数据的系统去订阅这个MQ，这样就可以实现系统解耦。这样A系统不仅不需关系哪个系统需要这个消息，也不需要关心消息发送失败或者超时等情况。 总结：通过MQ，Pub/Sub发布订阅消息模型，A系统就跟其他系统解耦了。 异步业务场景如图所示，用户向A系统发起请求，需要在本地写数据库，还需要在B C D系统写数据库，假设本地写耗时10ms，如下图需要10+20+30+40共100ms。那么这样相对来说比较慢。 但如果引入MQ，假设将消息放入MQ耗时5ms，那么总共需要15ms，极大提高了响应速度。 削峰假设每天0:00-12:00系统每秒并发请求数量只有十几个，但过了12点之后，请求数量猛增到几千个，而且数据库系统是mysql，每秒最多也就执行一千多条SQL语句，这样导致mysql崩溃，但过了高峰期之后并发请求又恢复到了相对比较低的水平，对整个系统又没啥压力了。 引入MQ之后，A系统每秒中只能处理一千个SQL，那就从MQ中取出一千个SQL去处理，这样即使在高峰的时候，A系统也不会崩溃，而在高峰期间MQ可能会有几百万的请求积压在MQ中，但这个短暂的高峰的积压是允许，等高峰期一过，MQ积压的数据就能够迅速被处理。 消息队列的缺点缺点有以下几个： 系统可用性降低引入的外部依赖越多，风险就越高。本来只是调用几个系统的接口就可以了，现在引入MQ之后，如果MQ挂了，整个系统就崩溃了。 系统复杂性提高引入MQ之后，需要考虑很多问题。例如如何保证消息传递的顺序，保证消息没有重复消费和怎么处理消息丢失的情况。 数据一致性问题A系统将数据放到MQ将返回成功了，但如果B系统数据丢失了，那就会造出数据不一致了。 ActiveMQ、RabbitMQ、RocketMQ和kafka有什么优缺点 特性 ActiveMQ RabbitMQ RocketMQ kafka 单机吞吐量 万级，比RocketMQ、kafka低一级 同ActiveMQ 10万级，支撑高吞吐 10万级，高吞吐，一般配合大数据类的系统机型实时数据计算，日志采集等场景 topic数量对吞吐量的影响 topic可以达到几百/几千的级别，这是RocketMQ的优势，在同等机器下，可以支撑大量的topic topic从几十到几百个左右的时候，吞吐量会大幅度下降，kafka尽量保证topic数量不要过多，如果要支撑大规模的topic，需要增加更多的机器资源 时效性 ms级 微秒级，这是RabbitMQ的一大特点，延迟最低 ms级 延迟在ms级以内 可用性 高，基于主从架构实现高可用 同ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到0丢失 同RocketMQ 功能支持 MQ领域的功能极其完备 基于erlang开发，并发能力很强，性能极好，延时很低 MQ功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis的过期策略和内存淘汰机制]]></title>
    <url>%2FCKING.github.io%2F2019%2F07%2F21%2FRedis%E7%9A%84%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5%E5%92%8C%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本节思维导图 ​ redis主要是基于内存来进行高性能、高并发的读写操作的。但既然内存是有限的，例如redis就只能使用10G，你写入了20G。这个时候就需要清理掉10G数据，保留10G数据。那应该保留哪些数据，清除哪些数据，为什么有些数据明明过期了，怎么还占用着内存？这都是由redis的过期策略来决定的。 redis过期策略​ redis的过期策略就是：定期删除 + 惰性删除。 ​ 定期删除，指的是redis默认是每隔100ms就随机抽取一些设置了过期时间的key，检查是否过期，如果过期就删除。 ​ 假设redis里放了10W个key，都设置了过期时间，你每隔几百毫秒就检查全部的key，那redis很有可能就挂了，CPU负载会很高，都消耗在检查过期的key上。注意，这里不是每隔100ms就遍历所有设置过期时间的key，那样就是一场性能灾难。实际上redis是每隔100ms就随机抽取一些key来检查和删除的。 ​ 定期删除可能会导致很多过期的key到了时间并没有被删除掉。这个时候就可以用到惰性删除了。惰性删除是指在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间并且已经过期了，此时就会删除，不会给你返回任何东西。 ​ 但即使是这样，依旧有问题。如果定期删除漏掉了很多过期的key，然后你也没及时去查，也就没走惰性删除。此时依旧有可能大量过期的key堆积在内存里，导致内存耗尽。 ​ 这个时候就需要内存淘汰机制了。 内存淘汰机制​ redis内存淘汰机制有以下几个： noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。这个一般很少用。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key，这个是最常用的。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。 LRU算法​ 上面的内存淘汰机制中，用到的是LRU算法。什么是LRU算法？LRU算法其实就是上面说的最近最少使用策略。实现LRU算法，大概的思路如下： ​ 维护一个有序单链表，越靠近链表尾部的节点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表： 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的节点，并将其从原来的位置删除，然后再插入到链表的头部。 如果此数据没有在缓存链表中，又可以分为两种情况： 如果此时缓存未满，则将此节点直接插入到链表的头部； 如果此时缓存已满，则链表尾节点删除，将新的数据节点插入链表的头部。 ​ 这就就实现了LRU算法。 ​ 当然我们也可以基于Java现有的数据结构LinkedHashMap手撸一个。LinkHashMap本质上是一个Map与双向链表的结合，比起上述的单链表，效率更高。代码如下： 1234567891011121314151617181920class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private final int CACHE_SIZE; /** * 传递进来最多能缓存多少数据 * * @param cacheSize 缓存大小 */ public LRUCache(int cacheSize) &#123; // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。 super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); CACHE_SIZE = cacheSize; &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。 return size() &gt; CACHE_SIZE; &#125;&#125;]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis的数据结构及其应用场景]]></title>
    <url>%2FCKING.github.io%2F2019%2F07%2F21%2FRedis%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[本节思维导图 redis主要有以下几种数据类型： string hash list set sorted set string这是最简单的类型，就是普通的set和get，做简单的KV缓存 1set college gpnu hash这个是类似map的一种结构，一般是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他对象）给缓存在redis里，然后每次读写缓存的时候，就可以操作hash里的某个字段。 1234hset person name bingohset person age 20hset person id 1hget person name 12345person = &#123; "name": "bingo", "age": 20, "id": 1&#125; listlist是有序列表。可以通过list存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的。 还可以通过lrange命令，读取某个闭区间内的元素，可以基于list实现分页查询，基于redis实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走 12# 0开始位置，-1结束位置，结束位置为-1时，表示列表的最后一个位置，即查看所有。lrange mylist 0 -1 123456lpush mylist 1lpush mylist 2lpush mylist 3 4 5# 1rpop mylist setset是无序集合，自动去重。 直接基于set将系统里需要去重的数据扔进去，自动就去去重了。如果你需要对一些数据进行快速的全局去重，如果是单机系统就可以基于Java的HashSet进行去重，如果你的某个系统部署在多台机器上，就可以基于redis进行全局的set去重。 可以基于set玩交集、并集、差集的操作。比如交集，可以把两个人的粉丝列表整一个交集，看看两人的共同好友是谁。或者把两个大V的粉丝放在两个set中，对两个set做交集。 1234567891011121314151617181920212223242526272829303132#-------操作一个set-------# 添加元素sadd mySet 1# 查看全部元素smembers mySet# 判断是否包含某个值sismember mySet 3# 删除某个/些元素srem mySet 1srem mySet 2 4# 查看元素个数scard mySet# 随机删除一个元素spop mySet#-------操作多个set-------# 将一个set的元素移动到另外一个setsmove yourSet mySet 2# 求两set的交集sinter yourSet mySet# 求两set的并集sunion yourSet mySet# 求在yourSet中而不在mySet中的元素sdiff yourSet mySet sorted setsorted set是排序的set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序 12345678910zadd board 85 zhangsanzadd board 72 lisizadd board 96 wangwuzadd board 63 zhaoliu# 获取排名前三的用户（默认是升序，所以需要 rev 改为降序）zrevrange board 0 3# 获取某用户的排名zrank board zhaoliu]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
</search>
